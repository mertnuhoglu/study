---
title: "Generate Sample Data from DDL"
date: 2018-01-19T18:30:12+03:00 
draft: false
description: ""
tags:
categories: sql, datafiller
type: post
url:
author: "Mert Nuhoglu"
output: rmarkdown::html_document
blog: mertnuhoglu.com
resource_files:
- data/ddl_to_data_schema_01.sql
- data/ddl_to_data_data_01.sql
- data/ddl_to_data_schema_02.sql
- data/ddl_to_data_data_02.sql
- data/ddl_to_data_schema_03.sql
- data/ddl_to_data_data_03.sql
- data/ddl_to_data_schema_04.sql
- data/ddl_to_data_data_04.sql
- data/ddl_to_data_schema_05.sql
- data/ddl_to_data_data_05.sql
- data/ddl_to_data_place_enum.txt
- data/ddl_to_data_schema_06.sql
- data/ddl_to_data_data_06.sql
- data/ddl_to_data_schema_07.sql
- data/ddl_to_data_data_07.sql
- data/ddl_to_data_action_enum.txt
- data/ddl_to_data_enum_value_id.txt
- data/ddl_to_data_place_enum.txt
path: ~/projects/study/problem/data_generation/ddl_to_data.Rmd
---

This document explains how to generate sample data for any database automatically from its database schema defined as `DDL` statements.

To do this, I use `datafiller` which is a command line tool that generates random data from database schemas described in DDL. 

## v01: Basic datafiller Usage

Here is the first version of my database schema with embedded datafiller directives. In this first version, there is only one datafiller directive. It is in this line: `CREATE TABLE Pln_Orl (  --df: mult=2.0`

``` {bash}
cat data/ddl_to_data_schema_01.sql
``` 

`df: mult=2.0` means to multiply number of samples to be generated for the corresponding entity with `2.0`. When running `datafiller` we specify size of the default sample set. Here I specified it as `--size=2`, so `datafiller` will generate 4 rows of `Pln_Orl`.

``` {bash}
datafiller --size=2 data/ddl_to_data_schema_01.sql > data/ddl_to_data_data_01.sql
``` 

``` {bash}
cat data/ddl_to_data_data_01.sql | sed -n "/COPY/,+2 p"
``` 

## v02: Use BIGINT Type for FK and PK Fields

This time I used BIGINT type for FK and PK fields instead of TEXT type.

``` {bash}
cat data/ddl_to_data_schema_02.sql
``` 

``` {bash}
datafiller --size=2 data/ddl_to_data_schema_02.sql > data/ddl_to_data_data_02.sql
``` 

``` {bash}
cat data/ddl_to_data_data_02.sql | sed -n "/COPY/,+2 p"
``` 

All BIGINT fields are left empty. It seems like datafiller does not support BIGINT properly.

## v03: Use INTEGER Type for FK and PK Fields

This time I used INTEGER type for FK and PK fields instead of TEXT type.

``` {bash}
cat data/ddl_to_data_schema_03.sql
``` 

``` {bash}
datafiller --size=2 data/ddl_to_data_schema_03.sql > data/ddl_to_data_data_03.sql
``` 

``` {bash}
cat data/ddl_to_data_data_03.sql | sed -n "/COPY/,+2 p"
``` 

This worked. So `datafiller` supports INTEGER properly.

## v04: Use Predefined Reference Ids as FK Data

In real world, I will have some predefined data in my database. I need to refer to them from foreign key fields that reference those data.

I will use the usual command `word` to reuse predefined list of values:

``` sql
-- df place_enum: word=./data/ddl_to_data_place_enum.txt
CREATE TABLE place (
  ...
  ,  place_enum INT
  -- df: text=place_enum length=1 
  ...
```

Note that, `place_enum` is not a FK managed by database server according to the above definition because it doesn't have `REFERENCES` clause yet.

Predefined reference ids are here:

``` {bash}
cat data/ddl_to_data_place_enum.txt
``` 

``` {bash}
cat data/ddl_to_data_schema_04.sql
``` 

``` {bash}
datafiller --size=5 data/ddl_to_data_schema_04.sql > data/ddl_to_data_data_04.sql
``` 

``` {bash}
cat data/ddl_to_data_data_04.sql | sed -n "/COPY/,+2 p"
``` 

This works. So, `word` directive can be used to refer to predefined rows from foreign key fields.

## v05: Use Predefined Reference Ids together with REFERENCES clause

I want to use `REFERENCES` clause of `SQL` in order to let the database server assure the integrity of foreign key data.

``` sql
-- df place_enum: word=./data/ddl_to_data_place_enum.txt
CREATE TABLE enum_value (
  enum_value_id INT 
  -- df: text=place_enum length=1 
  ) ;
CREATE TABLE place ( --df: mult=2.0
  ...
  ,  place_enum INT REFERENCES enum_value (enum_value_id)
  -- df: text=place_enum length=1 
  ...
```

``` {bash}
cat data/ddl_to_data_schema_05.sql
``` 

``` {bash}
datafiller --size=5 data/ddl_to_data_schema_05.sql > data/ddl_to_data_data_05.sql
``` 

``` {bash}
cat data/ddl_to_data_data_05.sql | sed -n "/COPY/,+2 p"
``` 

This works too. So, we can use `REFERENCES` clause such that database server assures data integrity between tables that are related to each other.

## v06: Generate Predefined Reference Id File From RDB

[RDB](/tech/what_is_rdb/) is an abbreviation that I use for requirements database. I try to store all software specifications as structured data in a relational database format. For example, I keep predefined data for my software inside these `RDB` files. Then I generate their `SQL INSERT` statements automatically and feed the database during deployment.

`RDB` files keep the predefined data such as enum values. This kind of data is at a higher knowledge level than other data. I will use `datafiller` to generate randomized data for all other tables.

We want to generate predefined reference ids from `RDB` data of the entity `enum_value` 

``` {r}
library(magrittr)
evl = tibble::tribble(
  ~enum_value_id,          ~enum_value_name, ~enum_var_id,
            1000,         "departure depot",            1,
            1001,            "return depot",            1,
            1002,               "pick stop",            1,
            1003,               "drop stop",            1,
            1004,          "pick/drop stop",            1,
            1005,  "departure/return depot",            1
  )
data_entities = list("enum_value" = evl)
get_entity_name_to_id = function(data_entities) {
  by_name_by_id = function(entity_name, data_entities) {
    id_var = sprintf("%s_id", entity_name)
    data_entities[[entity_name]][[id_var]]
  }
  lapply(names(data_entities), by_name_by_id, data_entities) %>%
    setNames(names(data_entities))
  # rutils::lnapply(data_entities, by_name_by_id) 
}
m = get_entity_name_to_id(data_entities)
writeLines(as.character(m$enum_value), "./data/ddl_to_data_enum_value_id.txt")
``` 

``` {bash}
cat data/ddl_to_data_schema_06.sql
``` 

Note that `word` attribute refers to the generated id file: `./data/ddl_to_data_enum_value_id.txt`

``` {bash}
datafiller --size=5 data/ddl_to_data_schema_06.sql > data/ddl_to_data_data_06.sql
``` 

``` {bash}
cat data/ddl_to_data_data_06.sql | sed -n "/COPY/,+2 p"
``` 

## v07: Filter Enum Values by Enum Variables

In my actual software, I define a single table for all nominal (enumerated) values which I call `enum_value`. All nominal variables which I call `enum_var` refer to these values. But different nominal variables have different nominal values. For example, `place_type` and `weight_unit` are two nominal variables whose values are elements of the sets `{"factory", "depot", "storehouse"}` and `{"kg", "ton", "g"}`. I don't want to define a new table for the value set of each `enum_var`. Therefore, I collect value sets of all different `enum_var`s inside one table which is `enum_value`.

For example, the following dataframe for `enum_value` contain values of 3 different `enum_var` instances.

``` {r}
evl = tibble::tribble(
  ~enum_value_id,          ~enum_value_name, ~enum_var_id,
            1000,         "departure depot",            1,
            1001,            "return depot",            1,
            1002,               "pick stop",            1,
            1003,               "drop stop",            1,
            1004,          "pick/drop stop",            1,
            1005,  "departure/return depot",            1,
            2000,               "pick/load",            2,
            2001,             "drop/unload",            2,
            3000,                   "depot",            3,
            3001,                 "factory",            3,
            3002,                "customer",            3
  )

evr = tibble::tribble(
  ~enum_var_id,     ~enum_var_name,
             1,  "route_stop_enum",
             2,      "action_enum",
             3,       "place_enum"
  )
``` 

`place_enum` field in the `place` table should contain those `enum_value` instances that belong to `place_enum` `enum_var`.

To assure this constraint, I will use `dplyr::filter` function before generating different predefined id reference files for datafiller.

``` {r}
get_enum_var_name_to_id = function(evl, evr) {
  get_enum_value_id_by = function(evn, evl_evr) {
    evl_evr %>%
      dplyr::filter(enum_var_name == evn) %>%
      magrittr::extract2("enum_value_id")
  }
  evl_evr = evl %>%
    dplyr::left_join(evr, by = "enum_var_id") 
  lapply(unique(evl_evr$enum_var_name), get_enum_value_id_by, evl_evr) %>%
    setNames(unique(evl_evr$enum_var_name))
}
m = get_enum_var_name_to_id(evl, evr)
m
for (evn in names(m)) {
  writeLines(as.character(m[[evn]]), sprintf("./data/ddl_to_data_%s.txt" , evn))
}
``` 

`writeLines` writes id values of enum variables into the following files:

``` {bash}
ls data/*_enum.txt
``` 

``` {bash}
cat data/ddl_to_data_schema_07.sql
``` 

Note that this time there are 2 different `word` attributes that refer to two separate reference id files: `./data/ddl_to_data_place_enum.txt` and `./data/ddl_to_data_action_enum.txt`

``` {bash}
datafiller --size=5 data/ddl_to_data_schema_07.sql > data/ddl_to_data_data_07.sql
``` 

``` {bash}
cat data/ddl_to_data_data_07.sql | sed -n "/COPY/,+2 p"
``` 

Note that, `place_enum` and `action_enum` values are sampled from their corresponding id reference files. 

## v08: Ordering of Tables: First Independent Tables Then Dependent Ones

`datafiller` gives `KeyError` when the order of the referenced tables is wrong.

For example, the following schema definition will give an error because `enum_var` is referred before its definition:

``` sql}
CREATE TABLE enum_value (
  enum_value_id INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY
  ,  enum_var_id INT NOT NULL REFERENCES enum_var (enum_var_id)
  ,  enum_value_name TEXT 
  ) ;
CREATE TABLE enum_var (
  enum_var_id INT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY
  ,  enum_var_name TEXT 
  ) ;
``` 


## Step 2: Importing Data into PostgreSQL Database

Now, we are going to start postgresql database server and import the generated data. As database server, I use a docker container published by https://github.com/subzerocloud/postgrest-starter-kit

``` 
docker start postgreststarterkit_db_1
``` 

I put user credentials for this database as environment variables into `.zshrc`:

``` bash
export PGHOST=localhost
export PGPORT=5432
export PGUSER=superuser
export PGDATABASE=app
```

Now, I need to run the files that contain `DDL` and `COPY` SQL commands using `psql` tool. I can run those files separately one by one. But instead I made a new `sql` file that first initializes database schema by dropping it and then running `DDL` and `COPY` files.

``` bash
psql -d app -h localhost -p 5432 -U superuser -f data/ddl_to_data_init_01.sql
``` 

``` {bash}
cat data/ddl_to_data_init_01.sql
```

### v09: Exclude some tables

Data for predefined tables comes from external sources. In order to exclude these tables from `datafiller` use `nogen` directive:

``` sql
CREATE TABLE enum_var ( -- df: nogen
``` 


