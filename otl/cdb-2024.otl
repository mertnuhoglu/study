TOC Refcard id=g10489
	ref
	  sqlite ~/projects/study/db/sqlite.otl
		SQLite <url:file:///~/projects/study/r/ex/examples_r/database/sqlite/sqlite01.rmd#r=g12949>
		~/projects/study/r/ex/examples_r/database/sqlite_datetime.rmd
	cdb.md <url:file:///~/projects/study/otl/cdb.otl>
		## Topics <url:file:///~/projects/study/otl/cdb.otl#r=g10304>
			Run db clients  <url:file:///~/projects/study/otl/cdb.otl#r=g10305>
			Database Administration <url:file:///~/projects/study/otl/cdb.otl#r=g10306>
			psql <url:file:///~/projects/study/otl/cdb.otl#r=g10307>
			Data Types <url:file:///~/projects/study/otl/cdb.otl#r=g10308>
			Constraints <url:file:///~/projects/study/otl/cdb.otl#r=g10309>
			Views <url:file:///~/projects/study/otl/cdb.otl#r=g10310>
			Window Functions <url:file:///~/projects/study/otl/cdb.otl#r=g10311>
			Common Table Expressions (CTE) <url:file:///~/projects/study/otl/cdb.otl#r=g10312>
			Functions <url:file:///~/projects/study/otl/cdb.otl#r=g10313>
			Builtin Functions <url:file:///~/projects/study/otl/cdb.otl#r=g10314>
			JOIN <url:file:///~/projects/study/otl/cdb.otl#r=g10315>
			Row Level Security (RLS) <url:file:///~/projects/study/otl/cdb.otl#r=g10316>
			ddl <url:file:///~/projects/study/otl/cdb.otl#r=g10320>
				examples <url:file:///~/projects/study/otl/cdb.otl#r=g10321>
		## PostgreSQL how tos  <url:file:///~/projects/study/otl/cdb.otl#r=g10325>
			select into = create table as <url:file:///~/projects/study/otl/cdb.otl#r=g10326>
			record type <url:file:///~/projects/study/otl/cdb.otl#r=g10327>
			change current_schema <url:file:///~/projects/study/otl/cdb.otl#r=g10328>
			set a password - store check passwords crypt() <url:file:///~/projects/study/otl/cdb.otl#r=g10329>
			single quote vs double quote for text variables <url:#r=adb_016>
			Difference between set, \set and \pset in psql <url:file:///~/projects/study/otl/cdb.otl#r=g10330>
			GRANT USAGE <url:file:///~/projects/study/otl/cdb.otl#r=g10331>
			PostgreSQL Recursive Query By Example <url:#r=adb_005>
			17 Practical psql Commands <url:file:///~/projects/study/otl/cdb.otl#r=g10332>
			PostgreSQL Cheat Sheet <url:file:///~/projects/study/otl/cdb.otl#r=g10333>
			PostgreSQL Basics by Example <url:file:///~/projects/study/otl/cdb.otl#r=g10334>
			Application users vs. Row Level Security  id=g10344 <url:file:///~/projects/study/otl/cdb.otl#r=g10344>
				<url:/Users/mertnuhoglu/projects/study/otl/cdb.otl#tn=ex: RLS>
		SOF PostgreSQL: Stackoverflow SOF Popular Questions   <url:file:///~/projects/study/otl/cdb.otl#r=g10238>
			ref - examples sof postgresql <url:file:///~/projects/study/otl/cdb.otl#r=g10706>
			PostgreSQL unnest() with element number <url:file:///~/projects/study/otl/cdb.otl#r=g10363>
			how people ask questions?  <url:file:///~/projects/study/otl/cdb.otl#r=g10345>
			Return setof record (virtual table) from function <url:file:///~/projects/study/otl/cdb.otl#r=g10346>
			Select first row in each GROUP BY group? <url:#r=adb_015>
			PostgreSQL Crosstab Query <url:#r=adb_010>
			Select rows which are not present in other table <url:file:///~/projects/study/otl/cdb.otl#r=g10347>
			How to implement a many-to-many relationship in PostgreSQL? <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10176>
			PostgreSQL 10 identity columns explained <url:file:///~/projects/study/otl/cdb.otl#r=g10348>
			Index for finding an element in a JSON array <url:file:///~/projects/study/otl/cdb.otl#r=g10349>
			Check if NULL exists in Postgres array <url:file:///~/projects/study/otl/cdb.otl#r=g10350>
			Fast way to discover the row count of a table in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10351>
			Double colon (::) notation in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10352>
			How to concatenate columns in a Postgres SELECT?  <url:file:///~/projects/study/otl/cdb.otl#r=g10353>
			Combine two columns and add into one new column <url:file:///~/projects/study/otl/cdb.otl#r=g10354>
			Any downsides of using data type “text” for storing strings?  <url:file:///~/projects/study/otl/cdb.otl#r=g10355>
			Change PostgreSQL columns used in views <url:file:///~/projects/study/otl/cdb.otl#r=g10356>
			Best way to check for “empty or null value” <url:file:///~/projects/study/otl/cdb.otl#r=g10357>
			Are PostgreSQL column names case-sensitive?  <url:file:///~/projects/study/otl/cdb.otl#r=g10358>
			Query a parameter (postgresql.conf setting) like “max_connections” <url:file:///~/projects/study/otl/cdb.otl#r=g10359>
			How to filter SQL results in a has-many-through relation <url:file:///~/projects/study/otl/cdb.otl#r=g10360>
			Export specific rows from a PostgreSQL table as INSERT SQL script <url:file:///~/projects/study/otl/cdb.otl#r=g10361>
			PostgreSQL sort by datetime asc, null first?  <url:file:///~/projects/study/otl/cdb.otl#r=g10362>
			Manual: WITH ORDINALITY <url:file:///~/projects/study/otl/cdb.otl#r=g10364>
			Concatenate multiple result rows of one column into one, group by another column [duplicate] <url:file:///~/projects/study/otl/cdb.otl#r=g10365>
			How to update selected rows with values from a CSV file in Postgres?  <url:file:///~/projects/study/otl/cdb.otl#r=g10366>
			Create PostgreSQL ROLE (user) if it doesn't exist <url:file:///~/projects/study/otl/cdb.otl#r=g10367>
			How to select id with max date group by category in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10368>
			Postgres FOR LOOP <url:file:///~/projects/study/otl/cdb.otl#r=g10369>
			What is the difference between LATERAL and a subquery in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10370>
			How do you declare a set-returning-function to only be allowed in the FROM clause? <url:#r=adb_008>
			Postgres - array for loop <url:file:///~/projects/study/otl/cdb.otl#r=g10371>
			Table name as a PostgreSQL function parameter <url:file:///~/projects/study/otl/cdb.otl#r=g10372>
			SQL Injection with user input <url:file:///~/projects/study/otl/cdb.otl#r=g10373>
			SQL injection in Postgres functions vs prepared queries <url:file:///~/projects/study/otl/cdb.otl#r=g10374>
			Postgresql manual: format()  id=g10375 <url:file:///~/projects/study/otl/cdb.otl#r=g10375>
			Sort NULL values to the end of a table <url:file:///~/projects/study/otl/cdb.otl#r=g10376>
			What are '$$' used for in PL/pgSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10377>
			Creating temporary tables in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10378>
			Grant all on a specific schema in the db to a group role in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10379>
			How to return result of a SELECT inside a function in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10380>
			PostgreSQL: running count of rows for a query 'by minute' <url:#r=adb_014>
			Calculating Cumulative Sum in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10381>
			Split comma separated column data into additional columns <url:file:///~/projects/study/otl/cdb.otl#r=g10382>
			PostgreSQL IF statement <url:file:///~/projects/study/otl/cdb.otl#r=g10383>
			Store common query as column <url:file:///~/projects/study/otl/cdb.otl#r=g10384>
			Refactor a PL/pgSQL function to return the output of various SELECT queries  id=g10385 <url:file:///~/projects/study/otl/cdb.otl#r=g10385>
			Finding similar strings with PostgreSQL quickly <url:file:///~/projects/study/otl/cdb.otl#r=g10386>
			Best way to delete millions of rows by ID <url:file:///~/projects/study/otl/cdb.otl#r=g10387>
			Find overlapping date ranges in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10388>
			Many to Many Table - Performance is bad <url:file:///~/projects/study/otl/cdb.otl#r=g10389>
			GROUP BY + CASE statement <url:file:///~/projects/study/otl/cdb.otl#r=g10390>
			Store the query result in variable using postgresql Stored procedure <url:file:///~/projects/study/otl/cdb.otl#r=g10391>
			Shell script to execute pgsql commands in files <url:file:///~/projects/study/otl/cdb.otl#r=g10392>
			Is there something like a zip() function in PostgreSQL that combines two arrays?  <url:file:///~/projects/study/otl/cdb.otl#r=g10393>
			Two SQL LEFT JOINS produce incorrect result <url:file:///~/projects/study/otl/cdb.otl#r=g10394>
			Alphanumeric sorting with PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10395>
			How to create simple fuzzy search with Postgresql only?  <url:file:///~/projects/study/otl/cdb.otl#r=g10396>
			PostgreSQL - GROUP BY clause or be used in an aggregate function <url:file:///~/projects/study/otl/cdb.otl#r=g10397>
			How do you find results that occurred in the past week?  <url:file:///~/projects/study/otl/cdb.otl#r=g10398>
			GROUP BY and COUNT in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10399>
			How to convert “string” to “timestamp without time zone” <url:file:///~/projects/study/otl/cdb.otl#r=g10400>
			Is there a postgres command to list/drop all materialized views?  <url:file:///~/projects/study/otl/cdb.otl#r=g10401>
			Return setof record (virtual table) from function <url:file:///~/projects/study/otl/cdb.otl#r=g10402>
			Why does PostgreSQL not return null values when the condition is <> true <url:file:///~/projects/study/otl/cdb.otl#r=g10403>
			INSERT rows into multiple tables in a single query, selecting from an involved table <url:file:///~/projects/study/otl/cdb.otl#r=g10404>
			Select today's (since midnight) timestamps only <url:file:///~/projects/study/otl/cdb.otl#r=g10405>
			Join table twice - on two different columns of the same table <url:file:///~/projects/study/otl/cdb.otl#r=g10406>
			The forgotten assignment operator “=” and the commonplace “:=” <url:file:///~/projects/study/otl/cdb.otl#r=g10407>
			ORDER BY the IN value list <url:file:///~/projects/study/otl/cdb.otl#r=g10408>
			How to delete duplicate entries?  <url:file:///~/projects/study/otl/cdb.otl#r=g10409>
			Multiple left joins on multiple tables in one query <url:file:///~/projects/study/otl/cdb.otl#r=g10410>
			Is there a way to define a named constant in a PostgreSQL query?  <url:file:///~/projects/study/otl/cdb.otl#r=g10411>
			Get month name from number in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10412>
			Selecting records between two timestamps <url:file:///~/projects/study/otl/cdb.otl#r=g10413>
			Pass In “WHERE” parameters to PostgreSQL View?  <url:file:///~/projects/study/otl/cdb.otl#r=g10414>
			How to export a PostgreSQL query output to a csv file <url:file:///~/projects/study/otl/cdb.otl#r=g10415>
			Difference between language sql and language plpgsql in PostgreSQL functions <url:file:///~/projects/study/otl/cdb.otl#r=g10416>
			Inlining of SQL Functions  id=g10417 <url:file:///~/projects/study/otl/cdb.otl#r=g10417>
			Return SETOF rows from PostgreSQL function <url:file:///~/projects/study/otl/cdb.otl#r=g10418>
			SQL update fields of one table from fields of another one <url:file:///~/projects/study/otl/cdb.otl#r=g10419>
			Difference between LIKE and ~ in Postgres <url:file:///~/projects/study/otl/cdb.otl#r=g10420>
			Postgres window function and group by exception <url:file:///~/projects/study/otl/cdb.otl#r=g10421>
			IN vs ANY operator in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10422>
			How to use ANY instead of IN in a WHERE clause with Rails?  <url:file:///~/projects/study/otl/cdb.otl#r=g10423>
			How to get the trigger(s) associated with a view or a table in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10424>
			Multiple CTE in single query <url:file:///~/projects/study/otl/cdb.otl#r=g10425>
			Find difference between two big tables in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10426>
			Guidance on using the WITH clause in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10427>
			How to use a SQL window function to calculate a percentage of an aggregate <url:file:///~/projects/study/otl/cdb.otl#r=g10428>
			Nesting queries in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10429>
			PostgreSQL aggregate or window function to return just the last value <url:file:///~/projects/study/otl/cdb.otl#r=g10430>
			Create a pivot table with PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10431>
			Generate id row for a view with grouping <url:file:///~/projects/study/otl/cdb.otl#r=g10432>
			Aggregate strings in descending order in a PostgreSQL query <url:file:///~/projects/study/otl/cdb.otl#r=g10433>
			How can I test if a column exists in a table using an SQL statement <url:file:///~/projects/study/otl/cdb.otl#r=g10434>
			How to join tables on regex <url:file:///~/projects/study/otl/cdb.otl#r=g10435>
			How to Write SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10436>
			SQL Regression Tests <url:file:///~/projects/study/otl/cdb.otl#r=g10437>
			Exploring a Data Set in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10438>
		postgrest <url:file:///~/projects/study/otl/cdb.otl#r=g10337>
			postgrest
				Postgrest Manual <url:file:///~/projects/study/otl/cdb.otl#r=g10158>
					Authentication Sequence <url:file:///~/projects/study/otl/cdb.otl#r=g10448>
						Tables and Views <url:file:///~/projects/study/otl/cdb.otl#r=g10440>
							horizontal filtering (rows) <url:file:///~/projects/study/otl/cdb.otl#r=g10441>
							vertical filtering (columns) <url:file:///~/projects/study/otl/cdb.otl#r=g10442>
							limits and pagination <url:file:///~/projects/study/otl/cdb.otl#r=g10443>
							response format <url:file:///~/projects/study/otl/cdb.otl#r=g10444>
						resource embedding <url:file:///~/projects/study/otl/cdb.otl#r=g10445>
						stored procedures <url:file:///~/projects/study/otl/cdb.otl#r=g10446>
						insertions/updates <url:file:///~/projects/study/otl/cdb.otl#r=g10447>
					API <url:file:///~/projects/study/otl/cdb.otl#r=g10439>
				postgrest <url:file:///~/projects/study/otl/cdb.otl#r=g10317>
			Postgrest Starter Kit 
				Postgrest Starter Kit Manual <url:#r=adb_003>
					Iterative Development Workflow <url:file:///~/projects/study/otl/cdb.otl#r=g10449>
					API Core <url:file:///~/projects/study/otl/cdb.otl#r=g10450>
						Schemas  <url:file:///~/projects/study/otl/cdb.otl#r=g10451>
						Securing your API <url:file:///~/projects/study/otl/cdb.otl#r=g10452>
						Using the API <url:file:///~/projects/study/otl/cdb.otl#r=g10453>
					Tutorial steps - postgrest starter kit <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10159>
				postgrest_starter_kit (psk) <url:file:///~/projects/study/otl/cdb.otl#r=g10318>
				refcard psk <url:file:///~/projects/study/otl/cdb.otl#r=g10492>
				postgrest_starter_kit (psk): source code review <url:file:///~/projects/study/otl/cdb.otl#r=g10319>
				study_postgrest_starter_kit.Rmd: SQL functions: array_to_json array_agg row_to_json <url:file:///~/projects/study/otl/cdb.otl#r=g10340>
				tests psk <url:file:///~/projects/study/otl/cdb.otl#r=g10494>
			articles
				Stop calling PostgREST “MAGIC”!  <url:file:///~/projects/study/otl/cdb.otl#r=g10338>
					Authentication / Authorization - postgrest <url:file:///~/projects/study/otl/cdb.otl#r=g10339>
					But why (do we need PostgREST)?  <url:file:///~/projects/study/otl/cdb.otl#r=g10341>
				HN: PostgREST – A fully RESTful API from any existing PostgreSQL database  <url:file:///~/projects/study/otl/cdb.otl#r=g10342>
				Swagger <url:file:///~/projects/study/otl/cdb.otl#r=g10343>
		Tools <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10175>
			sample data <url:file:///~/projects/study/otl/cdb.otl#r=g10454>
				ddlgenerator: generate ddl from data <url:file:///~/projects/study/otl/cdb.otl#r=g10455>
				datafiller: sample data generator <url:file:///~/projects/study/otl/cdb.otl#r=g10456>
		Modern SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10465>
			LATERAL <url:file:///~/projects/study/otl/cdb.otl#r=g10466>
			GROUPING SETS <url:file:///~/projects/study/otl/cdb.otl#r=g10467>
			WITH (common table expressions CTE) <url:file:///~/projects/study/otl/cdb.otl#r=g10468>
			WITH RECURSIVE <url:file:///~/projects/study/otl/cdb.otl#r=g10469>
			OVER and PARTITION BY <url:file:///~/projects/study/otl/cdb.otl#r=g10470>
			OVER and ORDER BY <url:file:///~/projects/study/otl/cdb.otl#r=g10471>
			WITHIN GROUP <url:file:///~/projects/study/otl/cdb.otl#r=g10472>
			OVER <url:file:///~/projects/study/otl/cdb.otl#r=g10473>
			FETCH FIRST <url:file:///~/projects/study/otl/cdb.otl#r=g10474>
			OFFSET <url:file:///~/projects/study/otl/cdb.otl#r=g10475>
			AS OF <url:file:///~/projects/study/otl/cdb.otl#r=g10476>
			LIST_AGG <url:file:///~/projects/study/otl/cdb.otl#r=g10477>
			Literate SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10478>
			case — Conditional Expressions <url:file:///~/projects/study/otl/cdb.otl#r=g10479>
			EXTRACT expression <url:file:///~/projects/study/otl/cdb.otl#r=g10480>
			FILTER clause <url:file:///~/projects/study/otl/cdb.otl#r=g10481>
			The FILTER clause in Postgres 9.4 <url:file:///~/projects/study/otl/cdb.otl#r=g10482>
			Table Column Aliases <url:file:///~/projects/study/otl/cdb.otl#r=g10485>
			SELECT without FROM <url:file:///~/projects/study/otl/cdb.otl#r=g10486>
			The Three-Valued Logic of SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10488>
		## Other <url:file:///~/projects/study/otl/cdb.otl#r=g10457>
			quotes <url:file:///~/projects/study/otl/cdb.otl#r=g10458>
			interval usage <url:file:///~/projects/study/otl/cdb.otl#r=g10459>
			SQL Joins with On or Using <url:file:///~/projects/study/otl/cdb.otl#r=g10460>
			Set Operations <url:file:///~/projects/study/otl/cdb.otl#r=g10461>
			Filtered Aggregates <url:file:///~/projects/study/otl/cdb.otl#r=g10462>
			Subquery Examples <url:file:///~/projects/study/otl/cdb.otl#r=g10463>
			Optimize GROUP BY query to retrieve latest record per user <url:#r=adb_007>
			Unused index in range of dates query <url:file:///~/projects/study/otl/cdb.otl#r=g10464>
			ex: get number of countries with GDP higher than 40000 for each year <url:file:///~/projects/study/otl/cdb.otl#r=g10483>
			filter by count of records <url:file:///~/projects/study/otl/cdb.otl#r=g10484>
			Use the Index Luke <url:file:///~/projects/study/otl/cdb.otl#r=g10487>
	articles_db.md <url:file:///~/gdrive/mynotes/content/articles/articles_db.md>
		PostgreSQL 9.6.5 Manual <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10156>
			4.2 Value Expressions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10250>
				positional parameters <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10249>
			5.7. Row Security Policies  id=g10251 <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10251>
				<url:/Users/mertnuhoglu/gdrive/mynotes/content/articles/articles_db.md#tn=ex: only "managers" role can access rows and only rows of their accounts:>
			Chapter 8. Data Types <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10252>
				8.5.1. Date/Time Input <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10253>
				8.16. Composite Types <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10737>
			Chapter 9. Functions and Operators <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10254>
				9.2. Comparison Operators <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10255>
				9.4. String Functions and Operators id=g10256
				9.7. Pattern Matching <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10257>
				9.8. Data Type Formatting Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10258>
				9.9. Date/Time Functions and Operators <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10259>
				9.10. Enum Support Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10260>
				9.16. Conditional Expressions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10261>
					9.16.2. COALESCE <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10262>
				9.17. Array Functions and Operators <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10263>
				9.19. Range Functions and Operators <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10264>
				9.18. Aggregate Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10265>
				9.19. Window Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10266>
				9.20. Subquery Expressions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10267>
				9.22. Set Returning Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10268>
				9.23. System Information Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10269>
				9.24. System Administration Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10270>
			Chapter 20. database Roles and Privileges <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10335>
				20.4. Role Membership <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10336>
			Chapter 21. Database Roles <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10271>
				21.3 Role Membership <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10272>
			chapter 35. The Information Schema <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10273>
			Chapter 36. Extending SQL <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10736>
			Part VI. Reference: SQL Commands <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10274>
		PostgreSQL Up and Running 1491963417 <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10157>
			chapter 2. database administration <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10276>
				Roles <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10275>
				Using Schemas <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10277>
				Privileges <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10278>
				GRANT <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10279>
				Backup and Restore <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10280>
			chapter 3. psql <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10281>
				Environment Variables <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10282>
				Interactive vs Noninteractive psql <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10283>
				psql Customizations <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10284>
				Importing and Exporting Data <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10285>
			chapter 4. Using pgAdmin <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10286>
			chapter 5. Data Types <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10287>
				Regex <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10288>
				Arrays <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10289>
				Range Types <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10290>
			chapter 6. Tables, Constraints and Indexes <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10291>
				Tables <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10292>
				Constraints <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10293>
			chapter 7. SQL: The PostgreSQL Way <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10294>
				Views <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10295>
				Handy Constructions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10296>
				Window Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10297>
				Common Table Expressions (CTE) <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10298>
			chapter 8. Writing Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10299>
				Aggregates <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10300>
				Writing Functions with SQL <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10301>
				Writing PL/pgSQL Functions <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10302>
			chapter 10. Replication and External Data <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10303>
Topics id=g10304
	ref
		<url:file:///~/projects/study/pg/postgrest02/README.md>
		<url:file:///~/gdrive/mynotes/content/articles/articles_db.md>
		<url:file:///~/projects/itr/vrp_doc/project_docs/doc_postgresql_with_R.Rmd>
	points - db
		clone duplicate schema
			https://wiki.postgresql.org/wiki/Clone_schema
				pgsql
					CREATE OR REPLACE FUNCTION clone_schema(source_schema text, dest_schema text) RETURNS void AS
					$BODY$
					DECLARE 
						objeto text;
						buffer text;
					BEGIN
							EXECUTE 'CREATE SCHEMA ' || dest_schema ;
					 
							FOR objeto IN
									SELECT TABLE_NAME::text FROM information_schema.TABLES WHERE table_schema = source_schema
							LOOP        
									buffer := dest_schema || '.' || objeto;
									EXECUTE 'CREATE TABLE ' || buffer || ' (LIKE ' || source_schema || '.' || objeto || ' INCLUDING CONSTRAINTS INCLUDING INDEXES INCLUDING DEFAULTS)';
									EXECUTE 'INSERT INTO ' || buffer || '(SELECT * FROM ' || source_schema || '.' || objeto || ')';
							END LOOP;
					END;
					$BODY$
					LANGUAGE plpgsql VOLATILE;
				running
					SELECT clone_schema('old_schema','new_schema');
		size of disk space usage in database id=g10735
			size of disk space usage in database <url:file:///~/projects/study/otl/cdb.otl#r=g10735>
			ref
				~/projects/bizqualify/BQ-data-run/datarun/disk_cleanup_20181208.md
			Total free disk space on file system:
				df -H
			database size
				SELECT d.datname AS Name,  pg_catalog.pg_get_userbyid(d.datdba) AS Owner,
						CASE WHEN pg_catalog.has_database_privilege(d.datname, 'CONNECT')
								THEN pg_catalog.pg_size_pretty(pg_catalog.pg_database_size(d.datname))
								ELSE 'No Access'
						END AS SIZE
				FROM pg_catalog.pg_database d
						ORDER BY
						CASE WHEN pg_catalog.has_database_privilege(d.datname, 'CONNECT')
								THEN pg_catalog.pg_database_size(d.datname)
								ELSE NULL
						END DESC -- nulls first
						LIMIT 20;
			biggest relations
				SELECT nspname || '.' || relname AS "relation",
						pg_size_pretty(pg_relation_size(C.oid)) AS "size"
					FROM pg_class C
					LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
					WHERE nspname NOT IN ('pg_catalog', 'information_schema')
					ORDER BY pg_relation_size(C.oid) DESC
					LIMIT 20;
			size of schema
				CREATE OR REPLACE FUNCTION pg_schema_size(text) RETURNS BIGINT AS $$
				SELECT SUM(pg_total_relation_size(quote_ident(schemaname) || '.' || quote_ident(tablename)))::BIGINT FROM pg_tables WHERE schemaname = $1
				$$ LANGUAGE SQL;
				SELECT pg_size_pretty(pg_schema_size('data_20181115_v3b'));
		quotes id=g10458
			quotes <url:file:///~/projects/study/otl/cdb.otl#r=g10458>
			[S]ingle quote for [S]trings, [D]ouble quote for things in the [D]atabase
			single quote vs double quote for text variables <url:#r=adb_016>
				double quotes used only for quoted identifiers
			yes: All identifiers (including column names) that are not double-quoted are folded to lower case in PostgreSQL
				SELECT * FROM persons WHERE "first_Name" = 'xyz';
				Values (string literals) are enclosed in single quotes.
			No need for double-quoting perfectly legal identifiers. Double-quotes are only needed to force otherwise illegal names (mixed-case, illegal characters or reserved words).
			escaping single quotes
				double ': ''
				'user''s log'
		interval usage id=g10459
			interval usage <url:file:///~/projects/study/otl/cdb.otl#r=g10459>
			ex
				+ date '2001-09-28' + interval '1 hour' timestamp '2001-09-28 01:00:00'
				SELECT '2012-03-11 3:10 AM'::timestamp + '1 hour'::interval
				SELECT '23 hours'::interval + '1 hour'::interval
				SELECT extract(epoch from now() + '5 minutes'::interval) :: integer;
				OR published > now() - interval '1 day'
			A Comprehensive Look at PostgreSQL Interval Data Type
				http://www.postgresqltutorial.com/postgresql-interval/
				syntax
					@ interval [ fields  ] [ (p)  ]   
				optional: @, (p)
					p: precision: 0 to 6 
						fraction digits
				ex
					interval '2 months ago';
					interval '3 hours 20 minutes';
				internal storage
					interval values stored as months, days, and seconds
					seconds can have fractions
		docker postgres id=g10547
			docker postgres <url:file:///~/projects/study/otl/cdb.otl#r=g10547>
			setup logging and postgresql.conf on docker postgres id=g10549
				setup logging and postgresql.conf on docker postgres <url:file:///~/projects/study/otl/cdb.otl#r=g10549>
				https://stackoverflow.com/questions/30848670/how-to-customize-the-configuration-file-of-the-official-postgresql-docker-image/41912295#41912295
					permission to write to log dir
						chgroup ./logs docker && chmod 770 ./logs
					docker.compose.yml
						db:
							 image: postgres:9.6.1
							 # Make Postgres log to a file.
							 # More on logging with Postgres: https://www.postgresql.org/docs/current/static/runtime-config-logging.html
							 command: postgres -c logging_collector=on -c log_destination=stderr -c log_directory=/logs -c config_file=/etc/postgresql.conf
							 environment:
								 # Provide the password via an environment variable. If the variable is unset or empty, use a default password
								 - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-4WXUms893U6j4GE&Hvk3S*hqcqebFgo!vZi}
							 # If on a non-Linux OS, make sure you share the drive used here. Go to Docker's settings -> Shared Drives
							 volumes:
								 # Persist the data between container invocations
								 - postgresVolume:/var/lib/postgresql/data
								 - ./logs:/logs
								 - ./customPostgresql.conf:/etc/postgresql.conf
							 networks:
								 myApp-network:
									 # Our application can communicate with the database using this hostname
									 aliases:
										 - postgresForMyApp
						# Creates a named volume to persist our data. When on a non-Linux OS, the volume's data will be in the Docker VM
						# (e.g., MobyLinuxVM) in /var/lib/docker/volumes/
						volumes:
							postgresVolume:
			tries docker postgresql.conf for logging  id=g10548
				tries docker postgresql.conf for logging  <url:file:///~/projects/study/otl/cdb.otl#r=g10548>
				doc - dersler
					dc up -d yapma, çünkü -d docker'ın çalışma sırasındaki logları gizliyor +
					postgresql.conf dosyasında "" kullanma '' kullan +
					docker rm etmeye gerek yok, dc stop up yeterli +
					sadece postgresql.conf dosyası data/ klasöründe bulunursa, postgresql başlamıyor. data/ klasörünün boş olmasını şart koşuyor
				try03:
					# ~/projects/itr/itr_docker/docker/postgresql01/log_try03.log
					cd /Users/mertnuhoglu/projects/itr/itr_docker/docker/postgresql01
					# docker-compose.yml contains only db
						- "./data:/var/lib/postgresql/data"
					# no data dir at start
				try04: start with complete data/ dir but default .conf
					# ~/projects/itr/itr_docker/docker/postgresql01/log_try04.log
					# after try03 
						dc stop
						docker rm postgresql01_db_1
						dc up
				try05: change docker-compose.yml but don't rm container
					# ~/projects/itr/itr_docker/docker/postgresql01/log_try05.log
					# after try04
						dc stop
						edit docker.compose.yml
							- "./data2:/var/lib/postgresql/data"
				try06: don't stop container but dc up again
					# after try05
						dc up
					# existing docker is untouched
				try07: don't stop container. change docker-compose.yml and dc up again
					# after try06
						edit docker.compose.yml
							- "./data3:/var/lib/postgresql/data"
						dc up
					# existing docker is stopped and a new one is started
					# data3/ dir is created and filled
				try08: don't stop. update postgresql.conf and dc up again
					# after try07
						edit postgresql.conf
							log_destination = 'stderr'
							logging_collector = on
							log_directory = 'pg_log'		# directory where log files are written,
							log_filename = 'postgresql-%Y-%m-%d.log'
							log_statement = 'all'
						dc up
					# existing docker is untouched
				try09: stop. update postgresql.conf and dc up again
					# after try08
						dc stop
						dc up
					# ~/projects/itr/itr_docker/docker/postgresql01/log_try09.log
						db_1  | LOG:  syntax error in file "/var/lib/postgresql/data/postgresql.conf" line 332, near token """
							log_destination = "stderr"
							->
							log_destination = 'stderr'
					# docker exited
				try10: fixed postgresql.conf. changed "" to ''
					# after try09
						dc up
					# ~/projects/itr/itr_docker/docker/postgresql01/log_try10.log
					# runs as expected
				try11: data/ folder contains only postgresql.conf
					dc stop db
					mkdir -p data4/postgresql.conf && rsync -a data3/postgresql.conf data4/postgresql.conf
					# edit docker-compose.yml
						- "./data4:/var/lib/postgresql/data"
					# error: ~/projects/itr/itr_docker/docker/postgresql01/log_try11.log
						db_1  | initdb: directory "/var/lib/postgresql/data" exists but is not empty
				try12: volumes refs postgresql.conf and pg_log
					# after try11
					docker rm postgresql01_db_1
					mkdir -p data5/postgresql.conf && rsync -a data3/postgresql.conf data5/postgresql.conf
					# edit docker-compose.yml
						- "./data5/postgresql.conf:/var/lib/postgresql/data/postgresql.conf"
						- "./data5/pg_log:/var/lib/postgresql/data/pg_log"
					dc up db
					# error: 
						db_1  | initdb: directory "/var/lib/postgresql/data" exists but is not empty
				try13: volumes refs postgresql.conf only
					# after try12
					docker rm postgresql01_db_1
					mkdir -p data5/postgresql.conf && rsync -a data3/postgresql.conf data5/postgresql.conf
					# edit docker-compose.yml
						- "./data5/postgresql.conf:/var/lib/postgresql/data/postgresql.conf"
					dc up db
					# error: 
						db_1  | initdb: directory "/var/lib/postgresql/data" exists but is not empty
				try14: command: postgres -c ile logging_collector aktifleştir
					# docker-compose.yml
						command: postgres -c logging_collector=on -c log_destination=stderr -c log_directory=/pg_log -c log_filename='postgresql-%Y-%m-%d.log' -c log_statement=all
						volumes:
							#- postgresql01:/var/lib/postgresql/data
							- ./logs:/pg_log
					# runs well
		order of columns matters in INSERT and SELECT id=g10662
			order of columns matters in INSERT and SELECT <url:file:///~/projects/study/otl/cdb.otl#r=g10662>
			ex:
				INSERT INTO ... SELECT ... FROM ...
			ex: type mismatch error caused by different orders in two tables
				ref: <url:/Users/mertnuhoglu/projects/bizqualify/BQ-data-run/datarun/debug_missing_data_20181027.md#tn=Type Mismatch in INSERT INTO ... SELECT .. FROM ...>
	pg_dump
		export database schema
			pg_dump -s <database> > <export_file>
				-s --schema-only
				-Fc		custom format: allows for selection and is compressed by default
				-T --exclude-table=<table>
	Run db clients  id=g10305
		Run db clients  <url:file:///~/projects/study/otl/cdb.otl#r=g10305>
		docker start postgreststarterkit_db_1
			opt
				sudo docker run --name tutorial -p 5432:5432 ... -d postgres
		DataGrip > postgrest_starter_kit superuser@docker
			DB_NAME=app
			DB_SCHEMA=api
			SUPER_USER=superuser
			SUPER_USER_PASSWORD=superuserpass
		franchise - notebook
			npx franchise-client@0.2.7
			https://franchise.cloud/app/
		psql -d app -h localhost -p 5432 -U superuser
			psql -d app -U superuser -h localhost -w
			set search_path = data, public;
			psql postgresql://superuser:superuserpass@localhost/app
				psql postgresql://authenticator:authenticatorpass@localhost/app
		environment variables
			export PGHOST=localhost
			export PGPORT=5432
			export PGUSER=superuser
			export PGDATABASE=app
			psql
		jupyter
			%load_ext sql
			%%sql postgresql://superuser:superuserpass@localhost/app
			select * from api.todos;
			%sql select 1;
		pgcli
			pgcli postgresql://superuser:superuserpass@localhost/app
			pgcli app
			set search_path = data, public;
		db create
			psql -c 'CREATE DATABASE library OWNER = superuser'
		R dbplyr
			con <- DBI::dbConnect(RPostgreSQL::PostgreSQL()
				, user = Sys.getenv("SUPER_USER")
				, password = Sys.getenv("SUPER_USER_PASSWORD")
				, dbname = "app"
				, host = "localhost"
				, port = "5432"
			)
			df = DBI::dbGetQuery(con, "SELECT * FROM data.client")
		vim :DB
			DB g:prod = postgresql://superuser:superuserpass@localhost/app
			DB g:prod select * from data.client
	Database Administration id=g10306
		Database Administration <url:file:///~/projects/study/otl/cdb.otl#r=g10306>
		Roles
			Roles
				CREATE ROLE name;
				GRANT group_role TO role1, ...;
				REVOKE group_role TO role1, ...;
				CREATE ROLE joe LOGIN INHERIT;
				CREATE ROLE admin NOINHERIT;
				SET ROLE wheel;
				ALTER ROLE john LOGIN
			CREATE ROLE leo LOGIN PASSWORD 'king' CREATEDB VALID UNTIL 'infinity'
			CREATE ROLE royalty INHERIT
			GRANT group_role TO leo  
			SET ROLE royalty
			CREATE DATABASE mydb
		Information Schema
			select table_name FROM information_schema.tables;
				all tables in pgs
				~/projects/study/pg/postgresql/select_table_name_FROM_information_schem.tsv
					pg_statistic
					pg_type
					...
			SELECT * FROM information_schema.columns WHERE table_name = 'pg_database';
				| table_catalog | table_schema | table_name  | column_name | ... |
				| app           | pg_catalog   | pg_database | datname     | ... |
			select * from information_schema.information_schema_catalog_name;
				catalog_name
				---
				app
			select * from information_schema.schemata;
				catalog_name  schema_name schema_owner  ...
				app pg_toast  postgres        
				app pg_temp_1 postgres        
			select * from information_schema.sequences;
				sequence_catalog  sequence_schema sequence_name ...
				---
				app data  user_id_seq bigint  64  2 0 1 1 9223372036854775807 1 NO
				app data  todo_id_seq bigint  64  2 0 1 1 9223372036854775807 1 NO
		Extensions
			CREATE SCHEMA my_extensions
			ALTER DATABASE mydb SET search_path = '"$user", public, my_extensions'
			SELECT name FROM pg_available_extensions WHERE installed_version IS NOT NULL ORDER BY name;
				name
				--
				pgcrypto
				plpgsql
			CREATE EXTENSION fuzzystrmatch SCHEMA my_extensions
		GRANT
			GRANT some_privilege TO some_role
			GRANT ALL ON ALL TABLES IN SCHEMA public TO mydb_admin WITH GRANT OPTION
			GRANT SELECT, REFERENCES, TRIGGER ON ALL TABLES IN SCHEMA my_schema TO PUBLIC
			GRANT SELECT, UPDATE ON ALL SEQUENCES IN SCHEMA my_schema TO PUBLIC
			GRANT USAGE ON SCHEMA my_schema TO PUBLIC
			ALTER DEFAULT PRIVILEGES IN SCHEMA my_schema GRANT SELECT, REFERENCES ON TABLES TO PUBLIC
			ALTER DEFAULT PRIVILEGES IN SCHEMA my_schema GRANT ALL ON TABLES TO MYDB_ADMIN WITH GRANT OPTION
			GRANT USAGE ON SCHEMA
		Backup and Restore
			pg_dump
			pg_dumpall
			~/.pgpass
			pg_dump -h localhost -p 5432 -U someuser -F c -b -v -f mydb.backup mydb
			... -F c -b -v -t *.pay* -f pay.backup mydb
			... -F c -b -v -N public -f all_schema_except_public.backup mydb
			... -F p --column-inserts -f select_tables.backup mydb
			... -F d -f /path/dir mydb
			pg_dumpall ... -f myglobals.sql --globals-only
			psql -U postgres -f myglobals.sql
			psql -U postgres --set ON_ERROR_STOP=on -f myglobals.sql
			psql -U postgres -d mydb -f myglobals.sql
			CREATE DATABASE mydb
				pg_restore --dbname=mydb --jobs=4 --verbose mydb.backup
				--section=pre-data
		Tablespaces
			pg_default
			pg_global
			CREATE TABLESPACE secondary LOCATION 'path'
			ALTER DATABASE mydb SET TABLESPACE secondary
		postgresql.conf
			# note: always use '' never "" for conf settings
			SELECT *  FROM pg_settings ORDER BY context, name;
				SELECT____FROM_pg_settings_ORDER_BY_cont.csv
				name  setting unit  category  short_desc  extra_desc  context vartype source  min_val max_val enumvals  boot_val  reset_val sourcefile  sourceline  pending_restart
				---
				ignore_system_indexes off   Developer Options Disables reading from system indexes. It does not prevent updating the indexes, so it is safe to use.  The worst consequence is slowness. backend bool  default       off off     false
				post_auth_delay 0 s Developer Options Waits N seconds on connection startup after authentication. This allows attaching a debugger to the process.  backend integer default 0 2147    0 0     false
	install/uninstall postgres
		https://gist.github.com/Atlas7/b1a40a2ffd85728b33e7
			brew uninstall postgresql
		install psql only
			https://stackoverflow.com/questions/44654216/correct-way-to-install-psql-without-full-postgres-on-macos
				brew install libpq
	other
		CREATE OR REPLACE
		SELECT 'CREATE TABLE
			staging.count_to_50(array_to_string(array_agg('x' || i::text ' varchar(10)));' As create_sql
			FROM generate_series(1,9) As i;
	psql id=g10307
		psql <url:file:///~/projects/study/otl/cdb.otl#r=g10307>
		https://www.postgresql.org/docs/current/static/app-psql.html
		psql -d database -U  user -W
		psql -h <host> -p <port> -U <username> -W <password> <database>
			PGPASSWORD=superuserpass psql -h localhost -U superuser app
		environment variables
			ref
				https://www.postgresql.org/docs/9.3/static/libpq-envars.html
			ex
				export PGHOST=localhost
				export PGPORT=5432
				export PGUSER=$DB_USER
				export PGDATABASE=$DB_NAME
				export PGPASSWORD=$DB_PASS
		meta-commands (backslash commands) from prompt
			\c <dbname> <username>
			list tables: \dt
				more: \dt+
				\dt schema.*
			\i file.sql;
				run file.sql
			describe a table: \d <table_name>
			list schemas: \dn
			list functions: \df
			list views: \dv
				\dv schema.*
			list users and roles: \du
			save command history: \s <filename>
			exec commands from file: \i <filename>
			help on statement: \h <ALTER TABLE>
			edit command in editor: \e
			\l                 # list databases
			\set AUTOCOMMIT off
			\timing
			\set eav 'EXPLAIN ANALYZE VERBOSE'
				:eav SELECT COUNT(*) FROM pg_tables
			\set HISTSIZE 10
			\! ls
			\copy
			\connect postgresql_book
				\cd /postgresql_book/ch03
				\copy staging.factfinder_import FROM DEC_..csv CSV NULL AS '' DELIMITER '|' FROM somefile.txt
			\connect postgresql_book
				\copy (SELECT * FROM staging.factfinder_import WHERE s01 ~ E'^[0-9]+') TO '/test.tab' WITH DELIMITER E'\t' CSV HEADER
		errors/issues
			no output inside psql: if no semi-colon is used id=g10493
				no output inside psql: if no semi-colon is used <url:file:///~/projects/study/otl/cdb.otl#r=g10493>
				app=# select * from data.todo
				app=# select * from data.todo;
				 id |  todo  | private | owner_id
				----+--------+---------+----------
					1 | item_1 | f       |        1
					2 | item_2 | t       |        1
	Data Types id=g10308
		Data Types <url:file:///~/projects/study/otl/cdb.otl#r=g10308>
		Serials
			CREATE SEQUENCE
			nextval(sequence_name)
		String
			concat: ||
			extracting: substring
			split_part("text.sub", ".", 1) 
			SELECT unnest(string_to_array("split.this", ".")) As x
		regex
			regexp_replace('text', 'regex', E'replace')
			backreferences: \\1
			unnest(regexp_matches('text', 'match', 'flags'))
		Temporals
			SELECT '2012-03-11 3:10 AM America/Los_Angeles'::timestamptz
			SELECT '2012-03-11 3:10 AM'::timestamp - '2012-03-11 1:10 AM'::timestampt
			SELECT '2012-03-11 3:10 AM America/Los_Angeles'::timestamptz AT TIME ZONE 'Europe/Paris'
			SELECT '2012-03-11 3:10 AM'::timestamp + '1 hour'::interval
			SELECT '23 hours'::interval + '1 hour'::interval
			SELECT ('2012-03-11 3:10 AM'::timestamp ,'2012-03-11 5:10 AM'::timestamp) OVERLAPS (<timestamp>) 
			generate_series(<timestamp>, <end_timestamp>, <interval>)
			date_part('hour', <timestamp>)
			to_char(<timestamp>, 'HH12:MI AM')
		Arrays
			integer[]
			character[]
			SELECT ARRAY[2001, 2002] As years
			SELECT array(SELECT ...)
			'{Alex,Sonia}'::text[] As name
			string_to_array('ca.ma.tx', '.')
			SELECT array_agg(log_ts) FROM logs WHERE log_ts BETWEEN ...
			my_array[1] # first
			my_array[array_upper(my_array,1)] # last
			my_array[2:4]
			arr[1:2] || arr[5:6]
			unnest('{1,2,3}'::smallint[])
		Range
			(-2,2]
			INSERT INTO employment (employee, period) VALUES ('Alex', '[2012-04-24, infinity)'::daterange)
			overlap (&&)
			contains (@>)
			SELECT employee FROM employment WHERE period @> CURRENT_DATE GROUP BY employee
		JSON
			https://www.postgresql.org/docs/current/static/functions-json.html
			INSERT INTO families_j (profile) VALUES (
				'{"name":"Gomez", "members": [{"member":{"relation":"padre",...}}]}')
			json_extract_path(profile, 'name') As family,
			json_extract_path_text( json_array_elements( json_extract_path(profile, 'members') ), 'member', 'name') As member
			json_array_elements( (profile->'members')) #>> '{member,name}'::text[] As member
			json_array_length(profile->'members') As numero,
			profile->'members'->0#>>'{member,name}'::text[] As primero
			profile->'members'
			profile->0
			SELECT row_to_json(f) As x
				FROM (SELECT id, profile->>'name' As name FROM families_j) As f
			SELECT profile->>'name' As family
				FROM families_b
				WHERE profile @> '{"members":[{"member":{"name":"Alex"} }]}'
	Constraints id=g10309
		Constraints <url:file:///~/projects/study/otl/cdb.otl#r=g10309>
		ALTER TABLE facts ADD CONSTRAINT fk_facts_f 
			FOREIGN KEY (fact_type_id) REFERENCES lu_fact_types (fact_type_id)
			ON UPDATE CASCADE ON DELETRESTRICT;
			CREATE INDEX fki_facts_1 ON facts (fact_type_id);
		ALTER TABLE logs_2011 ADD CONSTRAINT uq 
			UNIQUE (user_name, log_ts);
		ALTER TABLE logs ADD CONSTRAINT chk 
			CHECK (user_name = lower(user_name));
		ALTER TABLE schedules ADD CONSTRAINT ex_schedules
			EXCLUDE USING gist (room WITH =, time_slot WITH &&)
	Views id=g10310
		Views <url:file:///~/projects/study/otl/cdb.otl#r=g10310>
		CREATE view_name AS query
		CREATE RECURSIVE VIEW view_name(columns) AS query
		CREATE VIEW census.vw_facts_2011 AS
			SELECT fact_type_id, ... FROM <table> WHERE yr = 2011;
		WITH CHECK OPTION
			keep data in view always
		UPDATE <view> SET val = 1 WHERE val = 0;
		DELETE FROM <view> WHERE val = 0
		Using Triggers to Update Views
			CREATE FUNCTION census.trig_vw_facts() RETURNS trigger AS $$
				BEGIN
					IF (TG_OP = 'INSERT') THEN
						INSERT INTO <table(col1, col2)>
						SELECT NEW.col1, NEW.col2;
						RETURN NEW;
					END IF;
				END;
				$$
				LANGUAGE plpgsql VOLATILE;
			CREATE TRIGGER census.trig_vw_facts
				INSTEAD OF INSERT OR UPDATE OR DELETE ON <view>
				FOR EACH ROW EXECUTE PROCEDURE census.trig_vw_facts();
	Handy Constructions
		DISTINCT ON
		LIMIT and OFFSET
		CAST('2011-1-11' AS date)
			'2011-1-1'::date
		INSERT INTO <table> (<cols>)
			VALUES
				(<row1>),
				(<row2>);
		SELECT .. WHERE <col1> ILIKE '%duke%'
		DELETE FROM census.facts
			USING census.lu_fact_types As ft
			WHERE facts.fact_type_id = ft.fact_type_id AND ft_short_name = 's01'
		UPDATE census.lu_fact_types AS f
			SET short_name = ..
			WHERE ..
			RETURNING fact_type_id, short_name
		SELECT x FROM census.lu_fact_types As x LIMIT 2;
		SELECT array_to_json(array_agg(f)) As cat FROM (
			SELECT MAX(fact_type_id) As max_type, category FROM census.lu_fact_types
			GROUP BY category
			) As f
		DO language plpgsql
			$$
			DECLARE var_sql text
			BEGIN
				var_sql := string_agg(
					'INSERT INTO ...
					' || lpad(i::text, 2, '0') || '..
					'
				)
				FROM generate_series(1,51) As i;
				EXECUTE var_sql;
			END
			$$;
		SELECT student,
				AVG(score) FILTER (WHERE subject ='algebra') As algebra, 
				AVG(score) FILTER (WHERE subject ='physics') As physics
			FROM test_scores 
			GROUP BY student;
	Window Functions id=g10311
		Window Functions <url:file:///~/projects/study/otl/cdb.otl#r=g10311>
		SELECT tract_id, val, AVG(val) OVER () as val_avg
			FROM census.facts
			WHERE fact_type_id = 86;
		SELECT tract_id, val, AVG(val) OVER (PARTITION BY left(tract_id,5)) as val_avg_county
			FROM census.facts
			WHERE fact_type_id = 86 ORDER BY tract_id;
		SELECT ROW_NUMBER() OVER (ORDER BY tract_name) As rnum, tract_name
		SELECT 
			ROW_NUMBER() OVER (wt) As rnum, 
			substring(tract_id,1,5) As county_code,
			tract_id,
			LAG(tract_id,2) OVER wt As tract_2_before,
			LEAD(tract_id) OVER wt As tract_after,
			FROM census.lu_tracts
			WINDOW wt AS (PARTITION BY substring(tract_id,1,5) ORDER BY tract_id)
	Common Table Expressions (CTE) id=g10312
		Common Table Expressions (CTE) <url:file:///~/projects/study/otl/cdb.otl#r=g10312>
		WITH 
			cte1 AS (
				SELECT * FROM <table1>
			), 
			cte2 AS (
				SELECT * FROM <table2>
			)
			SELECT *
			FROM cte1
			WHERE ..
		ex: writable cte
			CREATE TABLE logs_2011_01 (
					PRIMARY KEY (..),
					CONSTRAINT chk
						CHECK (log_ts >= '2011-01-01')
				)
				INHERITS (logs_2011);
			WITH t AS (
					DELETE FROM ONLY logs_2011 WHERE log_ts < '2011-03-0' RETURNING *
				)
				INSERT INTO logs_2011_01_02 SELECT * FROM t;
		WITH RECURSIVE subordinates AS (
			SELECT employee_id, manager_id, full_name
				FROM employees
				WHERE employee_id = 2
			UNION
			SELECT e.employee_id, e.manager_id, e.full_name
				FROM employees e
				INNER JOIN subordinates s ON s.employee_id = e.manager_id
			) 
			SELECT * FROM subordinates;
	Functions id=g10313
		Functions <url:file:///~/projects/study/otl/cdb.otl#r=g10313>
		CREATE OR REPLACE FUNCTION func_name(arg1 arg1_datatype DEFAULT arg1_default)
			RETURNS some type | set of some type | TABLE (..) AS
			$$ 
			BODY of function
			$$ 
			LANGUAGE languafe_of_function;
		SELECT lanname FROM pg_language;
		CREATE AGGREGATE my_agg (input data type) ( ..)
		CREATE OR REPLACE FUNCTION write_to_log(param_user_name varchar, param_description text)
			RETURNS integer AS
			$$
			INSERT INTO logs(user_name, description) VALUES($1, $2)
			RETURNING log_id;
			$$
			LANGUAGE 'sql' VOLATILE;
		SELECT write_to_log('alejandro', 'Woke up') As new_id;
		CREATE OR REPLACE FUNCTION select_logs_rt(param_user_name varchar)
			RETURNS TABLE (log_id int, user_name varchar(50), description text, log_ts timestamptz) AS
			...
		CREATE AGGREGATE geom_mean (numeric) (
				SFUNC=geom_mean_state,
				STYPE=numeric[],
				FINALFUNC=geom_mean_final,
				INITCOND='{0,0}'
			);
		CREATE EXTENSION plpython3u;
	Builtin Functions id=g10314
		Builtin Functions <url:file:///~/projects/study/otl/cdb.otl#r=g10314>
		between is
			a BETWEEN x and y
			a <= x AND a <= y
			a NOT BETWEEN x AND y
			<expression> IS [NOT] NULL
			IS
				TRUE
				FALSE
				UNKNOWN
				NOT
		string substring to_char
			char_length(string)
			substring(string, int, int)
			'thomas' ~ '.*thomas.*'
			to_char(current_timestamp, 'HH12:MI:SS')
			to_date('05 Dec 2000', 'DD Mon YYYY')
		date temporal extract
			current_date()
			current_time()
			date_part('hour', timestamp '2011-02-16 20:38:40')
			SELECT EXTRACT(DAY FROM TIMESTAMP '2001-02-16 20:38:40');
			SELECT CURRENT_TIMESTAMP;
			SELECT now();
		case coalesce
			SELECT a,
				CASE WHEN a=1 THEN 'one'
						 WHEN a=2 THEN 'two'
						 ELSE 'other'
				END
				FROM test;
			SELECT COALESCE(description, short_description, '(none)') ...
		array append ndims length unnest agg
			array_append(ARRAY[1,2], 3) 
			array_cat(ARRAY[1,2,3], ARRAY[4,5]) 
			array_ndims(ARRAY[[1,2,3], [4,5,6]])  
			array_length(array[1,2,3], 1)  # 3
			unnest(ARRAY[1,2])  
				1
				2
				(2 rows)
			array_agg(expression) any non-array type    
		aggregate functions
			avg bool_and count
				avg(expr)
				bool_and(expr)
				bool_or(expr)
				count(*)
				count(expr)
				max
				min
				sum(expr)
			json_agg string_agg array_agg
				json_agg(expression)  
					description: aggregates values as json array
				json_object_agg(name, value)  
					description: aggregates name/value pairs as json object
				string_agg(expr, delimiter)
					description: input values concatenated into an string
					SELECT movie, string_agg(actor, ', ') AS actor_list FROM tbl GROUP  BY 1;
					STRING_AGG(a.activity, ';' ORDER BY a.activity) As activities
					ex
						SELECT movie, string_agg(actor, ', ') AS actor_list
						FROM   tbl
						GROUP  BY 1;
				array_agg(expression) 
					description: input values concatenated into an array
					SELECT array_agg('{a,b,c}'::text[],'{d,e,f}'::text[]); 
						--> {{a,d},{b,e},{c,f}}
					SELECT company_id, array_agg(employee ORDER BY company_id DESC)::text FROM tbl GROUP  BY 1;
					select title as album,
							array_agg(distinct genre.name order by genre.name) as genres
						from      track
							join genre using(genreid)
							join album using (albumid)
						group by title
							having count(distinct genre.name) > 1;
					-->
						Battlestar Galactica, Season 3 │ {"Sci Fi & Fantasy","Science Fiction","TV Shows"}
						Greatest Hits                  │ {Metal,Reggae,Rock}
		window row_number rank cume_dist ntile
			row_number()
			rank()
			dense_rank()
			percent_rank()
			cume_dist()
			ntile
			lag
			lead
			first_value
			last_value
			nth_value
		IN generate_series 
			SELECT col1
				FROM tab1
				WHERE EXISTS (SELECT 1 FROM tab2 WHERE col2 = tab1.col2);
			expression IN (subquery)
			generate_series(start, stop, step interval)
			SELECT generate_subscripts('{NULL,1,NULL,2}'::int[], 1) AS s;
		current db user schema setting set_config
			current_catalog
			current_database()
			current_role
			current_schema
			current_user
			has_any_column_privilege(user, table, privilege)
			has_column_privilege(user, table, column, privilege)
			SELECT current_setting('datestyle');
			SELECT set_config('log_statement_stats', 'off', false);
			pg_cancel_backend(pid int)  
			select pg_start_backup('label_goes_here');
	JOIN id=g10315
		JOIN <url:file:///~/projects/study/otl/cdb.otl#r=g10315>
		FROM people AS p 
			LEFT JOIN people_activities As a ON (p.p_name = a.p_name)
			GROUP BY p.p_name
			ORDER BY p.p_name; 
		FROM employment As e1 INNER JOIN employment As e2
			ON e1.period && e2.period
			WHERE e1.employee <> e2.employee GROUP BY e1.employee;
		CREATE OR ... AS
			SELECT ..
			FROM <table> INNER JOIN <table>
			ON <fk1> = <fk2>
	Row Level Security (RLS) id=g10316
		Row Level Security (RLS) <url:file:///~/projects/study/otl/cdb.otl#r=g10316>
		CREATE TABLE accounts (manager text, company text);
		ALTER TABLE accounts ENABLE ROW LEVEL SECURITY;
		CREATE POLICY user_policy ON users
			USING (true)
			WITH CHECK (user_name = current_user);
		GRANT SELECT
			(user_name, uid, gid, real_name, home_phone, extra_info, home_dir, shell)
			ON passwd TO public;
		GRANT UPDATE
			(pwhash, real_name, home_phone, extra_info, shell)
			ON passwd TO public;
	ddl id=g10320
		ddl <url:file:///~/projects/study/otl/cdb.otl#r=g10320>
		analysis
			pk
				id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
				product_id serial PRIMARY KEY  -- implicit primary key constraint
				multiple keys
					bill_id    int REFERENCES bill (bill_id) ON UPDATE CASCADE ON DELETE CASCADE
					, product_id int REFERENCES product (product_id) ON UPDATE CASCADE
					, CONSTRAINT bill_product_pkey PRIMARY KEY (bill_id, product_id)  -- explicit pk
				bid INTEGER NOT NULL REFERENCES Book, 
					PRIMARY KEY(bid) -- a book is borrowed once at a time!
				id INT NOT NULL PRIMARY KEY,
				employee_id   NUMBER         NOT NULL,
					CONSTRAINT employees_pk PRIMARY KEY (employee_id)
			text varchar
				payload text
				, product    text NOT NULL
				employee varchar(20)
				first_name    VARCHAR2(1000) NOT NULL,
			numeric int
				, price      numeric NOT NULL DEFAULT 0
				, product_id int REFERENCES product (product_id) ON UPDATE CASCADE
				room smallint
				bid INTEGER NOT NULL REFERENCES Book, 
				a1 int[]
				a_id int
				running_total INTEGER
				gdp_per_capita DECIMAL(10, 2) NOT NULL
				employee_id   NUMBER         NOT NULL,
			fk
				bill_id    int REFERENCES bill (bill_id) ON UPDATE CASCADE ON DELETE CASCADE
				, product_id int REFERENCES product (product_id) ON UPDATE CASCADE
				bid INTEGER NOT NULL REFERENCES Book, 
				user_id INTEGER,
			date
				aggr_date DATE,
				, billdate date NOT NULL DEFAULT CURRENT_DATE
				period daterange
				time_slot tstzrange
				due TIMESTAMPTZ
				added TIMESTAMP WITHOUT TIME ZONE NOT NULL
				borrowed TIMESTAMP NOT NULL, 
				created_on   timestamptz not null default now(),
			other
				gender BOOLEAN NOT NULL, 
				profile json
				profile jsonb
		examples id=g10321
			examples <url:file:///~/projects/study/otl/cdb.otl#r=g10321>
			Tables
				create table
					CREATE TABLE accounts (manager text, company text);
					CREATE TABLE dir_list (filename text) 
					CREATE TABLE employment (id serial PRIMARY KEY, employee varchar(20), period daterange)
					CREATE TABLE families_j (id serial PRIMARY KEY, profile json)
					CREATE TABLE families_b (id serial PRIMARY KEY, profile jsonb)
					CREATE TABLE logs_2011 (PRIMARY KEY(log_id)) INHERITS (logs)
					CREATE TABLE schedules(id .., room smallint, time_slot tstzrange);
					CREATE TABLE logs_2011 (PRIMARY KEY(log_id)) INHERITS (logs)
					CREATE TABLE api.todos (
						id SERIAL PRIMARY KEY,
						done BOOLEAN NOT NULL DEFAULT FALSE,
						task TEXT NOT NULL,
						due TIMESTAMPTZ
						);
					CREATE TABLE people (
						fname text,
						lname text
						);
					CREATE TABLE sample_data01 (
						product_id INTEGER NOT NULL, 
						title VARCHAR(11) NOT NULL, 
						added TIMESTAMP WITHOUT TIME ZONE NOT NULL
						);
					CREATE TABLE generated_table (
									name VARCHAR(6) NOT NULL,
									kg INTEGER NOT NULL,
									species VARCHAR(8) NOT NULL
						) ;
					CREATE TABLE Book( 
						bid SERIAL PRIMARY KEY,
						title TEXT NOT NULL, 
						isbn ISBN13 NOT NULL 
						);
					CREATE TABLE Reader( 
						rid SERIAL PRIMARY KEY,
						firstname TEXT NOT NULL, 
						lastname TEXT NOT NULL, 
						born DATE NOT NULL, 
						gender BOOLEAN NOT NULL, 
						phone TEXT 
						);
					CREATE TABLE Borrow( 
						borrowed TIMESTAMP NOT NULL, 
						rid INTEGER NOT NULL REFERENCES Reader,
						bid INTEGER NOT NULL REFERENCES Book, 
						PRIMARY KEY(bid) -- a book is borrowed once at a time!
						);
					CREATE TABLE channel(
						id INT NOT NULL PRIMARY KEY,
						fixture INT NOT NULL
						);
					CREATE TABLE user_msg_log (
							aggr_date DATE,
							user_id INTEGER,
							running_total INTEGER
						);
					CREATE TABLE countries (
						code CHAR(2) NOT NULL,
						year INT NOT NULL,
						gdp_per_capita DECIMAL(10, 2) NOT NULL
						);
					CREATE TABLE states
						("name" int, "admin" int)
						;
					CREATE TABLE employees (
						 employee_id   NUMBER         NOT NULL,
						 first_name    VARCHAR2(1000) NOT NULL,
						 last_name     VARCHAR2(1000) NOT NULL,
						 date_of_birth DATE           NOT NULL,
						 phone_number  VARCHAR2(1000) NOT NULL,
						 CONSTRAINT employees_pk PRIMARY KEY (employee_id)
						)
					CREATE TABLE tracks (id SERIAL, artists JSON);
					CREATE TABLE tbl
						(zipcode text NOT NULL, city text NOT NULL, state text NOT NULL);
					CREATE TABLE monkey(name text NOT NULL)
					CREATE TABLE tbl (a1 int[], a2 int[]);
					CREATE TABLE foo (id serial, data text);
					CREATE TABLE staff (
						staff_id serial PRIMARY KEY,
						staff    text NOT NULL
						);
					CREATE TABLE tbl_a (a_id int, col1 int, col2 int);
					create table client (
						id           serial primary key,
						name         text not null,
						address      text,
						user_id      int not null references "user"(id),
						created_on   timestamptz not null default now(),
						updated_on   timestamptz
						);
				alter table
					ALTER TABLE logs_2011 ADD CONSTRAINT chk_y2011
						CHECK (log_ts >= '2011-1-1'::timestamptz 
							AND log_ts < <timestamp>)
					ALTER TABLE table_name ADD COLUMN new_col TYPE;
					ALTER TABLE table_name DROP COLUMN col1;
					ALTER TABLE table_name RENAME col1 TO col2;
					ALTER TABLE table_name ADD PRIMARY KEY (column,...)
					ALTER TABLE table_name DROP CONSTRAINT pk_constraint_name
					ALTER TABLE table_name RENAME TO new_table
				CREATE UNLOGGED TABLE ...
				DROP TABLE
					DROP TABLE table_name CASCADE;
					DROP TABLE IF EXISTS table_name CASCADE;
			identity column
				CREATE TABLE test_new (
						id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
						payload text
				);
			How to implement a many-to-many relationship in PostgreSQL? <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10176>
				CREATE TABLE product (
					product_id serial PRIMARY KEY  -- implicit primary key constraint
					, product    text NOT NULL
					, price      numeric NOT NULL DEFAULT 0
				);
				CREATE TABLE bill (
					bill_id  serial PRIMARY KEY
				, bill     text NOT NULL
				, billdate date NOT NULL DEFAULT CURRENT_DATE
				);
				CREATE TABLE bill_product (
					bill_id    int REFERENCES bill (bill_id) ON UPDATE CASCADE ON DELETE CASCADE
				, product_id int REFERENCES product (product_id) ON UPDATE CASCADE
				, amount     numeric NOT NULL DEFAULT 1
				, CONSTRAINT bill_product_pkey PRIMARY KEY (bill_id, product_id)  -- explicit pk
				);
			writable CTE
				CREATE TABLE logs_2011_01 (
					PRIMARY KEY (..),
					CONSTRAINT chk
						CHECK (log_ts >= '2011-01-01')
				)
				INHERITS (logs_2011);
	questions and answers
		deck01: from sof
			ref
				SOF PostgreSQL: Stackoverflow SOF Popular Questions   <url:file:///~/projects/study/otl/cdb.otl#r=g10238>
			new function: header line?
				CREATE OR REPLACE FUNCTION f_foo()
			new function: CREATE FUNCTION foo() 
Postgresql Setup id=g12106
	Lokalde kurulum yaptıktan sonra, önceden posgtersql env varların var mı kontrol et. Yoksa düzgün çalışmaz.
	[Tutorial: How to use PostgreSQL. In this tutorial you’ll learn how to… | by Martin Lasek | Medium](https://martinlasek.medium.com/tutorial-how-to-use-postgresql-88cddc858d9f)
	This formula has created a default database cluster with:
		initdb --locale=C -E UTF-8 /usr/local/var/postgres
	To have launchd start postgresql now and restart at login:
		brew services start postgresql
	Or, if you don't want/need a background service you can just run:
		pg_ctl -D /usr/local/var/postgres start
	Error: psql: error: FATAL:  role "superuser" does not exist <url:file:///~/projects/study/logbook/log_20210419.md#r=g12104>
		opt02: Single user mode ile kullanıcıları resetleme <url:file:///~/projects/study/logbook/log_20210419.md#r=g12107>
			brew services stop postgresql
			postgres --single -D /usr/local/var/postgres postgres
	what is default username and password?
		default username: username of the operating system 
			[What is the Default Password for PostgreSQL? | Liquid Web](https://www.liquidweb.com/kb/what-is-the-default-password-for-postgresql/)
		check pg_hba.conf
			cat /usr/local/var/postgres/pg_hba.conf
			SHOW hba_file;
		ref: [postgresql - What's the default superuser username/password for postgres after a new install? - Server Fault](https://serverfault.com/questions/110154/whats-the-default-superuser-username-password-for-postgres-after-a-new-install)
	connect to postgres
		psql postgres
	current users
		select * from user;
	jdbc connection string
		[Connecting to the Database](https://jdbc.postgresql.org/documentation/80/connect.html)
			jdbc:postgresql:database
			jdbc:postgresql://host/database
			jdbc:postgresql://host:port/database
	createdb: create database
		createdb mydb;
PostgreSQL how tos  id=g10325
	find tables that contain a column id=g10739
		find tables that contain a column <url:file:///~/projects/study/otl/cdb.otl#r=g10739>
		ref
			<url:/Users/mertnuhoglu/projects/bizqualify/BQ-data-run/datarun/debug_if_11_forecast_is_cause_of_2017_problem.md#tn=03.01.01. Which tables do have `acknowledgement_id`?>
		opt01
			SELECT table_name FROM information_schema.columns WHERE column_name = 'acknowledgement_id';
		opt02
			SELECT c.relname
			FROM pg_class as c
			INNER JOIN pg_attribute as a on a.attrelid = c.oid
			INNER JOIN pg_namespace as n on c.relnamespace = n.oid
			WHERE a.attname = 'acknowledgement_id' 
			AND c.relkind = 'r' 
			AND n.nspname = 'data_20181202';
	count rows
		row count of all tables id=g10738
			row count of all tables <url:file:///~/projects/study/otl/cdb.otl#r=g10738>
			https://stackoverflow.com/questions/28667604/get-a-row-count-of-every-table-in-postgres-database
				opt01: for a table:
					SELECT reltuples FROM pg_class WHERE oid = 'my_schema.my_table'::regclass;
				opt02: for all tables
					SELECT
						pgClass.relname   AS tableName,
						pgClass.reltuples AS rowCount
					FROM
						pg_class pgClass
					LEFT JOIN
						pg_namespace pgNamespace ON (pgNamespace.oid = pgClass.relnamespace)
					WHERE
						pgNamespace.nspname NOT IN ('pg_catalog', 'information_schema') AND
						pgClass.relkind='r'
			https://www.periscopedata.com/blog/exact-row-counts-for-every-database-table
				ref
					<url:/Users/mertnuhoglu/projects/bizqualify/BQ-data-run/datarun/study_schema.md#tn=02.01. Verify that these are really unique>
				SELECT count(1) FROM data_20181202.master_variables;
				7930263
				opt01: dynamic count() sql code:
					create or replace function 
					count_rows(schema text, tablename text) returns integer
					as
					$body$
					declare
						result integer;
						query varchar;
					begin
						query := 'SELECT count(1) FROM ' || schema || '.' || tablename;
						execute query into result;
						return result;
					end;
					$body$
					language plpgsql;
				usage for all tables
					select 
						table_schema,
						table_name, 
						count_rows(table_schema, table_name)
					from information_schema.tables
					where 
						table_schema not in ('pg_catalog', 'information_schema') 
						and table_type='BASE TABLE'
					order by 3 desc
	json_populate_record id=g10324
		json_populate_record <url:file:///~/projects/study/otl/cdb.otl#r=g10324>
		select * from json_populate_record(null::x, '{"a":1,"b":2}')  
		 a | b
		---+---
		 1 | 2
	create table <tablename> (like <source_table>) = create table as (select)
		https://www.postgresql.org/docs/9.6/sql-createtable.html
		create table ", data_schema, ".master_variables_forecast (like ", data_schema, ".master_variables)
		CREATE TABLE data_20181217.all_data AS TABLE data_20181202.all_data;
	select into = create table as id=g10326
		select into = create table as <url:file:///~/projects/study/otl/cdb.otl#r=g10326>
		pgsql doesn't create table with "select into"
			https://stackoverflow.com/questions/11979154/select-into-to-create-a-table-in-pl-pgsql
				use: create table as
					CREATE TEMP TABLE mytable AS
					SELECT *
					FROM orig_table;
				https://www.postgresql.org/docs/current/static/sql-selectinto.html
				note: CREATE TABLE AS is functionally similar to SELECT INTO. CREATE TABLE AS is the recommended syntax, since this form of SELECT INTO is not available in ECPG or PL/pgSQL, because they interpret the INTO clause differently. Furthermore, CREATE TABLE AS offers a superset of the functionality provided by SELECT INTO.
		https://www.w3schools.com/SQl/sql_select_into.asp
			ex: copy all columns into new table
				SELECT *
				INTO newtable [IN externaldb]
				FROM oldtable
				WHERE condition;
			ex: creates a backup copy
				SELECT * INTO CustomersBackup2017
				FROM Customers;
			ex: backup in another database
				SELECT * INTO CustomersBackup2017 IN 'Backup.mdb'
				FROM Customers;
		http://www.dofactory.com/sql/select-into
			ex
				SELECT column-names
					INTO new-table-name
					FROM table-name
				 WHERE EXISTS 
							(SELECT column-name
								 FROM table-name
								WHERE condition)
		with no data
			https://stackoverflow.com/questions/1220453/copy-table-structure-into-new-table
			CREATE TABLE films2 AS
					TABLE films
					WITH NO DATA;
	record type id=g10327
		record type <url:file:///~/projects/study/otl/cdb.otl#r=g10327>
		https://www.postgresql.org/docs/9.0/static/plpgsql-declarations.html#PLPGSQL-DECLARATION-RECORDS
		<name> RECORD;
		similar to row-type variables
			but have no predefined structure
			like tuple
			they take row structure of the row they are assigned during a SELECT or FOR
		it is not a true data type, only a placeholder
		when pgsql function returns type record
			record variable can change its row structure on the fly
	change current_schema id=g10328
		change current_schema <url:file:///~/projects/study/otl/cdb.otl#r=g10328>
		first schema in search path is current schema
		ex
			set search_path = data, public;
			select settings.set('auth.data-schema', current_schema);
	set a password - store check passwords crypt() id=g10329
		set a password - store check passwords crypt() <url:file:///~/projects/study/otl/cdb.otl#r=g10329>
		https://www.postgresql.org/docs/current/static/pgcrypto.html
			Example of setting a new password:
				UPDATE ... SET pswhash = crypt('new password', gen_salt('bf'));
			Example of authentication:
				SELECT (pswhash = crypt('entered password', pswhash)) AS pswmatch FROM ... ;
			This returns true if the entered password is correct.
	list tables: \dt
		ex:
			# \dt settings.
			List of relations
			Schema  |  Name   | Type  |   Owner
		 ----------+---------+-------+-----------
		 settings | secrets | table | superuser
	list functions: \df
		ex:
			# \df settings.
			List of functions
			Schema  | Name | Result data type | Argument data types |  Type
			----------+------+------------------+---------------------+--------
			settings | get  | text             | text                | normal
			settings | set  | void             | text, text          | normal 
	single quote vs double quote for text variables id=adb_016
		single quote vs double quote for text variables <url:#r=adb_016>
		single quote vs. double quote
			strings: always single quoted
				double quote as: '' (double single quote)
			double quotes used only for quoted identifiers
		ex
			# SELECT settings.get('jwt_secret');
				get
				----------------------------------
				reallyreallyreallyreallyverysafe
			# SELECT settings.get("jwt_secret");
				error: column "jwt_secret" does not exist
	echo variables of psql
		ex:
			# \set test1 hello
			# \echo :test1
			hello
		ex
			# \setenv base_dir :DIR
			# \set test2 `echo $base_dir`
			# \echo :test2
			:DIR
	Difference between set, \set and \pset in psql id=g10330
		Difference between set, \set and \pset in psql <url:file:///~/projects/study/otl/cdb.otl#r=g10330>
		https://stackoverflow.com/questions/29593908/difference-between-set-set-and-pset-in-psql#29594391
		SET: sql command
		\set \pset: psql meta-commands
		SET
			to change runtime params
			executed on server
		\set psql local variables 
		\pset: affecting the output of query result tables
		ex: 
			SET ROLE dba;
			\set time 'select current_timestamp'
			\pset border 2
	GRANT USAGE id=g10331
		GRANT USAGE <url:file:///~/projects/study/otl/cdb.otl#r=g10331>
		https://stackoverflow.com/questions/2126225/why-is-a-grant-usage-created-the-first-time-i-grant-a-user-privileges#3972085
			USAGE === "no privileges"
		https://stackoverflow.com/questions/17338621/what-grant-usage-on-schema-exactly-do#17355059
			rights tests are done in order:
				1. do you have USAGE on schema?
				2. do you have appropriate rights on table?
				3. check column privileges
			USAGE on schema, not on objects within the schema
			it is like a directory teree
				ex: somedir/somefile
					rwx------ on dir
					rw------- on file
			in general use this configuration:
				--ACCESS DB
				REVOKE CONNECT ON DATABASE nova FROM PUBLIC;
				GRANT  CONNECT ON DATABASE nova  TO user;
				--ACCESS SCHEMA
				REVOKE ALL     ON SCHEMA public FROM PUBLIC;
				GRANT  USAGE   ON SCHEMA public  TO user;
				--ACCESS TABLES
				REVOKE ALL ON ALL TABLES IN SCHEMA public FROM PUBLIC ;
				GRANT SELECT                         ON ALL TABLES IN SCHEMA public TO read_only ;
				GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO read_write ;
				GRANT ALL                            ON ALL TABLES IN SCHEMA public TO admin ;
	PostgreSQL Recursive Query By Example id=adb_005
		PostgreSQL Recursive Query By Example <url:#r=adb_005>
		http://www.postgresqltutorial.com/postgresql-recursive-query/
		used for hierarchical queries
			ex: product bill, organization hierarchy
		general form
			WITH RECURSIVE cte_name(
					CTE_query_definition -- non-recursive term
					UNION [ALL]
					CTE_query definion  -- recursive term
			) SELECT * FROM cte_name;
		ex:
			recursive query
				WITH RECURSIVE subordinates AS (
					SELECT employee_id, manager_id, full_name
						FROM employees
						WHERE employee_id = 2
					UNION
					SELECT e.employee_id, e.manager_id, e.full_name
						FROM employees e
						INNER JOIN subordinates s ON s.employee_id = e.manager_id
				) 
				SELECT * FROM subordinates;
			two parts:
				1. before UNION: non-recursive
					| employee_id | manager_id | full_name   |
					| 2           | 1          | Megan Berry |
				2. after UNION: recursive
				| employee_id | manager_id | full_name     |
				| 6           | 2          | Bella Tucker  |
				| 7           | 2          | Ryan Metcalfe |
	psql
		17 Practical psql Commands id=g10332
			17 Practical psql Commands <url:file:///~/projects/study/otl/cdb.otl#r=g10332>
			http://www.postgresqltutorial.com/psql-commands/
			connect
				psql -d database -U  user -W
					asks password
			switch to new database
				\c <dbname> <username>
			list available databases: \l
			list tables: \dt
				more: \dt+
			describe a table: \d <table_name>
				more: \d+ <table>
			list schemas: \dn
			list functions: \df
			list views: \dv
			list users and roles: \du
				more information: \du+
			execute previous command: \g
			command history: \s
			save command history: \s <filename>
			exec commands from file: \i <filename>
			send query results to file: \o <file>
			get help: \?
			help on statement: \h <ALTER TABLE>
			turn on query exec time: \timing
			edit command in editor: \e
			edit function in editor: \ef <function_name>
			quit: \q
			query buffer
				reset query buffer: \r
				write query buffer: \w <file>
			general
				exec command in shell: \! <command>
				show/set encoding: \encoding <enc>
				change working dir: \cd <dir>
	PostgreSQL Cheat Sheet id=g10333
		PostgreSQL Cheat Sheet <url:file:///~/projects/study/otl/cdb.otl#r=g10333>
		http://www.postgresqltutorial.com/postgresql-cheat-sheet/
		connect
			psql -U <username>
			\c <database_name>
		pretty format: \x
		create new role:
			CREATE ROLE <role_name>;
			CREATE ROLE <username> NOINHERIT LOGIN PASSWORD <password>;
		change role for session:
			SET ROLE <role>;
		allow <role_1> to set its role as <role_2>
			GRANT role_2 TO role_1;
		create db
			CREATE DATABASE [IF NOT EXISTS] db_name
		delete db
			DROP DATABASE [IF EXISTS] db_name
		managing tables
			add new column
				ALTER TABLE table_name ADD COLUMN new_col TYPE;
			drop column
				ALTER TABLE table_name DROP COLUMN col1;
			rename column
				ALTER TABLE table_name RENAME col1 TO col2;
			add primary key
				ALTER TABLE table_name ADD PRIMARY KEY (column,...)
			remove pk
				ALTER TABLE table_name DROP CONSTRAINT pk_constraint_name
			rename table
				ALTER TABLE table_name RENAME TO new_table
		managing views
			create a view
				CREATE OR REPLACE view_name AS query
			create recursive view
				CREATE RECURSIVE VIEW view_name(columns) AS query
			create materialized view
	PostgreSQL Basics by Example id=g10334
		PostgreSQL Basics by Example <url:file:///~/projects/study/otl/cdb.otl#r=g10334>
		http://blog.trackets.com/2013/08/19/postgresql-basics-by-example.html
		psql
			\h                 # help on SQL commands
			\?                 # help on psql commands, such as \? and \h
			\l                 # list databases
			\c database_name   # connect to a database
			\d                 # list of tables
			\d table_name      # schema of a given table
			\du                # list roles
			\e                 # edit in $EDITOR
		user management
			allow login
				CREATE ROLE john WITH LOGIN
				ALTER ROLE john LOGIN
			multiple attributes
				postgres=# CREATE ROLE deploy SUPERUSER LOGIN;
				postgres=# ALTER ROLE deploy NOSUPERUSER CREATEDB;  # the LOGIN privilege is not touched here
			add role to some group
				postgres=# CREATE GROUP admin LOGIN;
				postgres=# GRANT admin TO john;
	pgScript for Geocoding
		http://www.postgresonline.com/journal/archives/181-pgAdmin-pgScript.html
		plpgsql function
		for adhoc batch jobs
		compared to stored procedures
			not a single transaction
				you run something in a loop
				each loop commit separately
		ex: geocodes 500 records at a time, repeats 20000 times
			code
				DECLARE @I;
				SET @I = 0;
				WHILE @I < 20000
				BEGIN
					UPDATE addr_to_geocode
					SET (rating, norm_address, pt)
					= (
						g.rating,
						COALESCE ((g.addy).address::text, '')
							|| COALESCE(' ' || (g.addy).predirabbrev, '')
							|| COALESCE(' ' || (g.addy).streetname,'')
							|| ' ' || COALESCE(' ' || (g.addy).streettypeabbrev, '')
							|| COALESCE(' ' || (g.addy).location || ', ', '')
							|| COALESCE(' ' || (g.addy).stateabbrev, '')
							|| COALESCE(' ' || (g.addy).zip, '')
						,
						g.geomout
					)
							FROM (SELECT DISTINCT ON (addid) addid, (g1.geo).*
							FROM (SELECT addid, (geocode(address)) As geo
							FROM (SELECT * FROM addr_to_geocode WHERE ag.rating IS NULL ORDER BY addid LIMIT 500) As ag
					) As g1
					-- 5 pick lowest rating
					ORDER BY addid, rating) As g
					WHERE g.addid = addr_to_geocode.addid;
					SET @I = @I + 1;
					PRINT @I;
				END
	FROM people AS p 
			LEFT JOIN people_activities As a ON (p.p_name = a.p_name)
			GROUP BY p.p_name
			ORDER BY p.p_name; 
	Application users vs. Row Level Security id=adb_004 id=g10344
		Application users vs. Row Level Security  id=g10344 <url:file:///~/projects/study/otl/cdb.otl#r=g10344>
		Application users vs. Row Level Security <url:#r=adb_004>
		https://blog.2ndquadrant.com/application-users-vs-row-level-security/
		Introduction to RLS
			chat table
				CREATE TABLE chat (
						message_uuid    UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
						message_time    TIMESTAMP NOT NULL DEFAULT now(),
						message_from    NAME      NOT NULL DEFAULT current_user,
						message_to      NAME      NOT NULL,
						message_subject VARCHAR(64) NOT NULL,
						message_body    TEXT
				);
			problem: 
				prevent users from 
					reading messages intended for other users
					sending messages with a fake message_from
			ex:
				CREATE POLCIY chat_policy ON chat
					USING ((message_to = current_user) OR (message_from = current_user))
					WITH CHECK (message_from = current_user)
		Application Users
			all examples use current_user
				but most database applications don't maintain 1:1 mapping to database users
				how to use RLS with application users?
			Session variables
				goal: pass additional context to database session
				ex
					SET my.username = 'tomas'
				my: new namespace
				useranem: a variable in the namespace
				ex: RLS
					CREATE POLCIY chat_policy ON chat
						USING (current_setting('my.username') IN (message_from, message_to))
						WITH CHECK (message_from = current_setting('my.username'))
				problem: what if  the user can set arbitrary username due to SQL injection
			Signed session variables
				goal: verify that the value was not subverted
				ex: trusted part (application) does:
					signature = sha256(username + timestamp + SECRET)
					# store both value and signature in session variable
					SET my.username = 'username:timestamp:signature'
					# the user cannot modify the value without invalidating the signature
				to protect SECRET value 
					store it in a table inaccessible by the user
					providing a security definer function
					requiring a password
					ex:
						CREATE FUNCTION set_username(uname TEXT, pwd TEXT) RETURNS text AS $$
						DECLARE 
							v_key TEXT;
							v_value TEXT;
						BEGIN
							SELECT sign_key INTO v_key FROM secrets;
							...
	Auditing Users and Roles in PostgreSQL
		https://blog.2ndquadrant.com/auditing-users-and-roles-in-postgresql/
		security reviews (audits)
			most issues: roles and privileges
		Owner is a small superuser
			initial checks:
				role should not 
					be a superuser
					have excessive privileges (ex: CREATEDB, CREATEROLE)
					own database objects (tables, functions, ...)
						since owners can grant arbitrary privileges
		Role inheritance
			split application into multiple parts
				into schemas
				each module uses a separate set of roles
			ex
				CREATE ROLE module_users;  -- user info
				CREATE ROLE module_users_ro;
				CREATE ROLE module_posts;  -- blog posts
				CREATE ROLE module_posts_ro;
				CREATE USER admin_user IN ROLE module_users, module_posts;
				CREATE USER web_user IN ROLE module_users_ro, module_posts_ro;
			roles used as groups
SOF PostgreSQL: Stackoverflow SOF Popular Questions   id=g10238
	meta
		how people ask questions? id=g10345
			how people ask questions?  <url:file:///~/projects/study/otl/cdb.otl#r=g10345>
			opt1: equivalent of the following mysql command
			opt2: i want to do this: select * ... but there is an error. 
			opt3: i can do this. but i want more in addition to it: ...
			opt4: i do this. but sometimes i get this error. how can i do this?
			opt5: show table sample and table ddl
					id | fixture
				 ----|----------
					 1 |    1
					 1 |    2
					 2 |    3
				 CREATE TABLE channel(
					 id INT NOT NULL PRIMARY KEY,
					 fixture INT NOT NULL
					 );
			opt6: prepare sqlfiddle page
			opt7: \d+ bss.amplifier_saturation
			https://stackoverflow.com/questions/653714/insert-results-of-a-stored-procedure-into-a-temporary-table
			https://stackoverflow.com/questions/452859/inserting-multiple-rows-in-a-single-sql-query
			https://stackoverflow.com/questions/63447/how-do-i-perform-an-if-then-in-an-sql-select
				q
					How do I perform an IF...THEN in an SQL SELECT statement?
					For example:
					SELECT IF(Obsolete = 'N' OR InStock = 'Y' ? 1 : 0) AS Saleable, * FROM Product
			https://stackoverflow.com/questions/109325/postgresql-describe-table
				q
					How do you perform the equivalent of Oracle's DESCRIBE TABLE in PostgreSQL (using the psql command)?
			https://stackoverflow.com/questions/769683/show-tables-in-postgresql
				q
					What's the alternative to SHOW TABLES (from MySQL) in PostgreSQL?
			https://stackoverflow.com/questions/1109061/insert-on-duplicate-update-in-postgresql
				q
					Several months ago I learned from an answer on Stack Overflow how to perform multiple updates at once in MySQL using the following syntax:
					INSERT INTO table (id, field, field2) VALUES (1, A, X), (2, B, Y), (3, C, Z)
					ON DUPLICATE KEY UPDATE field=VALUES(Col1), field2=VALUES(Col2);
			https://stackoverflow.com/questions/7869592/how-to-do-an-update-join-in-postgresql
				q
					Basically, I want to do this:
					UPDATE vehicles_vehicle v 
							JOIN shipments_shipment s on v.shipment_id=s.id 
					SET v.price=s.price_per_vehicle;
			https://stackoverflow.com/questions/2596670/how-do-you-find-the-row-count-for-all-your-tables-in-postgres
				q
					I'm looking for a way to find the row count for all my tables in Postgres. I know I can do this one table at a time with:
					SELECT count(*) FROM table_name;
					but I'd like to see the row count for all the tables and then order by that to get an idea of how big all my tables are.
			https://stackoverflow.com/questions/4069718/postgres-insert-if-does-not-exist-already
				q
					because some of my rows are identical, I get the following error:
						psycopg2.IntegrityError: duplicate key value  
							violates unique constraint "hundred_pkey"
					How can I write an 'INSERT unless this row already exists' SQL statement?
					I've seen complex statements like this recommended:
					...
			https://stackoverflow.com/questions/16677191/sql-one-to-many
				q
					I cannot have a table :
					 id | fixture
					----|----------
						1 |    1
						1 |    2
						2 |    3
					CREATE TABLE channel(
						id INT NOT NULL PRIMARY KEY,
						fixture INT NOT NULL
						);
					...
			https://stackoverflow.com/questions/10292355/how-do-i-create-a-real-one-to-one-relationship-in-sql-server
				q
					I have two tables tableA and tableB, I set tableB's primary key as foreign key which references  tableA's primary. But when I use Entity Framework database-first, the model is 1 to 0..1.
					Any one know how to create real 1 to 1 relationship in database?
			https://stackoverflow.com/questions/15251761/sql-query-one-to-many-relationship
				q
					Example Employee table
					╔════╦══════╗
					║ ID ║ NAME ║
					╠════╬══════╣
					║  1 ║ Bob  ║
					║  2 ║ Tom  ║
					║  3 ║ John ║
					╚════╩══════╩
			https://dba.stackexchange.com/questions/90128/unused-index-in-range-of-dates-query/90183#90183
				code
					mustang=# \d+ bss.amplifier_saturation
																												 Table "bss.amplifier_saturation"
					 Column |           Type           |                             Modifiers                             | Storage | Description 
					--------+--------------------------+-------------------------------------------------------------------+---------+-------------
					 value  | integer                  | not null                                                          | plain   | 
					 target | integer                  | not null                                                          | plain   | 
			next
				https://stackoverflow.com/questions/17946221/sql-join-and-different-types-of-joins
	ref - examples sof postgresql id=g10706
		ref - examples sof postgresql <url:file:///~/projects/study/otl/cdb.otl#r=g10706>
		mnemonics 20181121 
			AND    s.status IS DISTINCT FROM 'ACTIVE';  -- avoid empty updates. see below
			SELECT *
				SELECT DISTINCT ON (address_id) *
				ORDER  BY purchased_at DESC;
			SELECT DISTINCT ON (customer)
			, CASE WHEN a.result = 0 THEN 0 ELSE 1 END AS result
			, CURRENT_DATE - 1 AS day
			JOIN   prod_hw_id p USING (hard_id)
			WHERE  ts >= '2013-11-06 00:00:00'  
			GROUP  BY 1,2,3
			WHERE aggr_date <= :mydate 
			CREATE INDEX user_msg_log_combo_idx ON user_msg_log (user_id, aggr_date DESC NULLS LAST)
			LEFT   JOIN (
				GROUP  BY 1
				) u ON u.car_id  = c.id
			WHERE  EXISTS (SELECT 1 FROM votes v WHERE v.post_id = p.id);
			JOIN   votes v ON v.post_id = p.id;
			SELECT count(DISTINCT p.id) AS post_ct
			, sum(sum(sp.payout)) OVER w
				WINDOW w AS (ORDER BY e.date, e.event_id)
			SELECT d1, d2, sv/sum(sv) OVER (PARTITION BY d1) AS share
			SELECT DISTINCT ON (a)
				ORDER  BY a, b DESC;
			SELECT row_number() OVER () AS id
			SELECT company_id, array_agg(employee ORDER BY company_id DESC)::text
				GROUP  BY 1;
			SELECT * FROM tracks WHERE artists @> '[{"name": "The Dirty Heads"}]';
			What is semi-join <url:file:///~/projects/study/otl/cdb.otl#r=g10687>
				WHERE EXISTS ( 
			JOIN delivery ON (delivery.magazine_id = subscription.magazine_id
				AND delivery.user_id = subscription.user_id)
			JOIN   msg ON msg.src_addr ~~ ('%38' || mnc.code || '%')
				JOIN   msg ON (msg.src_addr || '+' || msg.dst_addr) ~ (38 || mnc.code)
				select * from pg_class where oid = 'table1'::regclass; -- ok
				FROM   information_schema.tables 
				WHERE  table_schema = 'schema_name'
			pl
				1. do not mix psql meta-commands and SQL commands
				psql postgres -c "CREATE DATABASE mytemplate1 WITH ENCODING 'UTF8' TEMPLATE template0"
				psql -c 'CREATE DATABASE myDB TEMPLATE mytemplate1'
			anti-join
				WHERE  NOT EXISTS (
					WHERE  l.ip = i.ip
				LEFT   JOIN ip_location i USING (ip)  -- short for: ON i.ip = l.ip
					WHERE  i.ip IS NULL;
				EXCEPT ALL               -- ALL, to keep duplicate rows and make it faster
				WHERE  ip NOT IN (
			select  a||', '||b from foo
				SELECT concat_ws(', ', a::text, b::text) AS ab FROM foo;
				SELECT concat(a::text, b::text) AS ab FROM foo;
			SELECT $1.zipcode || ' - ' || $1.city || ', ' || $1.state;
			ORDER BY last_updated NULLS FIRST
			select id, unnest(string_to_array(elements, ',')) AS elem
			FROM   tbl t, unnest(t.arr) WITH ORDINALITY a(elem, nr);
			SELECT movie, string_agg(actor, ', ') AS actor_list
			SELECT split_part(col, ',', 1) AS col1
			SELECT DISTINCT
				date_trunc('minute', "when") AS minute
				, count(*) OVER (ORDER BY date_trunc('minute', "when")) AS running_ct
			SELECT tbl_a.col1 FROM tbl_a;
				SELECT col3(tbl_a);
				IMMUTABLE vs STABLE
			SELECT * FROM books WHERE returned_date > now()::date - 7
			SELECT f_val(2);  -- returns 'baz'
			'user''s log'
			b.date_completed >  a.dc::date + INTERVAL '1 DAY 7:20:00'
			SELECT * FROM persons WHERE "first_Name" = 'xyz';
			array_replace(ar, NULL, 0) <> ar
			coalesce( trim(stringexpression),'')=''
			expression IS DISTINCT FROM expression
			AND    u.login >= now()::date + interval '1h'
			AND    c.date_join  <  date '2010-01-01'
			to_timestamp('20/8/2013 14:52:49', 'DD/MM/YYYY hh24:mi:ss')
			'2013-08-20 14:52:49'::timestamp
			SELECT to_char(to_timestamp (4::text, 'MM'), 'TMmon')
			INSERT INTO products (
				ON CONFLICT DO UPDATE SET description=excluded.description;
			sql uses three-valued logic (3VL)
				SUM of 1 and NULL = 1
				1 + NULL = NULL
				aggregates ignore nulls but why not + operator?
				ex: SUM over zero tuples: shouldn't it be zero?
			learning SQL is harder than learning an ORM
			UNIQUE      (animal_name),
				EXCLUDE USING gist (animal_type WITH <>, cage WITH =)
			Exclusion Constraints are generalized SQL UNIQUE id=g10682
			ALTER TABLE a ADD EXCLUDE (i WITH =, j WITH =);
			overlaps &&
			ADD EXCLUDE USING gist
				(room WITH =, during WITH &&);
			PERIOD data type
			book: C.J.Date An Introduction to Database Systems
			Preface — Why is indexing a development task?
				database indexing: a development task
				db automatically creates an index for pk
				put "explain" command in front of a sql statement
				Visualizing index
					ORDER BY <INDEX COLUMN LIST>
					FETCH FIRST 100 ROWS ONLY
			, LIST_AGG(val, ', ')
			) t1(c1)
			WITH t1 (c1) AS (
			turn nesting into chaining
			Unit Tests on Transient Data
				WITH cart (product_id, qty) AS (
				VALUES (1, 2)
			EXTRACT(<field> FROM <expression>)
				CAST(<timestamp> AS [DATE|TIME])
				SUM(<expression>) FILTER (WHERE <condition>)
			SELECT CURRENT_DATE
				VALUES (CURRENT_DATE)
			Exception: Check Constraints
				they reject false, thus they accept true and unknown
		data types
			array
				Check if value exists in Postgres array <url:file:///~/projects/study/otl/cdb.otl#r=g10696>
					SELECT '{1,2,3}'::int[] @> ARRAY[value_variable::int]
					SELECT value_variable = ANY ('{1,2,3}'::int[])
				IN vs ANY operator in PostgreSQL <url:#r=g10422>
					IN () is equivalent to = ANY()
					SELECT 'foo' = ANY('{FOO,bar,%oo%}');
				How to use ANY instead of IN in a WHERE clause with Rails?  <url:#r=g10423>
					expr IN (subquery)
					expr IN (value [, ...])
					expr operator ANY (subquery)
					expr operator ANY (array expression)
					array constructor: ARRAY[1,2,3]
					array literal '{1,2,3}'
					ARRAY[1,2,3]::numeric[]
					'{1,2,3}'::bigint[]
					IN is a special case of ANY
					SELECT 'foo' LIKE ANY('{FOO,bar,%oo%}');
			Any downsides of using data type “text” for storing strings?  <url:#r=g10355>
				ALTER TABLE tbl ADD CONSTRAINT tbl_col_len CHECK (length(col) < 100);
			PostgreSQL: Which Datatype should be used for Currency <url:file:///~/projects/study/otl/cdb.otl#r=g10697>
				personally: store currency as "integer" representing Cents
			Cannot cast type numeric to boolean <url:file:///~/projects/study/otl/cdb.otl#r=g10698>
				ALTER TABLE products
					ALTER power_price DROP DEFAULT
				 ,ALTER power_price TYPE bool USING (power_price::int::bool)
				 ,ALTER power_price SET NOT NULL
				 ,ALTER power_price SET DEFAULT false;
		distinct
			How do I (or can I) SELECT DISTINCT on multiple columns?  <url:file:///~/projects/study/otl/cdb.otl#r=g10699>
				opt1:
					UPDATE sales
					SET    status = 'ACTIVE'
					WHERE  (saleprice, saledate) IN (
							SELECT saleprice, saledate
							FROM   sales
							GROUP  BY saleprice, saledate
							HAVING count(*) = 1 
							);
				opt2: much faster: NOT EXISTS
					UPDATE sales s
					SET    status = 'ACTIVE'
					WHERE  NOT EXISTS (
						 SELECT 1
						 FROM   sales s1
						 WHERE  s.saleprice = s1.saleprice
						 AND    s.saledate  = s1.saledate
						 AND    s.id <> s1.id                     -- except for row itself
						 );
					AND    s.status IS DISTINCT FROM 'ACTIVE';  -- avoid empty updates. see below
			PostgreSQL DISTINCT ON with different ORDER BY <url:file:///~/projects/study/otl/cdb.otl#r=g10700>
				SELECT DISTINCT ON (address_id) purchases.address_id, purchases.*
					FROM purchases
					WHERE purchases.product_id = 1
					ORDER BY purchases.purchased_at DESC
				But I get this error: ...
				opt: subquery
					SELECT *
					FROM  (
							SELECT DISTINCT ON (address_id) *
							FROM   purchases
							WHERE  product_id = 1
							) p
					ORDER  BY purchased_at DESC;
		group by - window - aggregate
			Select first row in each GROUP BY group? <url:#r=adb_015>
				SELECT DISTINCT ON (customer)
							 id, customer, total
				FROM   purchases
				ORDER  BY customer, total DESC, id;
			GROUP BY + CASE statement <url:#r=g10390>
				SELECT m.name
						 , a.type
						 , CASE WHEN a.result = 0 THEN 0 ELSE 1 END AS result
						 , CURRENT_DATE - 1 AS day
						 , count(*) AS ct
				FROM   attempt    a
				JOIN   prod_hw_id p USING (hard_id)
				JOIN   model      m USING (model_id)
				WHERE  ts >= '2013-11-06 00:00:00'  
				AND    ts <  '2013-11-07 00:00:00'
				GROUP  BY 1,2,3
				ORDER  BY 1,2,3;
			Optimize GROUP BY query to retrieve latest record per user <url:#r=adb_007>
				SELECT user_id, max(aggr_date), max(running_total) 
				FROM user_msg_log 
				WHERE aggr_date <= :mydate 
				GROUP BY user_id;
				CREATE INDEX user_msg_log_combo_idx
					ON user_msg_log (user_id, aggr_date DESC NULLS LAST)
			PostgreSQL - GROUP BY clause or be used in an aggregate function <url:#r=g10397>
				SELECT id, name, created_at, updated_at, u.ct
				FROM   cars c
				LEFT   JOIN (
						SELECT car_id, count(*) AS ct
						FROM   users
						GROUP  BY 1
						) u ON u.car_id  = c.id
				ORDER  BY u.ct DESC;
			GROUP BY and COUNT in PostgreSQL <url:#r=g10399>
				ex1: EXISTS
					SELECT count(*) AS post_ct
					FROM   posts p
					WHERE  EXISTS (SELECT 1 FROM votes v WHERE v.post_id = p.id);
				ex2: count(DISTINCT p.id)
					SELECT count(DISTINCT p.id) AS post_ct
					FROM   posts p
					JOIN   votes v ON v.post_id = p.id;
			Postgres window function and group by exception <url:#r=g10421>
				SELECT p.name
						 , e.event_id
						 , e.date
						 , sum(sum(sp.payout)) OVER w
						 - sum(sum(s.buyin  )) OVER w AS "Profit/Loss" 
				FROM   player            p
				JOIN   result            r ON r.player_id     = p.player_id  
				JOIN   game              g ON g.game_id       = r.game_id 
				JOIN   event             e ON e.event_id      = g.event_id 
				JOIN   structure         s ON s.structure_id  = g.structure_id 
				JOIN   structure_payout sp ON sp.structure_id = g.structure_id
																	AND sp.position     = r.position
				WHERE  p.player_id = 17 
				GROUP  BY e.event_id
				WINDOW w AS (ORDER BY e.date, e.event_id)
				ORDER  BY e.date, e.event_id;
			How to use a SQL window function to calculate a percentage of an aggregate <url:#r=g10428>
				WITH x AS (
						SELECT d1, d2, sum(v) AS sv
						FROM   test
						GROUP  BY d1, d2
						)
				SELECT d1, d2, sv/sum(sv) OVER (PARTITION BY d1) AS share
				FROM   x;
			PostgreSQL aggregate or window function to return just the last value <url:#r=g10430>
				SELECT DISTINCT ON (a)
							 a, b
				FROM  (
					 VALUES
						 (1, 'do not want this')
					 , (1, 'just want this')
					 ) sub(a, b)
				ORDER  BY a, b DESC;
			Calculating Cumulative Sum in PostgreSQL <url:#r=g10381>
				SELECT ea_month, id, amount, ea_year, circle_id
						 , sum(amount) OVER (PARTITION BY circle_id ORDER BY month) AS cum_amt
				FROM   tbl
				ORDER  BY circle_id, month;
			Generate id row for a view with grouping <url:#r=g10432>
				CREATE OR REPLACE VIEW daily_transactions as
				SELECT row_number() OVER () AS id
						 , t.ic
						 , t.bio_id
						 , t.wp 
						 , t.transaction_time::date AS transaction_date
						 , min(t.transaction_time)::time AS time_in
						 , w.start_time AS wp_start
						 , w.start_time - min(t.transaction_time)::time AS in_diff
						 , max(t.transaction_time)::time AS time_out
						 , w.end_time AS wp_end
						 , max(t.transaction_time)::time - w.end_time AS out_diff
						 , count(*) AS total_transactions
						 , calc_att_status(t.transaction_time::date, min(t.transaction_time)::time
														 , max(t.transaction_time)::time
														 , w.start_time, w.end_time) AS status
				FROM   transactions t
				LEFT   JOIN wp w ON t.wp = w.wp_name
				GROUP  BY t.ic, t.bio_id, t.wp, t.transaction_time::date
								, w.start_time, w.end_time;
			Aggregate strings in descending order in a PostgreSQL query <url:#r=g10433>
				SELECT company_id, array_agg(employee ORDER BY company_id DESC)::text
				FROM   tbl
				GROUP  BY 1;
		index
			Index for finding an element in a JSON array <url:#r=g10349>
				CREATE TABLE tracks (id SERIAL, artists JSON);
				INSERT INTO tracks (id, artists) 
					VALUES (1, '[{"name": "blink-182"}]');
				SELECT * FROM tracks WHERE artists @> '[{"name": "The Dirty Heads"}]';
		joins
			What is semi-join <url:file:///~/projects/study/otl/cdb.otl#r=g10687>
				SELECT * 
				FROM Customers C 
				WHERE EXISTS ( 
					SELECT * 
					FROM Sales S 
					WHERE S.Cust_Id = C.Cust_Id 
					) 
			Find records where join doesn't exist <url:file:///~/projects/study/otl/cdb.otl#r=g10688>
				WHERE NOT EXISTS (
					 SELECT 1
					 FROM   votes v
					 WHERE  v.some_id = base_table.some_id
					 AND    v.user_id = ?
					 )
			What is the difference between LATERAL and a subquery in PostgreSQL <url:#r=g10370>
				ex
					CREATE TABLE tbl (a1 int[], a2 int[]);
					SELECT *
					FROM   tbl t
							 , unnest(t.a1, t.a2) u(elem1, elem2);  -- implicit LATERAL
				ex
					SELECT *, unnest(t.a1) AS elem1, unnest(t.a2) AS elem2
					FROM   tbl t;
			Multiple left joins on multiple tables in one query <url:#r=g10410>
				SELECT something
				FROM   master      parent
				JOIN   master      child ON child.parent_id = parent.id
				LEFT   JOIN second parentdata ON parentdata.id = parent.secondary_id
				LEFT   JOIN second childdata ON childdata.id = child.secondary_id
				WHERE  parent.parent_id = 'rootID'
			Join tables on columns of composite foreign / primary key in a query <url:file:///~/projects/study/otl/cdb.otl#r=g10689>
				opt01
					SELECT *
					FROM subscription
					JOIN delivery ON (delivery.magazine_id = subscription.magazine_id
												AND delivery.user_id = subscription.user_id)
				opt02
					SELECT *
					FROM subscription NATURAL JOIN delivery
			How to join tables on regex <url:#r=g10435>
				ex1: LIKE
					SELECT msg.message
								,msg.src_addr
								,msg.dst_addr
								,mnc.name
					FROM   mnc
					JOIN   msg ON msg.src_addr ~~ ('%38' || mnc.code || '%')
										 OR msg.dst_addr ~~ ('%38' || mnc.code || '%')
					WHERE  length(mnc.code) = 3
				ex2: regex
					SELECT msg.message
								,msg.src_addr
								,msg.dst_addr
								,mnc.name
					FROM   mnc
					JOIN   msg ON (msg.src_addr || '+' || msg.dst_addr) ~ (38 || mnc.code)
										 AND length(mnc.code) = 3
		metaprogramming
			What does regclass signify in Postgresql <url:file:///~/projects/study/otl/cdb.otl#r=g10690>
				select * from pg_class where oid = 'table1'::regclass; -- ok
				select * from pg_class where oid = 'table1'; -- wrong
			How to check if a table exists in a given schema <url:file:///~/projects/study/otl/cdb.otl#r=g10691>
				opt1: information_schema
					SELECT EXISTS (
						 SELECT 1
						 FROM   information_schema.tables 
						 WHERE  table_schema = 'schema_name'
						 AND    table_name = 'table_name'
						 );
		queries
			Select rows which are not present in other table <url:#r=g10347>
				opt1: NOT EXISTS (fastest) = anti-join
					SELECT ip 
					FROM   login_log l 
					WHERE  NOT EXISTS (
						 SELECT 1              -- it's mostly irrelevant what you put here
						 FROM   ip_location i
						 WHERE  l.ip = i.ip
						 );
				opt2: LEFT JOIN - shortest
					SELECT l.ip 
					FROM   login_log l 
					LEFT   JOIN ip_location i USING (ip)  -- short for: ON i.ip = l.ip
					WHERE  i.ip IS NULL;
				opt3: EXCEPT: not for complex queries
					SELECT ip 
					FROM   login_log
					EXCEPT ALL               -- ALL, to keep duplicate rows and make it faster
					SELECT ip
					FROM   ip_location;
				opt4: NOT IN: only if no NULL values exist
					SELECT ip 
					FROM   login_log
					WHERE  ip NOT IN (
						 SELECT DISTINCT ip  -- DISTINCT is optional
						 FROM   ip_location
						 );
			Best way to select random rows PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10692>
				SELECT *
				FROM  (
						SELECT DISTINCT 1 + trunc(random() * 5100000)::integer AS id
						FROM   generate_series(1, 1100) g
						) r
				JOIN   big USING (id)
				LIMIT  1000;
			How to concatenate columns in a Postgres SELECT?  <url:#r=g10353>
				SELECT a::text || ', ' || b::text AS ab FROM foo;
				SELECT concat_ws(', ', a::text, b::text) AS ab FROM foo;
				SELECT concat(a::text, b::text) AS ab FROM foo;
			Combine two columns and add into one new column <url:#r=g10354>
				CREATE FUNCTION combined(rec tbl)
					RETURNS text
					LANGUAGE SQL
				AS $$
					SELECT $1.zipcode || ' - ' || $1.city || ', ' || $1.state;
				$$;
				SELECT *, tbl.combined FROM tbl;
				SELECT concat(col_a, col_b);
			PostgreSQL unnest() with element number <url:#r=g10363>
				SELECT t.id, a.elem, a.nr
				FROM   tbl t, unnest(t.arr) WITH ORDINALITY a(elem, nr);
			Manual: WITH ORDINALITY <url:#r=g10364>
				SELECT * FROM pg_ls_dir('.') WITH ORDINALITY AS t(ls,n);
			Concatenate multiple result rows of one column into one, group by another column [duplicate] <url:#r=g10365>
				SELECT movie, string_agg(actor, ', ') AS actor_list
				FROM   tbl
				GROUP  BY 1;
			How to select id with max date group by category in PostgreSQL <url:#r=g10368>
				SELECT DISTINCT ON (category)
							 id
				FROM   tbl
				ORDER  BY category, "date" DESC;
			PostgreSQL: running count of rows for a query 'by minute' <url:#r=adb_014>
				SELECT DISTINCT
							 date_trunc('minute', "when") AS minute
						 , count(*) OVER (ORDER BY date_trunc('minute', "when")) AS running_ct
				FROM   mytable
				ORDER  BY 1;
			Split comma separated column data into additional columns <url:#r=g10382>
				SELECT split_part(col, ',', 1) AS col1
						 , split_part(col, ',', 2) AS col2
						 , split_part(col, ',', 3) AS col3
						 , split_part(col, ',', 4) AS col4
				FROM   tbl;
			Store common query as column <url:#r=g10384>
				CREATE FUNCTION col3(tbl_a)
					RETURNS int8 AS
				$func$
						SELECT sum(colx)
						FROM   tbl_b b
						WHERE  b.a_id = $1.a_id
				$func$ LANGUAGE SQL STABLE;
				SELECT a_id, col1, col2, tbl_a.col3
				FROM   tbl_a;
				SELECT tbl_a.col3 FROM tbl_a;
				SELECT col3(tbl_a);
			Best way to delete millions of rows by ID <url:#r=g10387>
				temp
					CREATE TEMP TABLE tmp AS
					SELECT t.*
					FROM   tbl t
					LEFT   JOIN del_list d USING (id)
					WHERE  d.id IS NULL;      -- copy surviving rows into temporary table
					TRUNCATE tbl;             -- empty table - truncate is very fast for big tables
				insert back
					INSERT INTO tbl
					SELECT * FROM tmp;        -- insert back surviving rows.
			Finding similar strings with PostgreSQL quickly <url:#r=g10386>
				SELECT set_limit(0.8);   -- fewer hits and faster with higher limit
				SELECT similarity(n1.name, n2.name) AS sim, n1.name, n2.name
				FROM   t n1
				JOIN   t n2 ON n1.name <> n2.name
									 AND n1.name % n2.name
				ORDER  BY sim DESC;
			Alphanumeric sorting with PostgreSQL <url:#r=g10395>
				WITH x(t) AS (
						VALUES
						 ('10_asdaasda')
						,('100_inkskabsjd')
						,('11_kancaascjas')
						,('45_aksndsialcn')
						,('22_dsdaskjca')
						,('100_skdnascbka')
						)
				SELECT t
				FROM   x
				ORDER  BY (substring(t, '^[0-9]+'))::int     -- cast to integer
									,substring(t, '[^0-9_].*$')        -- works as text
			How do you find results that occurred in the past week?  <url:#r=g10398>
				SELECT * FROM books WHERE returned_date > now()::date - 7
				SELECT * FROM books WHERE returned_date > current_date - interval '7 days'
			ORDER BY the IN value list <url:#r=g10408>
				SELECT * FROM comments WHERE (comments.id IN (1,3,2,4));
				opt1: VALUES ()
					select c.*
					from comments c
					join (
						values
							(1,1),
							(3,2),
							(2,3),
							(4,4)
					) as x (id, ordering) on c.id = x.id
					order by x.ordering
			How to delete duplicate entries?  <url:#r=g10409>
				SELECT pg_size_pretty(pg_relation_size('tbl'));
				SET temp_buffers = 200MB;    -- example value
				BEGIN;
				-- CREATE TEMPORARY TABLE t_tmp ON COMMIT DROP AS -- drop temp table at commit
				CREATE TEMPORARY TABLE t_tmp AS  -- retain temp table after commit
				SELECT DISTINCT * FROM tbl;  -- DISTINCT folds duplicates
				TRUNCATE tbl;
				INSERT INTO tbl
				SELECT * FROM t_tmp;
				-- ORDER BY id; -- optionally "cluster" data while being at it.
				COMMIT;
			Is there a way to define a named constant in a PostgreSQL query?  <url:#r=g10411>
				opt01
					CREATE FUNCTION public.f_myid()
						RETURNS int IMMUTABLE LANGUAGE SQL AS
					'SELECT 5';
				opt02
					CREATE TEMP TABLE val (val_id int PRIMARY KEY, val text);
					INSERT INTO val(val_id, val) VALUES
						(  1, 'foo')
					, (  2, 'bar')
					, (317, 'baz');
					CREATE FUNCTION f_val(_id int)
						RETURNS text STABLE LANGUAGE SQL AS
					'SELECT val FROM val WHERE val_id = $1';
					SELECT f_val(2);  -- returns 'baz'
			Postgres Error: More than one row returned by a subquery used as an expression <url:file:///~/projects/study/otl/cdb.otl#r=g10693>
				UPDATE customer c
				SET    customer_id = s.store_key
				FROM   dblink('port=5432, dbname=SERVER1 user=postgres password=309245'
										 ,'SELECT match_name, store_key FROM store')
							 AS s(match_name text, store_key integer)
				WHERE c.match_name = s.match_name
				AND   c.customer_id IS DISTINCT FROM s.store_key;
			Pass In “WHERE” parameters to PostgreSQL View?  <url:#r=g10414>
				CREATE OR REPLACE FUNCTION param_labels(_region_label text, _model_label text)
					RETURNS TABLE (param_label text, param_graphics_label text) AS
				$func$
						SELECT p.param_label, p.param_graphics_label
						FROM   parameters      p 
						JOIN   parameter_links l USING (param_id)
						JOIN   regions         r USING (region_id)
						JOIN   models          m USING (model_id)
						WHERE  p.active
						AND    r.region_label = $1 
						AND    m.model_label = $2
						ORDER  BY p.param_graphics_label;
				$func$ LANGUAGE sql;
			SQL update fields of one table from fields of another one <url:#r=g10419>
				opt1: General solution with dynamic SQL
					code
						DO
						$do$
						BEGIN
						EXECUTE (
						SELECT
						'UPDATE b
						 SET   (' || string_agg(quote_ident(column_name), ',') || ')
								 = (' || string_agg('a.' || quote_ident(column_name), ',') || ')
						 FROM   a
						 WHERE  b.id = 123
						 AND    a.id = b.id'
						FROM   information_schema.columns
						WHERE  table_name   = 'a'       -- table name, case sensitive
						AND    table_schema = 'public'  -- schema name, case sensitive
						AND    column_name <> 'id'      -- all columns except id
						);
						END
						$do$;
				opt2: plain SQL with list of shared columns
					code
						UPDATE b
						SET   (  column1,   column2,   column3)
								= (a.column1, a.column2, a.column3)
						FROM   a
						WHERE  b.id = 123    -- optional, to update only selected row
						AND    a.id = b.id;
			Multiple CTE in single query <url:#r=g10425>
				WITH RECURSIVE
					cte1 AS (...)  -- can still be non-recursive
				, cte2 AS (SELECT ...
									 UNION ALL
									 SELECT ...)  -- recursive term
				, cte3 AS (...)
				SELECT ... FROM cte3 WHERE ...
			Find difference between two big tables in PostgreSQL <url:#r=g10426>
				opt1: EXISTS anti-semi-join:
					SELECT *
					FROM   tbl1
					WHERE  NOT EXISTS (SELECT 1 FROM tbl2 WHERE tbl2.col = tbl1.col);
				opt2: FULL OUTER JOIN
					SELECT *
					FROM   tbl1
					FULL   OUTER JOIN tbl2 USING (col)
					WHERE  tbl2 col IS NULL OR
								 tbl1.col IS NULL;
			Guidance on using the WITH clause in SQL <url:#r=g10427>
				ex1
					WITH x AS (
						 SELECT  psp_id
						 FROM    global.prospect
						 WHERE   status IN ('new', 'reset')
						 ORDER   BY request_ts
						 LIMIT   1
						 )
					UPDATE global.prospect psp
					SET    status = status || '*'
					FROM   x
					WHERE  psp.psp_id = x.psp_id
					RETURNING psp.*;
				ex2
					WITH x AS (
						 SELECT  psp_id
						 FROM    global.prospect
						 WHERE   status IN ('new', 'reset')
						 ORDER   BY request_ts
						 LIMIT   1
						 ), y AS (
						 UPDATE global.prospect psp
						 SET    status = status || '*'
						 FROM   x
						 WHERE  psp.psp_id = x.psp_id
						 RETURNING psp.*
						 )
					INSERT INTO z
					SELECT *
					FROM   y
			Nesting queries in SQL <url:#r=g10429>
				SELECT country.name as country, country.headofstate 
				FROM country
				INNER JOIN city ON city.id = country.capital
				WHERE city.population > 100000
				AND country.headofstate LIKE 'A%'
			Select NOT IN multiple columns <url:file:///~/projects/study/otl/cdb.otl#r=g10694>
				opt1
					SELECT * FROM friend f
					WHERE NOT EXISTS (
							SELECT 1 FROM likes l WHERE f.id1 = l.id and f.id2 = l.id2
					)
				opt2
					SELECT *
					FROM   friend f
					LEFT   JOIN likes l USING (id1, id2)
					WHERE  l.id1 IS NULL;
			Find The Missing Integer <url:file:///~/projects/study/otl/cdb.otl#r=g10695>
				ex
					SELECT i FROM generate_series(1,100) as t(i)
					EXCEPT
					SELECT i FROM ints;
				opt: Anti join technique
					SELECT series.i
					FROM ints
					RIGHT JOIN (SELECT i
						FROM generate_series(1,100) t(i)
						) series
					ON series.i = ints.i
					WHERE ints.i IS NULL;
				opt: NOT EXISTS
					SELECT i
					FROM generate_series(1,100) as t(i)
					WHERE NOT EXISTS (SELECT 1
						FROM ints
						WHERE ints.i = t.i);
		syntax rules
			Difference between LIKE and ~ in Postgres <url:#r=g10420>
				LIKE (~~) fastest
				~ is regex: more powerful
				never use: SIMILAR TO
				install pg_trgm: to use similarity operator %
				SELECT * FROM tbl WHERE string ~ '^1234';  -- left anchored pattern
		other
			null handling
				Check if NULL exists in Postgres array <url:#r=g10350>
					array_replace(ar, NULL, 0) <> ar
					array_remove(ar, NULL) <> ar
					array_position(ar, NULL) IS NOT NULL
					CREATE OR REPLACE FUNCTION f_array_has_null (anyarray)
						RETURNS bool LANGUAGE sql IMMUTABLE AS
					 'SELECT array_position($1, NULL) IS NOT NULL';
				Best way to check for “empty or null value” <url:#r=g10357>
					stringexpression = '' yields:
						TRUE   .. for '' (or for any string consisting of only spaces with the data type char(n))
						NULL   .. for NULL
						FALSE .. for anything else
					"stringexpression is either NULL or empty":
						(stringexpression = '') IS NOT FALSE
						coalesce(stringexpression, '') = ''
					stringexpression is neither NULL nor empty":
						stringexpression <> ''
				Sort NULL values to the end of a table <url:#r=g10376>
					NULL are sorted last in default ascending order
					in descending: NULL first
						ORDER BY col DESC NULLS LAST
				Why does PostgreSQL not return null values when the condition is <> true <url:#r=g10403>
					returns null:
						7 = NULL 
						7 <> NULL
					don't return null:
						expression IS DISTINCT FROM expression
						expression IS NOT DISTINCT FROM expression
			temporal date data types
				Select today's (since midnight) timestamps only <url:#r=g10405>
					SELECT u.login, u.id, u.first_name
					FROM   pref_users u
					WHERE  u.login > u.logout
					AND    u.login >= now()::date + interval '1h'
					ORDER  BY u.login;
				Find overlapping date ranges in PostgreSQL <url:#r=g10388>
					a BETWEEN x AND y translates to:
						a >= x AND a <= y
						Including the upper border, while people typically need to exclude it:
						a >= x AND a < y
					correct answer:
						SELECT DISTINCT p.* 
						FROM   contract c
						JOIN   player   p USING (name_player) 
						WHERE  c.name_team = ? 
						AND    c.date_join  <  date '2010-01-01'
						AND   (c.date_leave >= date '2009-01-01' OR c.date_leave IS NULL);
				How to convert “string” to “timestamp without time zone” <url:#r=g10400>
					'2013-08-20 14:52:49'::timestamp
					to_timestamp('20/8/2013 14:52:49', 'DD/MM/YYYY hh24:mi:ss')
				Get month name from number in PostgreSQL <url:#r=g10412>
					SELECT to_char(to_timestamp (4::text, 'MM'), 'TMmon')
				Selecting records between two timestamps <url:#r=g10413>
					SELECT a,b,c
					FROM   table
					WHERE  xtime BETWEEN '2012-04-01 23:55:00'::timestamp
													 AND now()::timestamp;
	by topic
		custom functions
			Return setof record (virtual table) from function id=g10346
				Return setof record (virtual table) from function <url:file:///~/projects/study/otl/cdb.otl#r=g10346>
				https://stackoverflow.com/questions/955167/return-setof-record-virtual-table-from-function
				code
					CREATE OR REPLACE FUNCTION foo(open_id numeric)
						RETURNS TABLE (a int, b int, c int) AS
					$func$
					BEGIN
						 -- do something with open_id?
						 RETURN QUERY VALUES
							 (1,2,3)
						 , (3,4,5)
						 , (3,4,5);
					END
					$func$  LANGUAGE plpgsql IMMUTABLE ROWS 3;
				Call:
					SELECT * FROM foo(1);
				major points
					Use RETURNS TABLE to define an ad-hoc row type to return.
					Or RETURNS SETOF mytbl to use a pre-defined row type.
					Use RETURN QUERY to return multiple rows with one command.
					Use a VALUES expression to enter multiple rows manually. This is standard SQL and has been around for ever.
					Use a parameter name (open_id numeric) instead of ALIAS, which is discouraged for standard parameter names. In the example the parameter isn't used and just noise ...
					No need for double-quoting perfectly legal identifiers. Double-quotes are only needed to force otherwise illegal names (mixed-case, illegal characters or reserved words).
			How do you declare a set-returning-function to only be allowed in the FROM clause? id=adb_008
				How do you declare a set-returning-function to only be allowed in the FROM clause? <url:#r=adb_008>
				https://dba.stackexchange.com/questions/160309/how-do-you-declare-a-set-returning-function-to-only-be-allowed-in-the-from-claus/160310#160310
				q
					"unnest" is only allowed in FROM clause
					unnest is set-returning function
					How does one declare their set returning function to be FROM-clause only, or is this only permissible with unnest?
				ans
					this is only for unnest()
					unnest() with multiple parameters is a special Postgres feature, that's internally rewritten into multiple unnest() calls
					The special table function UNNEST may be called with any number of array parameters, and it returns a corresponding number of columns, as if UNNEST (Section 9.18) had been called on each parameter separately and combined using the ROWS FROM construct.
					If you try unnest() with multiple parameters in the SELECT list, you get:
						ERROR: function unnest(text[], text[]) does not exist
					In fact, there is no unnest() function with multiple parameters registered in the system:
						SELECT proname, proargtypes, proargtypes[0]::regtype
						FROM   pg_proc
						WHERE  proname = 'unnest';
						 proname | proargtypes | proargtypes
						---------+-------------+-------------
						 unnest  | 2277        | anyarray
						 unnest  | 3614        | tsvector
			Table name as a PostgreSQL function parameter id=g10372
				Table name as a PostgreSQL function parameter <url:file:///~/projects/study/otl/cdb.otl#r=g10372>
				https://stackoverflow.com/questions/10705616/table-name-as-a-postgresql-function-parameter/10711349#10711349
				ans
					function
						CREATE OR REPLACE FUNCTION some_f(_tbl regclass, OUT result boolean) AS
						$func$
						BEGIN
						EXECUTE format('SELECT EXISTS (SELECT 1 FROM %s WHERE id = 1)', _tbl)
						INTO result;
						END
						$func$ LANGUAGE plpgsql;
					call
						SELECT some_f('myschema.mytable');  -- would fail with quote_ident()
					points
						Use an OUT parameter to simplify the function. You can directly select the result of the dynamic SQL into it and be done.
						I use the object identifier type regclass as input type for _tbl. That does everything quote_ident(_tbl) or format('%I', _tbl) would do, but better, because:
							it prevents SQL injection just as well.
							it fails immediately and more gracefully if the table name is invalid / does not exist / is invisible to the current user.
							it works with schema-qualified table names, where a plain quote_ident(_tbl) or format(%I) would fail because they cannot resolve the ambiguity.
			How to return result of a SELECT inside a function in PostgreSQL id=g10380
				How to return result of a SELECT inside a function in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10380>
				https://stackoverflow.com/questions/7945932/how-to-return-result-of-a-select-inside-a-function-in-postgresql/7945958#7945958
				ans
					Use RETURN QUERY:
					function
						CREATE OR REPLACE FUNCTION word_frequency(_max_tokens int)
							RETURNS TABLE (
								txt   text   -- visible as OUT parameter inside and outside function
							, cnt   bigint
							, ratio bigint) AS
						$func$
						BEGIN
							 RETURN QUERY
							 SELECT t.txt
										, count(*) AS cnt  -- column alias only visible inside
										, (count(*) * 100) / _max_tokens  -- I added brackets
							 FROM  (
									SELECT t.txt
									FROM   token t
									WHERE  t.chartype = 'ALPHABETIC'
									LIMIT  _max_tokens
									) t
							 GROUP  BY t.txt
							 ORDER  BY cnt DESC;  -- note the potential ambiguity 
						END
						$func$  LANGUAGE plpgsql;
					Call:
						SELECT * FROM word_frequency(123);
					points
						It is much more practical to explicitly define the return type than simply declaring it as record. This way you don't have to provide a column definition list with every function call. RETURNS TABLE is one way to do that
						Choose names for OUT parameters carefully. They are visible in the function body almost anywhere. Table-qualify columns of the same name to avoid conflicts or unexpected results
						Don't to use "text" and "count" as column names
							txt, cnt better
						final explicit RETURN statement is not required (but allowed) when working with OUT parameters or RETURNS TABLE (which makes implicit use of OUT parameters).
			Auto increment SQL function
				https://stackoverflow.com/questions/9875223/auto-increment-sql-function/9875517#9875517
				ans
					CREATE TABLE staff (
						staff_id serial PRIMARY KEY,
						staff    text NOT NULL
					);
					"id" as name is an anti-pattern, used by some middle-ware, but hardly descriptive. Similar with "name".
			DROP FUNCTION without knowing the number/type of parameters?
				https://stackoverflow.com/questions/7622908/drop-function-without-knowing-the-number-type-of-parameters/7623246#7623246
				ans
					code
						SELECT 'DROP FUNCTION ' || oid::regprocedure
						FROM   pg_proc
						WHERE  proname = 'my_function_name'  -- name without schema-qualification
						AND    pg_function_is_visible(oid);  -- restrict to current search_path ..
																								 -- .. you may or may not want this
					Output:
						DROP FUNCTION my_function_name(string text, form text, maxlen integer);
						DROP FUNCTION my_function_name(string text, form text);
						DROP FUNCTION my_function_name(string text);
			Return setof record (virtual table) from function id=g10402
				Return setof record (virtual table) from function <url:file:///~/projects/study/otl/cdb.otl#r=g10402>
				https://stackoverflow.com/questions/23060256/postgres-transpose-rows-to-columns/23061730#23061730
				q
					goal
						user.name | user.id | email1          | email2           | email3**
						Mary      | 123     | mary@gmail.com  | mary@yahoo.co.uk | mary@test.com
						Joe       | 345     | joe@gmail.com   | [NULL]           | [NULL]
				ans
					use crosstab
						ref
							PostgreSQL Crosstab Query <url:#r=adb_010>
							Pivot on Multiple Columns using Tablefunc <url:#r=adb_011>
						SELECT * FROM crosstab(
							 $$SELECT user_id, user_name, rn, email_address
								 FROM  (
										SELECT u.user_id, u.user_name, e.email_address
												 , row_number() OVER (PARTITION BY u.user_id
																				ORDER BY e.creation_date DESC NULLS LAST) AS rn
										FROM   usr u
										LEFT   JOIN email_tbl e USING (user_id)
										) sub
								 WHERE  rn < 4
								 ORDER  BY user_id
							 $$
							, 'VALUES (1),(2),(3)'
							 ) AS t (user_id int, user_name text, email1 text, email2 text, email3 text);
			Inlining of SQL Functions id=adb_012 id=g10417
				Inlining of SQL Functions  id=g10417 <url:file:///~/projects/study/otl/cdb.otl#r=g10417>
				Inlining of SQL Functions <url:#r=adb_012>
				https://wiki.postgresql.org/wiki/Inlining_of_SQL_functions
				scalar functions
					func(args) appears in the context of a value expression or predicate, i.e. anywhere an ordinary value or condition can appear. 
					ex
						select func(t.foo) from sometable t;
						select t.* from sometable t where func(t.foo, 123);
				table functions
					func() appears where a table is expected
					ex
						select * from func(123);
						select * from sometable t, func(t.foo, 123);   -- version 9.3+, implicitly LATERAL
			Return SETOF rows from PostgreSQL function id=g10418
				Return SETOF rows from PostgreSQL function <url:file:///~/projects/study/otl/cdb.otl#r=g10418>
				https://stackoverflow.com/questions/17864911/return-setof-rows-from-postgresql-function/17865708#17865708
				ans
					code
						CREATE OR REPLACE FUNCTION func_a(username text = '', databaseobject text = '')
							RETURNS ???? AS
						$func$
						BEGIN
						RETURN QUERY EXECUTE
						format ('SELECT * FROM %s v1 LEFT JOIN %I v2 USING (id)'
									 , CASE WHEN username = '*' THEN 'view1' ELSE 'view3' END, databaseobject);
						END
						$func$  LANGUAGE plpgsql;
					return type of dynamic query
						ref
							for advanced options
								Refactor a PL/pgSQL function to return the output of various SELECT queries <url:#r=adb_013>
						opt1: existing table/view/composite type
							CREATE FUNCTION foo()
								RETURNS SETOF my_view AS
						opt2: making the type up as you go, use anonymous records
							CREATE FUNCTION foo()
								RETURNS SETOF record AS
							cost:
								you will provide column list with every call
						opt3: provide a column definition list with RETURNS TABLE
							CREATE FUNCTION foo()
								RETURNS TABLE (col1 int, col2 text, ...) AS
						note: I wouldn't use SELECT * to begin with
							use a definitive list of columns to return
							code
								CREATE OR REPLACE FUNCTION func_a(username text = '', databaseobject text = '')
									RETURNS TABLE(col1 int, col2 text, col3 date) AS
								$func$
								BEGIN
								RETURN QUERY EXECUTE
								format ('SELECT v1.col1, v1.col2, v2.col3
												 FROM %s v1 LEFT JOIN %I v2 USING (id)$f$
											 , CASE WHEN username = '*' THEN 'view1' ELSE 'view3' END, databaseobject);
								END
								$func$;
						note: for completely dynamic queries, I'd use plain SQL query not a function
			Set Returning Functions and PostgreSQL 10
				http://tapoueh.org/blog/2017/10/set-returning-functions-and-postgresql-10/
				A Set Returning Function is a PostgreSQL Stored Procedure that can be used as a relation
				before
					select case jsonb_typeof(booster)
											when 'array'
											then initcap(jsonb_array_elements_text(booster))
											else initcap(booster #>> '{}')
									end
								 as rarity,
								 count(*)
						from magic.sets,
								 jsonb_array_elements(data->'booster') booster
					group by rarity
					order by count desc;
				The jsonb_array_elements_text() is a Set Returning Function as can be seen here:
					> \df jsonb_array_elements_text
					List of functions
					─[ RECORD 1 ]───────┬────────────────────────────────
					Schema              │ pg_catalog
					Name                │ jsonb_array_elements_text
					Result data type    │ SETOF text
					Argument data types │ from_json jsonb, OUT value text
					Type                │ normal
		data modelling
			PostgreSQL create table if not exists id=g10702
				PostgreSQL create table if not exists <url:file:///~/projects/study/otl/cdb.otl#r=g10702>
					https://stackoverflow.com/questions/1766046/postgresql-create-table-if-not-exists/7438222#7438222
					CREATE TABLE IF NOT EXISTS myschema.mytable (i integer);
			How to implement a many-to-many relationship in PostgreSQL? id=g10176
				How to implement a many-to-many relationship in PostgreSQL? <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10176>
				https://stackoverflow.com/questions/9789736/how-to-implement-a-many-to-many-relationship-in-postgresql/9790225#9790225
				ans
					The DDL statements could look like this:
						CREATE TABLE product (
							product_id serial PRIMARY KEY  -- implicit primary key constraint
						, product    text NOT NULL
						, price      numeric NOT NULL DEFAULT 0
						);
						CREATE TABLE bill (
							bill_id  serial PRIMARY KEY
						, bill     text NOT NULL
						, billdate date NOT NULL DEFAULT CURRENT_DATE
						);
						CREATE TABLE bill_product (
							bill_id    int REFERENCES bill (bill_id) ON UPDATE CASCADE ON DELETE CASCADE
						, product_id int REFERENCES product (product_id) ON UPDATE CASCADE
						, amount     numeric NOT NULL DEFAULT 1
						, CONSTRAINT bill_product_pkey PRIMARY KEY (bill_id, product_id)  -- explicit pk
						);
					major points
						use: IDENTITY instead of serial
						Another widespread anti-pattern would be just id as column name.
							I am not sure what the name of a bill would be. Maybe bill_id can be the name in this case.
						ON UPDATE CASCADE ON DELETE CASCADE
							you cannot delete product rows. but if you wish, you can mark them as "obsolete"
						sequence of key columns is relevant in multicolumn keys
							(bill_id, product_id) assumes queries look for bill_id always 
			Change PostgreSQL columns used in views id=g10356
				Change PostgreSQL columns used in views <url:file:///~/projects/study/otl/cdb.otl#r=g10356>
				https://stackoverflow.com/questions/8524873/change-postgresql-columns-used-in-views/8527792#8527792
				q
					Let's say I have the following:
						CREATE TABLE monkey
						(
							"name" character varying(50) NOT NULL,
						)
						CREATE OR REPLACE VIEW monkey_names AS 
						 SELECT name
							 FROM monkey
					I really just want to do the following in a migration script without having to drop and recreate the view.
						ALTER TABLE monkey ALTER COLUMN "name" character varying(100) NOT NULL
				ans
					use: "text" without length specifier
					CREATE TABLE monkey(name text NOT NULL)
					If you really want to enforce a maximum length, create a CHECK constraint:
						ALTER TABLE monkey 
							ADD CONSTRAINT monkey_name_len CHECK (length(name) < 101);
					points:
						view is not just an alias to subquery
							views are special tables with a rule:
								ON SELECT TO my_view DO INSTEAD
							you can GRANT, add comments, define column defaults
			Creating temporary tables in SQL id=g10378
				Creating temporary tables in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10378>
				https://stackoverflow.com/questions/15691243/creating-temporary-tables-in-sql/15700736#15700736
				ans
					CREATE TEMP TABLE temp1 AS
					SELECT dataid
							 , register_type
							 , timestamp_localtime
							 , read_value_avg
					FROM   rawdata.egauge
					WHERE  register_type LIKE '%gen%'
					ORDER  BY dataid, timestamp_localtime
					This creates a temporary table and copies data into it. A static snapshot of the data, mind you. It's just like a regular table, but resides in RAM if temp_buffers is set high enough, is only visible within the current session and dies at the end of it. When created with ON COMMIT DROP it dies at the end of the transaction.
					Temp tables comes first in the default schema search path, hiding other visible tables of the same name unless schema-qualified:
			What's the difference between a catalog and a schema in a relational database?
				https://stackoverflow.com/questions/7022755/whats-the-difference-between-a-catalog-and-a-schema-in-a-relational-database#17943883
				ans1
					Cluster = Database Server
					Catalog = Database
					Cluster > Catalog > Schema > Table > Columns & Rows
					one cluster per port
						multiple clusters: multiple ports if on same machine
			Many to Many Table - Performance is bad id=g10389
				Many to Many Table - Performance is bad <url:file:///~/projects/study/otl/cdb.otl#r=g10389>
				https://stackoverflow.com/questions/46301784/many-to-many-table-performance-is-bad/46303473#46303473
				q
					The following tables are given:
					--- player --
					id serial
					name VARCHAR(100)
					birthday DATE
					country VARCHAR(3)
					PRIMARY KEY id
					--- club ---
					id SERIAL
					name VARCHAR(100)
					country VARCHAR(3)
					PRIMARY KEY id
					--- playersinclubs ---
					id SERIAL
					player_id INTEGER (with INDEX)
					club_id INTEGER (with INDEX)
					joined DATE
					left DATE
					PRIMARY KEY id
					query
						SELECT * FROM player
						 JOIN playersinclubs ON player.id = playersinclubs.player_id
						 JOIN club ON club.id = playersinclubs.club_id
						WHERE club.dbid = 3;
				ans
					need an index on playersinclubs(club_id, player_id).
					SELECT p.* 
					FROM   playersinclubs pc
					JOIN   player         p ON p.id = pc.player_id
					WHERE  pc.club_id = 3;
			Is there a postgres command to list/drop all materialized views? id=g10401
				Is there a postgres command to list/drop all materialized views?  <url:file:///~/projects/study/otl/cdb.otl#r=g10401>
				ans
					Show all:
						SELECT oid::regclass::text
						FROM   pg_class
						WHERE  relkind = 'm';
					drop: dynamic SQL
						SELECT 'DROP MATERIALIZED VIEW ' || string_agg(oid::regclass::text, ', ') 
						FROM   pg_class
						WHERE  relkind = 'm';
						-->
						DROP MATERIALIZED VIEW mv1, some_schema_not_in_search_path.mv2, ...
			Cannot create a new table after “DROP SCHEMA public”
				https://stackoverflow.com/questions/14285854/cannot-create-a-new-table-after-drop-schema-public/14286370#14286370
				ans
					check
						SHOW search_path;
					CREATE SCHEMA public;
						in database "template1"
		data types
			array
				Check if value exists in Postgres array id=g10696
					Check if value exists in Postgres array <url:file:///~/projects/study/otl/cdb.otl#r=g10696>
					https://stackoverflow.com/questions/11231544/check-if-value-exists-in-postgres-array/11231965#11231965
					q
						Edit: Just realized I could do this
							select '{1,2,3}'::int[] @> ARRAY[value_variable::int]
						This is much better and I believe will suffice, but if you have other ways to do it please share.
					ans
						ex
							SELECT value_variable = ANY ('{1,2,3}'::int[])
						ANY rhs: can be a set or an array
						points:
							does not work with NULL elements
				IN vs ANY operator in PostgreSQL id=g10422
					IN vs ANY operator in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10422>
					https://stackoverflow.com/questions/34627026/in-vs-any-operator-in-postgresql/34627688#34627688
					ans
						IN () is equivalent to = ANY()
							SELECT 'foo' = ANY('{FOO,bar,%oo%}');
				How to use ANY instead of IN in a WHERE clause with Rails? id=g10423
					How to use ANY instead of IN in a WHERE clause with Rails?  <url:file:///~/projects/study/otl/cdb.otl#r=g10423>
					https://stackoverflow.com/questions/31191507/how-to-use-any-instead-of-in-in-a-where-clause-with-rails/31192557#31192557
					ans 
						two variants of IN:
							expr IN (subquery)
							expr IN (value [, ...])
						two variants of ANY
							expr operator ANY (subquery)
							expr operator ANY (array expression)
						array expression
							an array constructor (array is constructed from a list of values on the Postgres side) of the form: ARRAY[1,2,3]
							or an array literal of the form '{1,2,3}'.
							To avoid invalid type casts, you can cast explicitly:
								ARRAY[1,2,3]::numeric[]
								'{1,2,3}'::bigint[]
						which one to use?
							ANY is more versatile
								IN is a special case of ANY
								because it can be combined with several operators, not just "="
									SELECT 'foo' LIKE ANY('{FOO,bar,%oo%}');
				Postgres - array for loop id=g10371
					Postgres - array for loop <url:file:///~/projects/study/otl/cdb.otl#r=g10371>
					https://stackoverflow.com/questions/9783422/postgres-array-for-loop/9784986#9784986
					ans
						Since PostgreSQL 9.1 there is the convenient FOREACH:
							DO
							$do$
							DECLARE
								 m   varchar[];
								 arr varchar[] := array[['key1','val1'],['key2','val2']];
							BEGIN
								 FOREACH m SLICE 1 IN ARRAY arr
								 LOOP
										RAISE NOTICE 'another_func(%,%)',m[1], m[2];
								 END LOOP;
							END
							$do$
						Solution for older versions:
							DO
							$do$
							DECLARE
								 arr varchar[] := '{{key1,val1},{key2,val2}}';
							BEGIN
								 FOR i IN array_lower(arr, 1) .. array_upper(arr, 1)
								 LOOP
										RAISE NOTICE 'another_func(%,%)',arr[i][1], arr[i][2];
								 END LOOP;
							END
							$do$
				Selecting data into a Postgres array
					https://stackoverflow.com/questions/11762398/selecting-data-into-a-postgres-array/11763245#11763245
					ans
			PostgreSQL 10 identity columns explained id=g10348
				PostgreSQL 10 identity columns explained <url:file:///~/projects/study/otl/cdb.otl#r=g10348>
				https://blog.2ndquadrant.com/postgresql-10-identity-columns/
				PK columns should be IDENTITY to conform SQL standard
				before:
					CREATE TABLE test_old (
							id serial PRIMARY KEY,
							payload text
					);
					INSERT INTO test_old (payload) VALUES ('a'), ('b'), ('c') RETURNING *;
				now: 
					CREATE TABLE test_new (
							id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
							payload text
					);
					INSERT INTO test_new (payload) VALUES ('a'), ('b'), ('c') RETURNING *;
				managing sequences:
					before
						ALTER SEQUENCE test_old_id_seq RESTART WITH 1000;
					now: With an identity column, you don’t need to know the name of the sequence:
						ALTER TABLE test_new ALTER COLUMN id RESTART WITH 1000;
			Polymorphic Types
				https://www.postgresql.org/docs/current/static/extend-type-system.html#extend-types-polymorphic
				Five pseudo-types of special interest are anyelement, anyarray, anynonarray, anyenum, and anyrange, which are collectively called polymorphic types. 
				Any function declared using these types is said to be a polymorphic function
			Any downsides of using data type “text” for storing strings? id=g10355
				Any downsides of using data type “text” for storing strings?  <url:file:///~/projects/study/otl/cdb.otl#r=g10355>
				https://stackoverflow.com/questions/20326892/any-downsides-of-using-data-type-text-for-storing-strings/20334221#20334221
				q
					3 data-types for character data:
						character varying(n), varchar(n)  variable-length with limit
						character(n), char(n)             fixed-length, blank padded
						text                              variable unlimited length
				ans
					text is optimum
					to enforce maximum length, use CHECK constraint
						ALTER TABLE tbl ADD CONSTRAINT tbl_col_len CHECK (length(col) < 100);
					CHECK can do more, pretty much anything
			Which data type for latitude and longitude
				ans
					point in geography
			PostgreSQL: Which Datatype should be used for Currency id=g10697
				PostgreSQL: Which Datatype should be used for Currency <url:file:///~/projects/study/otl/cdb.otl#r=g10697>
				ans
					personally: store currency as "integer" representing Cents
			Cannot cast type numeric to boolean id=g10698
				Cannot cast type numeric to boolean <url:file:///~/projects/study/otl/cdb.otl#r=g10698>
				https://stackoverflow.com/questions/19290248/cannot-cast-type-numeric-to-boolean/19290671#19290671
				ans
					ALTER TABLE products
						ALTER power_price DROP DEFAULT
					 ,ALTER power_price TYPE bool USING (power_price::int::bool)
					 ,ALTER power_price SET NOT NULL
					 ,ALTER power_price SET DEFAULT false;
		distinct
			How do I (or can I) SELECT DISTINCT on multiple columns? id=g10699
				How do I (or can I) SELECT DISTINCT on multiple columns?  <url:file:///~/projects/study/otl/cdb.otl#r=g10699>
				https://stackoverflow.com/questions/54418/how-do-i-or-can-i-select-distinct-on-multiple-columns/12632129#12632129
				ans
					opt1:
						UPDATE sales
						SET    status = 'ACTIVE'
						WHERE  (saleprice, saledate) IN (
								SELECT saleprice, saledate
								FROM   sales
								GROUP  BY saleprice, saledate
								HAVING count(*) = 1 
								);
					opt2: much faster: NOT EXISTS
						UPDATE sales s
						SET    status = 'ACTIVE'
						WHERE  NOT EXISTS (
							 SELECT 1
							 FROM   sales s1
							 WHERE  s.saleprice = s1.saleprice
							 AND    s.saledate  = s1.saledate
							 AND    s.id <> s1.id                     -- except for row itself
							 );
						AND    s.status IS DISTINCT FROM 'ACTIVE';  -- avoid empty updates. see below
			PostgreSQL DISTINCT ON with different ORDER BY id=g10700
				PostgreSQL DISTINCT ON with different ORDER BY <url:file:///~/projects/study/otl/cdb.otl#r=g10700>
				https://stackoverflow.com/questions/9795660/postgresql-distinct-on-with-different-order-by/9796104#9796104
				q
					SELECT DISTINCT ON (address_id) purchases.address_id, purchases.*
						FROM purchases
						WHERE purchases.product_id = 1
						ORDER BY purchases.purchased_at DESC
					But I get this error:
						PG::Error: ERROR: SELECT DISTINCT ON expressions must match initial ORDER BY expressions
					Adding address_id as first ORDER BY expression silences the error, but I really don't want to add sorting over address_id. Is it possible to do without ordering by address_id?
				ans
					opt: subquery
						SELECT *
						FROM  (
								SELECT DISTINCT ON (address_id) *
								FROM   purchases
								WHERE  product_id = 1
								) p
						ORDER  BY purchased_at DESC;
					points
						Leading expressions in ORDER BY have to agree with columns in DISTINCT ON, so you can't order by different columns in the same SELECT.
		group by
			Select first row in each GROUP BY group? id=adb_015
				Select first row in each GROUP BY group? <url:#r=adb_015>
				https://stackoverflow.com/questions/3800551/select-first-row-in-each-group-by-group/7630564#7630564
				q
					SELECT * FROM purchases;
					id | customer | total
					---+----------+------
					 1 | Joe      | 5
					 2 | Joe      | 3
					query: something like this
						SELECT FIRST(id), customer, FIRST(total)
						FROM  purchases
						GROUP BY customer
						ORDER BY total DESC;
						FIRST(id) | customer | FIRST(total)
						----------+----------+-------------
										1 | Joe      | 5
				ans1:
					opt1
						SELECT DISTINCT ON (customer)
									 id, customer, total
						FROM   purchases
						ORDER  BY customer, total DESC, id;
					opt2
						SELECT DISTINCT ON (2)
									 id, customer, total
						FROM   purchases
						ORDER  BY 2, 3 DESC, 1;
					opt3: if total can be NULL
						...
						ORDER  BY customer, total DESC NULLS LAST, id;
					major points
						DISTINCT ON 
						CREATE INDEX purchases_3c_idx ON purchases (customer, total DESC, id);
			Optimize GROUP BY query to retrieve latest record per user
				https://stackoverflow.com/questions/25536422/optimize-group-by-query-to-retrieve-latest-record-per-user/25536748#25536748
				ans
			GROUP BY + CASE statement id=g10390
				GROUP BY + CASE statement <url:file:///~/projects/study/otl/cdb.otl#r=g10390>
				https://stackoverflow.com/questions/19848930/group-by-case-statement/19849537#19849537
				q
					goal
								day     |      name      | type | case | count
						------------+----------------+------+------+-------
						 2013-11-06 | modelA         |    1 |    0 |   972
						 2013-11-06 | modelA         |    1 |    1 |    55
						 2013-11-06 | modelB         |    1 |    0 |   456
				ans
					code
						SELECT m.name
								 , a.type
								 , CASE WHEN a.result = 0 THEN 0 ELSE 1 END AS result
								 , CURRENT_DATE - 1 AS day
								 , count(*) AS ct
						FROM   attempt    a
						JOIN   prod_hw_id p USING (hard_id)
						JOIN   model      m USING (model_id)
						WHERE  ts >= '2013-11-06 00:00:00'  
						AND    ts <  '2013-11-07 00:00:00'
						GROUP  BY 1,2,3
						ORDER  BY 1,2,3;
			Optimize GROUP BY query to retrieve latest record per user id=adb_007
				Optimize GROUP BY query to retrieve latest record per user <url:#r=adb_007>
				https://stackoverflow.com/questions/25536422/optimize-group-by-query-to-retrieve-latest-record-per-user/25536748#25536748
				code
					CREATE TABLE user_msg_log (
							aggr_date DATE,
							user_id INTEGER,
							running_total INTEGER
					);
					SELECT user_id, max(aggr_date), max(running_total) 
					FROM user_msg_log 
					WHERE aggr_date <= :mydate 
					GROUP BY user_id
				index: user_msg_log(aggr_date)
				query very slow. what index to define?
				multicolumn index
					CREATE INDEX user_msg_log_combo_idx
						ON user_msg_log (user_id, aggr_date DESC NULLS LAST)
				recursive cte
					WITH RECURSIVE cte AS (
						 (
						 SELECT u  -- whole row
						 FROM   user_msg_log u
						 WHERE  aggr_date <= :mydate
						 ORDER  BY user_id, aggr_date DESC NULLS LAST
						 LIMIT  1
						 )
						 UNION ALL
						 SELECT (SELECT u1  -- again, whole row
										 FROM   user_msg_log u1
										 WHERE  user_id > (c.u).user_id  -- parentheses to access row type
										 AND    aggr_date <= :mydate     -- repeat predicate
										 ORDER  BY user_id, aggr_date DESC NULLS LAST
										 LIMIT  1)
						 FROM   cte c
						 WHERE  (c.u).user_id IS NOT NULL        -- any NOT NULL column of the row
						 )
					SELECT (u).*                               -- finally decompose row
					FROM   cte
					WHERE  (u).user_id IS NOT NULL             -- any column defined NOT NULL
					ORDER  BY (u).user_id;
			PostgreSQL - GROUP BY clause or be used in an aggregate function id=g10397
				PostgreSQL - GROUP BY clause or be used in an aggregate function <url:file:///~/projects/study/otl/cdb.otl#r=g10397>
				https://stackoverflow.com/questions/10161696/postgresql-group-by-clause-or-be-used-in-an-aggregate-function/10167279#10167279
				q
					cars
						|id|name|created_at|update_at|
					users
						|user_name|car_id|used_at|
					-->
						|car_id|name|..|count_used|
				ans
					code 
						SELECT id, name, created_at, updated_at, u.ct
						FROM   cars c
						LEFT   JOIN (
								SELECT car_id, count(*) AS ct
								FROM   users
								GROUP  BY 1
								) u ON u.car_id  = c.id
						ORDER  BY u.ct DESC;
					points
						GROUP before JOIN
							=> fewer join operations
			GROUP BY and COUNT in PostgreSQL id=g10399
				GROUP BY and COUNT in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10399>
				https://stackoverflow.com/questions/11807386/group-by-and-count-in-postgresql/11809477#11809477
				q
					posts
						|post_id|
					votes
						|vote_id|post_id|
					-->
						|votes_for_post|post_id|
				ans
					ex1: EXISTS
						SELECT count(*) AS post_ct
						FROM   posts p
						WHERE  EXISTS (SELECT 1 FROM votes v WHERE v.post_id = p.id);
					ex2: count(DISTINCT p.id)
						SELECT count(DISTINCT p.id) AS post_ct
						FROM   posts p
						JOIN   votes v ON v.post_id = p.id;
					ex3
						SELECT count(*) AS post_ct
						FROM  (
								SELECT 1
								FROM   posts 
								JOIN   votes ON votes.post_id = posts.id 
								GROUP  BY posts.id
								) x;
			Postgres window function and group by exception id=g10421
				Postgres window function and group by exception <url:file:///~/projects/study/otl/cdb.otl#r=g10421>
				https://stackoverflow.com/questions/8844903/postgres-window-function-and-group-by-exception/8845062#8845062
				ans
					code
						SELECT p.name
								 , e.event_id
								 , e.date
								 , sum(sum(sp.payout)) OVER w
								 - sum(sum(s.buyin  )) OVER w AS "Profit/Loss" 
						FROM   player            p
						JOIN   result            r ON r.player_id     = p.player_id  
						JOIN   game              g ON g.game_id       = r.game_id 
						JOIN   event             e ON e.event_id      = g.event_id 
						JOIN   structure         s ON s.structure_id  = g.structure_id 
						JOIN   structure_payout sp ON sp.structure_id = g.structure_id
																			AND sp.position     = r.position
						WHERE  p.player_id = 17 
						GROUP  BY e.event_id
						WINDOW w AS (ORDER BY e.date, e.event_id)
						ORDER  BY e.date, e.event_id;
					points: 
						sum(sum(sp.payout)) OVER w, the outer sum() is a window function, the inner sum() is an aggregate function.
			How to use a SQL window function to calculate a percentage of an aggregate id=g10428
				How to use a SQL window function to calculate a percentage of an aggregate <url:file:///~/projects/study/otl/cdb.otl#r=g10428>
				https://stackoverflow.com/questions/8515152/how-to-use-a-sql-window-function-to-calculate-a-percentage-of-an-aggregate/8524258#8524258
				ans
					ex
						SELECT d1, d2, sum(v)/sum(sum(v)) OVER (PARTITION BY d1) AS share
						FROM   test
						GROUP  BY d1, d2;
					ex: CTE
						WITH x AS (
								SELECT d1, d2, sum(v) AS sv
								FROM   test
								GROUP  BY d1, d2
								)
						SELECT d1, d2, sv/sum(sv) OVER (PARTITION BY d1) AS share
						FROM   x;
					ex: subquery
						SELECT d1, d2, sv/sum(sv) OVER (PARTITION BY d1) AS share
						FROM   (
								SELECT d1, d2, sum(v) AS sv
								FROM   test
								GROUP  BY d1, d2
								) x;
			PostgreSQL aggregate or window function to return just the last value id=g10430
				PostgreSQL aggregate or window function to return just the last value <url:file:///~/projects/study/otl/cdb.otl#r=g10430>
				https://stackoverflow.com/questions/8320569/postgresql-aggregate-or-window-function-to-return-just-the-last-value/8320588#8320588
				ans
					ref
						PostgreSQL: running count of rows for a query 'by minute' <url:#r=adb_014>
						Select first row in each GROUP BY group? <url:#r=adb_015>
					opt1: DISTINCT + window function
						SELECT DISTINCT a
								 , last_value(b) OVER (PARTITION BY a ORDER BY b
																			 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)
						FROM  (
							 VALUES
								 (1, 'do not want this')
								,(1, 'just want this')
							 ) sub(a, b);
					opt2: DISTINCT ON (simpler)
						SELECT DISTINCT ON (a)
									 a, b
						FROM  (
							 VALUES
								 (1, 'do not want this')
							 , (1, 'just want this')
							 ) sub(a, b)
						ORDER  BY a, b DESC;
					opt3: simple with plain aggregate
						SELECT a, max(b)
						FROM  (
							 VALUES
								 (1, 'do not want this')
							 , (1, 'just want this')
							 ) sub(a, b)
						GROUP  BY a;
			Calculating Cumulative Sum in PostgreSQL id=g10381
				Calculating Cumulative Sum in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10381>
				https://stackoverflow.com/questions/22841206/calculating-cumulative-sum-in-postgresql/22843199#22843199
				q
					ea_month    id       amount    ea_year    circle_id    cum_amt
					February    92576    1000      2014        1           1000 
					March       92573    3000      2014        1           4000
				ans
					code
						SELECT ea_month, id, amount, ea_year, circle_id
								 , sum(amount) OVER (PARTITION BY circle_id ORDER BY month) AS cum_amt
						FROM   tbl
						ORDER  BY circle_id, month;
			Generate id row for a view with grouping id=g10432
				Generate id row for a view with grouping <url:file:///~/projects/study/otl/cdb.otl#r=g10432>
				https://stackoverflow.com/questions/8637857/generate-id-row-for-a-view-with-grouping/8638486#8638486
				ans
					CREATE OR REPLACE VIEW daily_transactions as
					SELECT row_number() OVER () AS id
							 , t.ic
							 , t.bio_id
							 , t.wp 
							 , t.transaction_time::date AS transaction_date
							 , min(t.transaction_time)::time AS time_in
							 , w.start_time AS wp_start
							 , w.start_time - min(t.transaction_time)::time AS in_diff
							 , max(t.transaction_time)::time AS time_out
							 , w.end_time AS wp_end
							 , max(t.transaction_time)::time - w.end_time AS out_diff
							 , count(*) AS total_transactions
							 , calc_att_status(t.transaction_time::date, min(t.transaction_time)::time
															 , max(t.transaction_time)::time
															 , w.start_time, w.end_time) AS status
					FROM   transactions t
					LEFT   JOIN wp w ON t.wp = w.wp_name
					GROUP  BY t.ic, t.bio_id, t.wp, t.transaction_time::date
									, w.start_time, w.end_time;
					points
						before: generate_series() is applied after aggregate functions, but produces multiple rows, thereby multiplying all output rows
						date_trunc() is redundant in date_trunc('day', t.transaction_time)::date. t.transaction_time::date achieves the same, simper & faster
			Aggregate strings in descending order in a PostgreSQL query id=g10433
				Aggregate strings in descending order in a PostgreSQL query <url:file:///~/projects/study/otl/cdb.otl#r=g10433>
				https://stackoverflow.com/questions/10470585/aggregate-strings-in-descending-order-in-a-postgresql-query/10470738#10470738
				ans
					SELECT company_id, array_agg(employee ORDER BY company_id DESC)::text
					FROM   tbl
					GROUP  BY 1;
					points
						you can order elements inside aggregate functions
						casting the array to text (array_agg(employee)::text), which gives you a basic, comma-separated string without additional white space
							array_to_string(array_agg(employee), ', ') 
					opt: CTE or subselect
						SELECT company_id, array_agg(employee)::text
						FROM  (SELECT * FROM tbl ORDER BY company_id, employee  DESC) x
						GROUP  BY 1;
		index
			Create unique constraint with null columns id=g10701
				Create unique constraint with null columns <url:file:///~/projects/study/otl/cdb.otl#r=g10701>
				https://stackoverflow.com/questions/8289100/create-unique-constraint-with-null-columns/8289253#8289253
				q
					goal:
						ALTER TABLE Favorites
						ADD CONSTRAINT Favorites_UniqueFavorite UNIQUE(UserId, MenuId, RecipeId);
						However, this will allow multiple rows with the same (UserId, RecipeId), if MenuId IS NULL
					ans
						Create two partial indexes:
						CREATE UNIQUE INDEX favo_3col_uni_idx ON favorites (user_id, menu_id, recipe_id)
						WHERE menu_id IS NOT NULL;
						CREATE UNIQUE INDEX favo_2col_uni_idx ON favorites (user_id, recipe_id)
						WHERE menu_id IS NULL;
			Index for finding an element in a JSON array id=g10349
				Index for finding an element in a JSON array <url:file:///~/projects/study/otl/cdb.otl#r=g10349>
				https://stackoverflow.com/questions/18404055/index-for-finding-an-element-in-a-json-array/18405706#18405706
				q
					CREATE TABLE tracks (id SERIAL, artists JSON);
					INSERT INTO tracks (id, artists) 
						VALUES (1, '[{"name": "blink-182"}]');
					INSERT INTO tracks (id, artists) 
						VALUES (2, '[{"name": "The Dirty Heads"}, {"name": "Louis Richards"}]');
					SELECT * FROM tracks
						WHERE 'The Dirty Heads' IN 
							(SELECT value->>'name' FROM json_array_elements(artists))
					this does a full table scan
				ans
					GIN index:
						CREATE TABLE tracks (id serial, artists jsonb);
						CREATE INDEX tracks_artists_gin_idx ON tracks USING gin (artists);
					No need for a function to convert the array. This would support a query:
						SELECT * FROM tracks WHERE artists @> '[{"name": "The Dirty Heads"}]';
					@> being the new jsonb "contains" operator, which can use the GIN index.
			Can PostgreSQL index array columns?
				https://stackoverflow.com/questions/4058731/can-postgresql-index-array-columns/29245753#29245753
			Unused index in range of dates query id=g10464
				Unused index in range of dates query <url:file:///~/projects/study/otl/cdb.otl#r=g10464>
				https://dba.stackexchange.com/questions/90128/unused-index-in-range-of-dates-query/90183#90183
				code
					mustang=# \d+ bss.amplifier_saturation
																												 Table "bss.amplifier_saturation"
					 Column |           Type           |                             Modifiers                             | Storage | Description 
					--------+--------------------------+-------------------------------------------------------------------+---------+-------------
					 value  | integer                  | not null                                                          | plain   | 
					 target | integer                  | not null                                                          | plain   | 
				code
					The query/plan:
					mustang=# explain select max(lddate) from bss.amplifier_saturation
					where start >= '1987-12-31 00:00:00'
					and   start <= '1988-04-09 00:00:00';
				best index:
					(lddate, start)
				why lddate IS NOT NULL
					Index Cond: ((lddate IS NOT NULL) AND ...
					Why does Postgres have to exclude NULL values?
					Because NULL sorts after the greatest value in ASCENDING or before in DESCENDING order. The maximum non-null value which is returned by the aggregate function max() is not at the beginning / end of the index if there are NULL values.  
					DESC NULLS LAST is the beter choice.
		joins
			What is semi-join id=g10687
				What is semi-join <url:file:///~/projects/study/otl/cdb.otl#r=g10687>
				http://www.answers.com/Q/What_is_semi_join_in_SQL
				ex
					SELECT * 
					FROM Customers C 
					WHERE EXISTS ( 
						SELECT * 
						FROM Sales S 
						WHERE S.Cust_Id = C.Cust_Id 
						) 
			Find records where join doesn't exist id=g10688
				Find records where join doesn't exist <url:file:///~/projects/study/otl/cdb.otl#r=g10688>
				https://stackoverflow.com/questions/14251180/find-records-where-join-doesnt-exist/14260510#14260510
				NULL trap of NOT IN
				ans
					WHERE NOT EXISTS (
						 SELECT 1
						 FROM   votes v
						 WHERE  v.some_id = base_table.some_id
						 AND    v.user_id = ?
						 )
					NOT EXISTS () vs NOT IN ()
						result of NULL in NOT IN is null
			What is the difference between LATERAL and a subquery in PostgreSQL id=g10370
				What is the difference between LATERAL and a subquery in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10370>
				https://stackoverflow.com/questions/28550679/what-is-the-difference-between-lateral-and-a-subquery-in-postgresql/28557803#28557803
				ans
					A LATERAL join (Postgres 9.3+) is more like a correlated subquery, not a plain subquery. Like @Andomar pointed out, a function or subquery to the right of a LATERAL join typically has to be evaluated many times - once for each row left of the LATERAL join
					examples:
						Optimize GROUP BY query to retrieve latest record per user <url:#r=adb_007>
					Things a subquery can't do
						correlated subquery can
							return one value (no multiple columns and rows)
						So this works, but cannot easily be replaced with a subquery:
							CREATE TABLE tbl (a1 int[], a2 int[]);
							SELECT *
							FROM   tbl t
									 , unnest(t.a1, t.a2) u(elem1, elem2);  -- implicit LATERAL
						The comma (,) in the FROM clause is short notation for CROSS JOIN.
							LATERAL is assumed automatically for table functions like unnest().)
							ref
								How do you declare a set-returning-function to only be allowed in the FROM clause? <url:#r=adb_008>
					Set-returning functions in the SELECT list
						SELECT *, unnest(t.a1) AS elem1, unnest(t.a2) AS elem2
						FROM   tbl t;
							http://dbfiddle.uk/?rdbms=postgres_10&fiddle=b78160f4899d95444e2a605a9609cf8f
			Join table twice - on two different columns of the same table id=g10406
				Join table twice - on two different columns of the same table <url:file:///~/projects/study/otl/cdb.otl#r=g10406>
				https://stackoverflow.com/questions/10710271/join-table-twice-on-two-different-columns-of-the-same-table/10710362#10710362
				ans
					code
						SELECT t2.table1_id
							, t2.id          AS table2_id
							, t2.table3_id_1
							, t2.table3_id_2
							, t31.value      AS x
							, t32.value      AS y
							FROM   table2 t2
							LEFT   JOIN table3 t31 ON t31.id = t2.table3_id_1
							LEFT   JOIN table3 t32 ON t32.id = t2.table3_id_2;
						There is no need to join in table1. table2 has all you need 
			Multiple left joins on multiple tables in one query id=g10410
				Multiple left joins on multiple tables in one query <url:file:///~/projects/study/otl/cdb.otl#r=g10410>
				https://stackoverflow.com/questions/14260860/multiple-left-joins-on-multiple-tables-in-one-query/14261094#14261094
				q
					"master" table: multiple levels (parents and childs)
					SELECT something FROM master as parent, master as child
						LEFT JOIN second as parentdata ON parent.secondary_id = parentdata.id
						LEFT JOIN second as childdata ON child.secondary_id = childdata.id
					WHERE parent.id = child.parent_id AND parent.parent_id = 'rootID'
					issues
						old style: two table names in FROM with new style (LEFT JOIN .. ON)
							causes bugs
				ans
					code
						SELECT something
						FROM   master      parent
						JOIN   master      child ON child.parent_id = parent.id
						LEFT   JOIN second parentdata ON parentdata.id = parent.secondary_id
						LEFT   JOIN second childdata ON childdata.id = child.secondary_id
						WHERE  parent.parent_id = 'rootID'
			Join tables on columns of composite foreign / primary key in a query id=g10689
				Join tables on columns of composite foreign / primary key in a query <url:file:///~/projects/study/otl/cdb.otl#r=g10689>
				https://stackoverflow.com/questions/21791478/join-tables-on-columns-of-composite-foreign-primary-key-in-a-query/21792127#21792127
				q
					shorter way to this:
						SELECT *
						FROM subscription
						JOIN delivery ON (delivery.magazine_id = subscription.magazine_id
													AND delivery.user_id = subscription.user_id)
				ans
					SELECT *
					FROM subscription NATURAL JOIN delivery
					manual:
						NATURAL is shorthand for a USING list that mentions all columns in the two tables that have the same names.
			How to join tables on regex id=g10435
				How to join tables on regex <url:file:///~/projects/study/otl/cdb.otl#r=g10435>
				https://stackoverflow.com/questions/8679079/how-to-join-tables-on-regex/8680446#8680446
				ans
					ex1: LIKE
						SELECT msg.message
									,msg.src_addr
									,msg.dst_addr
									,mnc.name
						FROM   mnc
						JOIN   msg ON msg.src_addr ~~ ('%38' || mnc.code || '%')
											 OR msg.dst_addr ~~ ('%38' || mnc.code || '%')
						WHERE  length(mnc.code) = 3
					ex2: regex
						SELECT msg.message
									,msg.src_addr
									,msg.dst_addr
									,mnc.name
						FROM   mnc
						JOIN   msg ON (msg.src_addr || '+' || msg.dst_addr) ~ (38 || mnc.code)
											 AND length(mnc.code) = 3
					ex3: regexp_matches()
						SELECT msg.message
									,msg.src_addr
									,msg.dst_addr
									,mnc.name
						FROM   mnc
						JOIN   msg ON EXISTS (
								SELECT * 
								FROM   regexp_matches(msg.src_addr ||'+'|| msg.dst_addr, '38(...)', 'g') x(y)
								WHERE  y[1] = mnc.code
								)
		metaprogramming
			What does regclass signify in Postgresql id=g10690
				What does regclass signify in Postgresql <url:file:///~/projects/study/otl/cdb.otl#r=g10690>
				https://stackoverflow.com/questions/13289107/what-does-regclass-signify-in-postgresql#13290020
				::regclass is a cast
				regclass is a "magic" data type
					an alias for `oid` (object identifier)
				you cannot compare `text` directly with `oid`
					select * from pg_class where oid = 'table1'::regclass; -- ok
					select * from pg_class where oid = 'table1'; -- wrong
			How to check if a table exists in a given schema id=g10691
				How to check if a table exists in a given schema <url:file:///~/projects/study/otl/cdb.otl#r=g10691>
				https://stackoverflow.com/questions/20582500/how-to-check-if-a-table-exists-in-a-given-schema/24089729#24089729
				How to check whether a table (or view) exists, and the current user has access to it?
					opt1: information_schema
						SELECT EXISTS (
							 SELECT 1
							 FROM   information_schema.tables 
							 WHERE  table_schema = 'schema_name'
							 AND    table_name = 'table_name'
							 );
					opt2: pg_tables
						SELECT EXISTS (
							 SELECT 1 
							 FROM   pg_tables
							 WHERE  schemaname = 'schema_name'
							 AND    tablename = 'table_name'
							 );
			How to get the trigger(s) associated with a view or a table in PostgreSQL id=g10424
				How to get the trigger(s) associated with a view or a table in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10424>
				https://stackoverflow.com/questions/25202133/how-to-get-the-triggers-associated-with-a-view-or-a-table-in-postgresql/25204845#25204845
				ans
					SELECT tgname
					FROM   pg_trigger
					WHERE  tgrelid = 'myschema.mytbl'::regclass; -- optionally schema-qualified
			How can I test if a column exists in a table using an SQL statement id=g10434
				How can I test if a column exists in a table using an SQL statement <url:file:///~/projects/study/otl/cdb.otl#r=g10434>
				https://stackoverflow.com/questions/9991043/how-can-i-test-if-a-column-exists-in-a-table-using-an-sql-statement/10002376#10002376
				ans
					code
						SELECT TRUE
						FROM   pg_attribute 
						WHERE  attrelid = 'myTable'::regclass  -- cast to a registered class (table)
						AND    attname = 'myColumn'
						AND    NOT attisdropped  -- exclude dropped (dead) columns
						-- AND attnum > 0        -- exclude system columns (you may or may not want this)
			Exploring a Data Set in SQL id=g10438
				Exploring a Data Set in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10438>
				http://tapoueh.org/blog/2017/06/exploring-a-data-set-in-sql/
				Artists, Albums, Tracks, Genre
					ex
						chinook# \d track
										Table "public.track"
								Column    │  Type   │ Modifiers 
						══════════════╪═════════╪═══════════
						 trackid      │ bigint  │ not null
						 name         │ text    │ 
						 albumid      │ bigint  │ 
						 mediatypeid  │ bigint  │ 
						 genreid      │ bigint  │ 
						 composer     │ text    │ 
						 milliseconds │ bigint  │ 
						 bytes        │ bigint  │ 
						 unitprice    │ numeric │ 
						Indexes:
								"idx_189229_ipk_track" PRIMARY KEY, btree (trackid)
								"idx_189229_ifk_trackalbumid" btree (albumid)
								"idx_189229_ifk_trackgenreid" btree (genreid)
								"idx_189229_ifk_trackmediatypeid" btree (mediatypeid)
						Foreign-key constraints:
								"track_albumid_fkey" FOREIGN KEY (albumid) REFERENCES album(albumid)
								"track_genreid_fkey" FOREIGN KEY (genreid) REFERENCES genre(genreid)
								"track_mediatypeid_fkey" FOREIGN KEY (mediatypeid) REFERENCES mediatype(mediatypeid)
						Referenced by:
								TABLE "invoiceline" CONSTRAINT "invoiceline_trackid_fkey" FOREIGN KEY (trackid) REFERENCES track(trackid)
								TABLE "playlisttrack" CONSTRAINT "playlisttrack_trackid_fkey" FOREIGN KEY (trackid) REFERENCES track(trackid)
					ex: use \set to query
						> \set albumid 193
						> select artist.name as artist,
										 album.title as album,
										 track.name as track
								from track
										 join album using(albumid)
										 join artist using(artistid)
							 where albumid = :albumid;
					ex: visual query
						select genre.name, count(*),
									 repeat('■', (  100.0
																* count(*)
																/ sum(count(*)) over()
															 )::integer
												 ) as pct
							from genre
									 left join track using(genreid)
							group by genre.name
							order by genre.name;
					out
						│ count │                  pct                  
						════════════════════╪═══════╪═══════════════════════════════════════
						Alternative        │    40 │ ■
						Alternative & Punk │   332 │ ■■■■■■■■■
						Blues              │    81 │ ■■
					ex: multi-genres albums
						code
							select title as album,
										 array_agg(distinct genre.name order by genre.name) as genres
								from      track
										 join genre using(genreid)
										 join album using (albumid)
							group by title
								having count(distinct genre.name) > 1;
						out
							album              │                      genres                       
							════════════════════════════════╪═══════════════════════════════════════════════════
							Battlestar Galactica, Season 3 │ {"Sci Fi & Fantasy","Science Fiction","TV Shows"}
							Greatest Hits                  │ {Metal,Reggae,Rock}
		pl
			PL/pgSQL checking if a row exists
				https://stackoverflow.com/questions/11892233/pl-pgsql-checking-if-a-row-exists/11892796#11892796
					ans
						IF EXISTS (SELECT 1 FROM people WHERE person_id = my_person_id) THEN
							-- do something
						END IF;
			Postgres FOR LOOP id=g10369
				Postgres FOR LOOP <url:file:///~/projects/study/otl/cdb.otl#r=g10369>
				https://stackoverflow.com/questions/19145761/postgres-for-loop/19147320#19147320
				ans
					DO
					$do$
					BEGIN 
					FOR i IN 1..25 LOOP
						 INSERT INTO playtime.meta_random_sample (col_i, col_id) -- use col names
						 SELECT i, id
						 FROM   tbl
						 ORDER  BY random()
						 LIMIT  15000;
					END LOOP;
					END
					$do$;
			What are '$$' used for in PL/pgSQL id=g10377
				What are '$$' used for in PL/pgSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10377>
				https://stackoverflow.com/questions/12144284/what-are-used-for-in-pl-pgsql/12172353#12172353
				ans
					dollar signs: used for dollar quoting
					opt: use single quotes for function body
						CREATE OR REPLACE FUNCTION check_phone_number(text)
						RETURNS boolean AS
						'
						BEGIN
							IF NOT $1 ~  e''^\\+\\d{3}\\ \\d{3} \\d{3} \\d{3}$'' THEN
								RAISE EXCEPTION ''Malformed string "%". Expected format is +999 999'';
							END IF;
							RETURN true; 
						END
						' LANGUAGE plpgsql STRICT IMMUTABLE;
					ex: better use $$ or $token$
						CREATE OR REPLACE FUNCTION check_phone_number(text)
							RETURNS boolean  
						AS
						$func$
						BEGIN
						 ...
						END
						$func$  LANGUAGE plpgsql STRICT IMMUTABLE;
			Store the query result in variable using postgresql Stored procedure id=g10391
				Store the query result in variable using postgresql Stored procedure <url:file:///~/projects/study/otl/cdb.otl#r=g10391>
				https://stackoverflow.com/questions/12328198/store-the-query-result-in-variable-using-postgresql-stored-procedure/12328820#12328820
				q
					begin
					 name ='SELECT name FROM test_table where id='||x;
					 if(name='test')then
						--do somthing
					 else
						--do the eles part
					 end if;
					end;
				ans
					opt1
						name := (SELECT t.name from test_table t where t.id = x);
					opt2: select into
						select test_table.name into name from test_table where id = x;
					opt3
						name := t.name from test_table t where t.id = x;
					opt4: IF EXISTS (best)
						BEGIN
							IF EXISTS(SELECT name
													FROM test_table t
												 WHERE t.id = x
													 AND t.name = 'test')
							THEN
								 ---
							ELSE
								 ---
							END IF;
			Refactor a PL/pgSQL function to return the output of various SELECT queries id=adb_013 id=g10385
				Refactor a PL/pgSQL function to return the output of various SELECT queries  id=g10385 <url:file:///~/projects/study/otl/cdb.otl#r=g10385>
				Refactor a PL/pgSQL function to return the output of various SELECT queries <url:#r=adb_013>
				https://stackoverflow.com/questions/11740256/refactor-a-pl-pgsql-function-to-return-the-output-of-various-select-queries/11751557#11751557
				q
					dynamic SELECT
					how to execute it return result
				ans:
					execute dynamic SQL
						in plpgsql: EXECUTE
					problem: return records of yet undefined type
						number, names, types can vary
					opt1: less structured document data type: json, jsonb, hstore, xml
					ex1:
						CREATE OR REPLACE FUNCTION data_of(_id integer)
							RETURNS TABLE (datahora timestamp, col2 text, col3 text) AS
						$func$
						DECLARE
							 _sensors text := 'col1::text, col2::text';  -- cast each col to text
							 _type    text := 'foo';
						BEGIN
							 RETURN QUERY EXECUTE '
									SELECT datahora, ' || _sensors || '
									FROM   ' || quote_ident(_type) || '
									WHERE  id = $1
									ORDER  BY datahora'
							 USING  _id;
						END
						$func$ LANGUAGE plpgsql;
					ex2: format()
						RETURN QUERY EXECUTE format('
							 SELECT datahora, %s  -- identifier passed as unescaped string
							 FROM   %I            -- assuming the name is provided by user
							 WHERE  id = $1
							 ORDER  BY datahora'
							,_sensors, _type)
						USING  _id;
					ex3: to return all columns of a table. uses polymorphic type
						function
							CREATE OR REPLACE FUNCTION data_of(_tbl_type anyelement, _id int)
								RETURNS SETOF anyelement AS
							$func$
							BEGIN
								 RETURN QUERY EXECUTE format('
										SELECT *
										FROM   %s  -- pg_typeof returns regtype, quoted automatically
										WHERE  id = $1
										ORDER  BY datahora'
									, pg_typeof(_tbl_type))
								 USING  _id;
							END
							$func$ LANGUAGE plpgsql;
						Call:
							SELECT * FROM data_of(NULL::pcdmet, 17);
						points
							anyelement is a pseudo data type, a polymorphic type, a placeholder for any non-array data type
							pg_typeof(_tbl_type) returns the name of the table as object identifier type regtype. When automatically converted to text, identifiers are automatically double-quoted and schema-qualified if needed. 
			Shell script to execute pgsql commands in files id=g10392
				Shell script to execute pgsql commands in files <url:file:///~/projects/study/otl/cdb.otl#r=g10392>
				https://stackoverflow.com/questions/8594717/shell-script-to-execute-pgsql-commands-in-files/8595568#8595568
				ans
					1. do not mix psql meta-commands and SQL commands
					Make your files contain only SQL commands.
					Do not include the CREATE DATABASE statement in the SQL files.
					create db
						psql postgres -c "CREATE DATABASE mytemplate1 WITH ENCODING 'UTF8' TEMPLATE template0"
					running pgsql commands
						psql mytemplate1 -f file
						batch
							for file in /path/to/files/*; do
									psql mytemplate1 -f "$file"
							done
						psql -c 'CREATE DATABASE myDB TEMPLATE mytemplate1'
						If you don't provide a database to connect to, psql will use the default maintenance database named postgres
			Difference between language sql and language plpgsql in PostgreSQL functions id=g10416
				Difference between language sql and language plpgsql in PostgreSQL functions <url:file:///~/projects/study/otl/cdb.otl#r=g10416>
				https://stackoverflow.com/questions/24755468/difference-between-language-sql-and-language-plpgsql-in-postgresql-functions/24771561#24771561
				ans
					sql better when:
						simple scalar queries
						single calls per session
						simple enough to be inlined 
							ref
								Inlining of SQL Functions <url:#r=adb_012>
					pgsql better when:
						you need procedural elements
						dynamic SQL: EXECUTE
						computations that can be reused and CTE can't be stretched
		queries
			Select rows which are not present in other table id=g10347
				Select rows which are not present in other table <url:file:///~/projects/study/otl/cdb.otl#r=g10347>
				https://stackoverflow.com/questions/19363481/select-rows-which-are-not-present-in-other-table/19364694#19364694
				q
					two postgresql tables:
						table name     column names
						-----------    ------------------------
						login_log      ip | etc.
						ip_location    ip | location | hostname | etc.
					query like
						SELECT login_log.ip 
						FROM login_log 
						WHERE NOT EXISTS (SELECT ip_location.ip
														 FROM ip_location
														 WHERE login_log.ip = ip_location.ip)
				ans
					opt1: NOT EXISTS (fastest) = anti-join
						SELECT ip 
						FROM   login_log l 
						WHERE  NOT EXISTS (
							 SELECT 1              -- it's mostly irrelevant what you put here
							 FROM   ip_location i
							 WHERE  l.ip = i.ip
							 );
					opt2: LEFT JOIN - shortest
						SELECT l.ip 
						FROM   login_log l 
						LEFT   JOIN ip_location i USING (ip)  -- short for: ON i.ip = l.ip
						WHERE  i.ip IS NULL;
					opt3: EXCEPT: not for complex queries
						SELECT ip 
						FROM   login_log
						EXCEPT ALL               -- ALL, to keep duplicate rows and make it faster
						SELECT ip
						FROM   ip_location;
					opt4: NOT IN: only if no NULL values exist
						SELECT ip 
						FROM   login_log
						WHERE  ip NOT IN (
							 SELECT DISTINCT ip  -- DISTINCT is optional
							 FROM   ip_location
							 );
			What is easier to read in EXISTS subqueries? [closed]
				https://stackoverflow.com/questions/7710153/what-is-easier-to-read-in-exists-subqueries
				SELECT foo FROM bar WHERE EXISTS (SELECT * FROM baz WHERE baz.id = bar.id);
					better because
						*: not relevant
						semi-join
				SELECT foo FROM bar WHERE EXISTS (SELECT 1 FROM baz WHERE baz.id = bar.id);
			Best way to select random rows PostgreSQL id=g10692
				Best way to select random rows PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10692>
				https://stackoverflow.com/questions/8674718/best-way-to-select-random-rows-postgresql/8675160#8675160
				q
					I want a random selection of rows in PostgreSQL, I tried this:
						select * from table where random() < 0.01;
					But some other recommend this:
						select * from table order by random() limit 1000;
				ans
					SELECT *
					FROM  (
							SELECT DISTINCT 1 + trunc(random() * 5100000)::integer AS id
							FROM   generate_series(1, 1100) g
							) r
					JOIN   big USING (id)
					LIMIT  1000;
			How to concatenate columns in a Postgres SELECT? id=g10353
				How to concatenate columns in a Postgres SELECT?  <url:file:///~/projects/study/otl/cdb.otl#r=g10353>
				https://stackoverflow.com/questions/19942824/how-to-concatenate-columns-in-a-postgres-select/19943343#19943343
				q
					I have two string columns a and b.
					So select a,b from foo returns values a and b. However, concatenation of a and b does not work. I tried :
						select a || b from foo
					and
						select  a||', '||b from foo
				ans
					SELECT a::text || ', ' || b::text AS ab FROM foo;
					if NULL exists:
						SELECT concat_ws(', ', a::text, b::text) AS ab FROM foo;
						Or just concat() if you don't need separators:
						SELECT concat(a::text, b::text) AS ab FROM foo;
			Combine two columns and add into one new column id=g10354
				Combine two columns and add into one new column <url:file:///~/projects/study/otl/cdb.otl#r=g10354>
				https://stackoverflow.com/questions/12310986/combine-two-columns-and-add-into-one-new-column/12320369#12320369
				q
					I want to use an SQL statement to combine two columns and create a new column from them.
					I'm thinking about using concat(...), but is there a better way?
				ans1
					concat: useful to deal with null values
					ddl
						CREATE TABLE tbl
							(zipcode text NOT NULL, city text NOT NULL, state text NOT NULL);
						INSERT INTO tbl VALUES ('10954', 'Nanuet', 'NY');
					before
						\pset border 2
						SELECT * FROM tbl;
						+---------+--------+-------+
						| zipcode |  city  | state |
						+---------+--------+-------+
						| 10954   | Nanuet | NY    |
						+---------+--------+-------+
					Now add a function with the desired "column name" which takes the record type of the table as its only parameter:
						CREATE FUNCTION combined(rec tbl)
							RETURNS text
							LANGUAGE SQL
						AS $$
							SELECT $1.zipcode || ' - ' || $1.city || ', ' || $1.state;
						$$;
					after
						This creates a function which can be used as if it were a column of the table, as long as the table name or alias is specified, like this:
						SELECT *, tbl.combined FROM tbl;
						+---------+--------+-------+--------------------+
						| zipcode |  city  | state |      combined      |
						+---------+--------+-------+--------------------+
						| 10954   | Nanuet | NY    | 10954 - Nanuet, NY |
						+---------+--------+-------+--------------------+
				ans2
					if NULL exists:
						use concat()
					difficult to read:
						SELECT COALESCE(col_a, '') || COALESCE(col_b, '');
					elegant:
						SELECT concat(col_a, col_b);
			How to filter SQL results in a has-many-through relation id=g10360
				How to filter SQL results in a has-many-through relation <url:file:///~/projects/study/otl/cdb.otl#r=g10360>
				https://stackoverflow.com/questions/7364969/how-to-filter-sql-results-in-a-has-many-through-relation/7774879#7774879
				q
					student {
							id
							name
					}
					club {
							id
							name
					}
					student_club {
							student_id
							club_id
					}
					goal
						SELECT student.*
						FROM   student
						INNER  JOIN student_club sc ON student.id = sc.student_id
						LEFT   JOIN club c ON c.id = sc.club_id
						WHERE  c.id = 30 AND c.id = 50
				ans
					ddl
						ALTER TABLE student ADD CONSTRAINT student_pkey PRIMARY KEY(stud_id );
						ALTER TABLE student_club ADD CONSTRAINT sc_pkey PRIMARY KEY(stud_id, club_id);
						ALTER TABLE club       ADD CONSTRAINT club_pkey PRIMARY KEY(club_id );
						CREATE INDEX sc_club_id_idx ON student_club (club_id);
					opt1: JOIN twice
						SELECT s.stud_id, s.name
						FROM   student s
						JOIN   student_club x ON s.stud_id = x.stud_id
						JOIN   student_club y ON s.stud_id = y.stud_id
						WHERE  x.club_id = 30
						AND    y.club_id = 50;
					opt2: IN
						SELECT s.stud_id,  s.name
						FROM   student s
						WHERE  s.stud_id IN (SELECT stud_id FROM student_club WHERE club_id = 30)
						AND    s.stud_id IN (SELECT stud_id FROM student_club WHERE club_id = 50);
					opt3: EXISTS
						SELECT s.stud_id,  s.name
						FROM   student s
						WHERE  EXISTS (SELECT * FROM student_club
													 WHERE  stud_id = s.stud_id AND club_id = 30)
						AND    EXISTS (SELECT * FROM student_club
													 WHERE  stud_id = s.stud_id AND club_id = 50);
					opt4: EXISTS
						SELECT s.*
						FROM   student s
						JOIN   student_club x USING (stud_id)
						WHERE  sc.club_id = 10                 -- member in 1st club ...
						AND    EXISTS (                        -- ... and membership in 2nd exists
							 SELECT *
							 FROM   student_club AS y
							 WHERE  y.stud_id = s.stud_id
							 AND    y.club_id = 14
							 )
			PostgreSQL sort by datetime asc, null first? id=g10362
				PostgreSQL sort by datetime asc, null first?  <url:file:///~/projects/study/otl/cdb.otl#r=g10362>
				https://stackoverflow.com/questions/9510509/postgresql-sort-by-datetime-asc-null-first/9511492#9511492
				ans
					NULLS FIRST | LAST keywords for the ORDER BY clause to cater for that need exactly:
					ORDER BY last_updated NULLS FIRST
			PostgreSQL unnest() with element number id=g10363
				PostgreSQL unnest() with element number <url:file:///~/projects/study/otl/cdb.otl#r=g10363>
				https://stackoverflow.com/questions/8760419/postgresql-unnest-with-element-number/8767450#8767450
				q
					myTable
						id | elements
						---+------------
						1  |ab,cd,efg,hi
						2  |jk,lm,no,pq
						3  |rstuv,wxyz
					select id, unnest(string_to_array(elements, ',')) AS elem
					from myTable
						id | elem
						---+-----
						1  | ab
						1  | cd
						1  | efg
						1  | hi
						2  | jk
						...
					How can I include element numbers? I.e.:
						id | elem | nr
						---+------+---
						1  | ab   | 1
						1  | cd   | 2
						1  | efg  | 3
				ans
					opt
						SELECT t.id, a.elem, a.nr
						FROM   tbl t, unnest(t.arr) WITH ORDINALITY a(elem, nr);
					opt
						SELECT t.id, a.elem, a.nr
						FROM   tbl AS t
						LEFT   JOIN LATERAL unnest(string_to_array(t.elements, ','))
																WITH ORDINALITY AS a(elem, nr) ON TRUE;
			Manual: WITH ORDINALITY id=g10364
				Manual: WITH ORDINALITY <url:file:///~/projects/study/otl/cdb.otl#r=g10364>
				https://www.postgresql.org/docs/current/static/functions-srf.html
				When a function in the FROM clause is suffixed by WITH ORDINALITY, a bigint column is appended to the output which starts from 1 and increments by 1 for each row of the function's output. This is most useful in the case of set returning functions such as unnest().
				SELECT * FROM pg_ls_dir('.') WITH ORDINALITY AS t(ls,n);
							 ls        | n
				-----------------+----
				 pg_serial       |  1
				 pg_twophase     |  2
			Concatenate multiple result rows of one column into one, group by another column [duplicate] id=g10365
				Concatenate multiple result rows of one column into one, group by another column [duplicate] <url:file:///~/projects/study/otl/cdb.otl#r=g10365>
				https://stackoverflow.com/questions/15847173/concatenate-multiple-result-rows-of-one-column-into-one-group-by-another-column/15850510#15850510
				q
					Movie   Actor   
						A       1
						A       2
						A       3
						B       4
					-->
					Movie   ActorList
					 A       1, 2, 3
				ans
					SELECT movie, string_agg(actor, ', ') AS actor_list
					FROM   tbl
					GROUP  BY 1;
					The 1 in GROUP BY 1 is a positional reference and a shortcut for GROUP BY movie in this case.
			How to select id with max date group by category in PostgreSQL id=g10368
				How to select id with max date group by category in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10368>
				https://stackoverflow.com/questions/16914098/how-to-select-id-with-max-date-group-by-category-in-postgresql/16920077#16920077
				ans
					SELECT DISTINCT ON (category)
								 id
					FROM   tbl
					ORDER  BY category, "date" DESC;
					points
						If the column can be NULL, you may want to add NULLS LAST
			PostgreSQL: running count of rows for a query 'by minute' id=adb_014
				PostgreSQL: running count of rows for a query 'by minute' <url:#r=adb_014>
				https://stackoverflow.com/questions/8193688/postgresql-running-count-of-rows-for-a-query-by-minute/8194088#8194088
				ans
					code
						SELECT DISTINCT
									 date_trunc('minute', "when") AS minute
								 , count(*) OVER (ORDER BY date_trunc('minute', "when")) AS running_ct
						FROM   mytable
						ORDER  BY 1;
					points
						count() is mostly used as plain aggregate function. 
							Appending an OVER clause makes it a window function. 
							Omit PARTITION BY in the window definition - you want a running count over all rows
			Split comma separated column data into additional columns id=g10382
				Split comma separated column data into additional columns <url:file:///~/projects/study/otl/cdb.otl#r=g10382>
				https://stackoverflow.com/questions/8584967/split-comma-separated-column-data-into-additional-columns/8612456#8612456
				q
					Column 
					------- 
					a,b,c,d 
					-->
					Column1  Column2 Column3 Column4 
					-------  ------- ------- -------
					a        b       c       d 
				ans
					split_part() does what you want in one step:
					code
						SELECT split_part(col, ',', 1) AS col1
								 , split_part(col, ',', 2) AS col2
								 , split_part(col, ',', 3) AS col3
								 , split_part(col, ',', 4) AS col4
						FROM   tbl;
			Store common query as column id=g10384
				Store common query as column <url:file:///~/projects/study/otl/cdb.otl#r=g10384>
				q
					SELECT <col 1>, <col 2>
						, (SELECT sum(<col x>)
						 FROM   <otherTable> 
						 WHERE  <other table foreignkeyCol>=<this table keycol>) AS <col 3>
						 FROM   <tbl>
					Given that the sub-select will be identical in every case, is there a way to store that sub-select as a pseudo-column in the table? 
				ans
					opt1: view
					opt2: function that emulates a computed field
						code
							CREATE TABLE tbl_a (a_id int, col1 int, col2 int);
							INSERT INTO tbl_a VALUES (1,1,1), (2,2,2), (3,3,3), (4,4,4);
							CREATE TABLE tbl_b (b_id int, a_id int, colx int);
							INSERT INTO tbl_b VALUES
							 (1,1,5),  (2,1,5),  (3,1,1)
							,(4,2,8),  (5,2,8),  (6,2,6)
							,(7,3,11), (8,3,11), (9,3,11);
							CREATE FUNCTION col3(tbl_a)
								RETURNS int8 AS
							$func$
									SELECT sum(colx)
									FROM   tbl_b b
									WHERE  b.a_id = $1.a_id
							$func$ LANGUAGE SQL STABLE;
						call
							SELECT a_id, col1, col2, tbl_a.col3
							FROM   tbl_a;
							Or even:
							SELECT *, a.col3 FROM tbl_a a;
						why does it work?
							The common way to reference a table column is with attribute notation:
								SELECT tbl_a.col1 FROM tbl_a;
							The common way to call a function is with functional notation:
								SELECT col3(tbl_a);
							Generally, it's best to stick to these canonical ways, which agree with the SQL standard.
							But in PostgreSQL, functional notation and attribute notation are equivalent. So these work as well:
								SELECT col1(tbl_a) FROM tbl_a;
								SELECT tbl_a.col3;
						IMMUTABLE vs STABLE
							The function should really be marked STABLE (meaning that if you call it with the same arguments more than once during the execution of a single query it will return the same value), not IMMUTABLE (meaning that it will always return the same value for the same arguments, regardless of database contents or the passage of time)
							IMMUTABLE would be OK if you were only using values from the row passed as a parameter
			Best way to delete millions of rows by ID id=g10387
				Best way to delete millions of rows by ID <url:file:///~/projects/study/otl/cdb.otl#r=g10387>
				https://stackoverflow.com/questions/8290900/best-way-to-delete-millions-of-rows-by-id/8290958#8290958
				ans
					big tables
						SET temp_buffers = 1000MB -- or whatever you can spare temporarily
						temp
							CREATE TEMP TABLE tmp AS
							SELECT t.*
							FROM   tbl t
							LEFT   JOIN del_list d USING (id)
							WHERE  d.id IS NULL;      -- copy surviving rows into temporary table
							TRUNCATE tbl;             -- empty table - truncate is very fast for big tables
						insert back
							INSERT INTO tbl
							SELECT * FROM tmp;        -- insert back surviving rows.
					small tables
						DELETE FROM tbl t
						USING  del_list d
						WHERE  t.id = d.id;
			Finding similar strings with PostgreSQL quickly id=g10386
				Finding similar strings with PostgreSQL quickly <url:file:///~/projects/study/otl/cdb.otl#r=g10386>
				https://stackoverflow.com/questions/11249635/finding-similar-strings-with-postgresql-quickly/11250001#11250001
				ans
					ex1: uses cross join
						SELECT set_limit(0.8);
						SELECT similarity(n1.name, n2.name) AS sim, n1.name, n2.name
						FROM   names n1
						JOIN   names n2 ON n1.name <> n2.name
													 AND n1.name % n2.name
						ORDER  BY sim DESC;
					restrict possible pairs before cross joining
					ex2: better
						SELECT set_limit(0.8);   -- fewer hits and faster with higher limit
						SELECT similarity(n1.name, n2.name) AS sim, n1.name, n2.name
						FROM   t n1
						JOIN   t n2 ON n1.name <> n2.name
											 AND n1.name % n2.name
						ORDER  BY sim DESC;
			Alphanumeric sorting with PostgreSQL id=g10395
				Alphanumeric sorting with PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10395>
				https://stackoverflow.com/questions/11417975/alphanumeric-sorting-with-postgresql/11418231#11418231
				ans
					ex1
						WITH x(t) AS (
								VALUES
								 ('10_asdaasda')
								,('100_inkskabsjd')
								,('11_kancaascjas')
								,('45_aksndsialcn')
								,('22_dsdaskjca')
								,('100_skdnascbka')
								)
						SELECT t
						FROM   x
						ORDER  BY (substring(t, '^[0-9]+'))::int     -- cast to integer
											,substring(t, '[^0-9_].*$')        -- works as text
					ex2
						If the underscore is unambiguous as separator anyway, split_part() is faster:
						ORDER  BY (split_part(t, '_', 1)::int
											,split_part(t, '_', 2)
					ex3
						SELECT name
						FROM   nametable
						ORDER  BY (split_part(name, '_', 1)::int
											,split_part(name, '_', 2)
			How do you find results that occurred in the past week? id=g10398
				How do you find results that occurred in the past week?  <url:file:///~/projects/study/otl/cdb.otl#r=g10398>
				https://stackoverflow.com/questions/8732517/how-do-you-find-results-that-occurred-in-the-past-week/8736967#8736967
				ans
					SELECT * FROM books WHERE returned_date > now()::date - 7
					points
						now()::date is the Postgres implementation of standard SQL CURRENT_DATE . Both do exactly the same in PostgreSQL
						now()::date - 7 works because one can subtract / add integer values (= days) from / to a date.
						With data type timestamp, you would have to add / subtract an interval
							select * from books where returned_date > current_date - interval '7 days'
			ORDER BY the IN value list id=g10408
				ORDER BY the IN value list <url:file:///~/projects/study/otl/cdb.otl#r=g10408>
				q
					SELECT * FROM comments WHERE (comments.id IN (1,3,2,4));
					This returns comments in an arbitrary order which in my happens to be ids like 1,2,3,4.
					I want the resulting rows sorted like the list in the IN construct: (1,3,2,4).
				ans
					opt1: VALUES ()
						select c.*
						from comments c
						join (
							values
								(1,1),
								(3,2),
								(2,3),
								(4,4)
						) as x (id, ordering) on c.id = x.id
						order by x.ordering
					opt2: unnest with ordinality and subquery
						select c.*
						from comments c
						join (
							select *
							from unnest(array[43,47,42]) with ordinality
						) as x (id, ordering) on c.id = x.id
						order by x.ordering
					opt3: array_position()
						select c.*
						from comments c
						where id in (42,48,43)
						order by array_position(array[42,48,43], c.id);
					opt3: ORDINALITY
						SELECT c.*
						FROM   comments c
						JOIN   unnest('{1,3,2,4}'::int[]) WITH ORDINALITY t(id, ord) USING (id)
						ORDER  BY t.ord;
			How to delete duplicate entries? id=g10409
				How to delete duplicate entries?  <url:file:///~/projects/study/otl/cdb.otl#r=g10409>
				https://stackoverflow.com/questions/1746213/how-to-delete-duplicate-entries/8826879#8826879
				ans
					Find out the size of your table:
						SELECT pg_size_pretty(pg_relation_size('tbl'));
					Set temp_buffers accordingly. Round up generously because in-memory representation needs a bit more RAM.
						SET temp_buffers = 200MB;    -- example value
						BEGIN;
						-- CREATE TEMPORARY TABLE t_tmp ON COMMIT DROP AS -- drop temp table at commit
						CREATE TEMPORARY TABLE t_tmp AS  -- retain temp table after commit
						SELECT DISTINCT * FROM tbl;  -- DISTINCT folds duplicates
						TRUNCATE tbl;
						INSERT INTO tbl
						SELECT * FROM t_tmp;
						-- ORDER BY id; -- optionally "cluster" data while being at it.
						COMMIT;
					points  
						TRUNCATE: begins clean state
			Is there a way to define a named constant in a PostgreSQL query? id=g10411
				Is there a way to define a named constant in a PostgreSQL query?  <url:file:///~/projects/study/otl/cdb.otl#r=g10411>
				https://stackoverflow.com/questions/13316773/is-there-a-way-to-define-a-named-constant-in-a-postgresql-query/13317628#13317628
				ans
					no built-in way to define (global) variables like MySQL or Oracle
					global persisten constant
						CREATE FUNCTION public.f_myid()
							RETURNS int IMMUTABLE LANGUAGE SQL AS
						'SELECT 5';
					Multiple values for current session:
						CREATE TEMP TABLE val (val_id int PRIMARY KEY, val text);
						INSERT INTO val(val_id, val) VALUES
							(  1, 'foo')
						, (  2, 'bar')
						, (317, 'baz');
						CREATE FUNCTION f_val(_id int)
							RETURNS text STABLE LANGUAGE SQL AS
						'SELECT val FROM val WHERE val_id = $1';
						SELECT f_val(2);  -- returns 'baz'
			Postgres Error: More than one row returned by a subquery used as an expression id=g10693
				Postgres Error: More than one row returned by a subquery used as an expression <url:file:///~/projects/study/otl/cdb.otl#r=g10693>
				https://stackoverflow.com/questions/21048955/postgres-error-more-than-one-row-returned-by-a-subquery-used-as-an-expression/21050919#21050919
				q
					UPDATE customer
					SET customer_id=
						 (SELECT t1 FROM dblink('port=5432, dbname=SERVER1 user=postgres password=309245',
						 'SELECT store_key FROM store') AS (t1 integer));
					-->
						ERROR:  more than one row returned by a subquery used as an expression
				ans
					code
						UPDATE customer c
						SET    customer_id = s.store_key
						FROM   dblink('port=5432, dbname=SERVER1 user=postgres password=309245'
												 ,'SELECT match_name, store_key FROM store')
									 AS s(match_name text, store_key integer)
						WHERE c.match_name = s.match_name
						AND   c.customer_id IS DISTINCT FROM s.store_key;
					points
						no correlated subquery
						match both tables with match_name
						prevent empty updates:
							AND   c.customer_id IS DISTINCT FROM s.store_key;
			Pass In “WHERE” parameters to PostgreSQL View? id=g10414
				Pass In “WHERE” parameters to PostgreSQL View?  <url:file:///~/projects/study/otl/cdb.otl#r=g10414>
				https://stackoverflow.com/questions/11401749/pass-in-where-parameters-to-postgresql-view/11402415#11402415
				ans
					code
						CREATE OR REPLACE FUNCTION param_labels(_region_label text, _model_label text)
							RETURNS TABLE (param_label text, param_graphics_label text) AS
						$func$
								SELECT p.param_label, p.param_graphics_label
								FROM   parameters      p 
								JOIN   parameter_links l USING (param_id)
								JOIN   regions         r USING (region_id)
								JOIN   models          m USING (model_id)
								WHERE  p.active
								AND    r.region_label = $1 
								AND    m.model_label = $2
								ORDER  BY p.param_graphics_label;
						$func$ LANGUAGE sql;
					points
						Care must be taken to avoid naming conflicts. That's why I make it a habit to prefix parameter names in the declaration (those are visible most everywhere inside the function) and table-qualify column names in the body
			SQL update fields of one table from fields of another one id=g10419
				SQL update fields of one table from fields of another one <url:file:///~/projects/study/otl/cdb.otl#r=g10419>
				https://stackoverflow.com/questions/2763817/sql-update-fields-of-one-table-from-fields-of-another-one/23283792#23283792
				ans
					Is there an UPDATE syntax ... without specifying the column names?
					opt1: General solution with dynamic SQL
						no column names except columns to join on
						building dynamic code based on information_schema.columns table
						code
							DO
							$do$
							BEGIN
							EXECUTE (
							SELECT
							'UPDATE b
							 SET   (' || string_agg(quote_ident(column_name), ',') || ')
									 = (' || string_agg('a.' || quote_ident(column_name), ',') || ')
							 FROM   a
							 WHERE  b.id = 123
							 AND    a.id = b.id'
							FROM   information_schema.columns
							WHERE  table_name   = 'a'       -- table name, case sensitive
							AND    table_schema = 'public'  -- schema name, case sensitive
							AND    column_name <> 'id'      -- all columns except id
							);
							END
							$do$;
					opt2: plain SQL with list of shared columns
						code
							UPDATE b
							SET   (  column1,   column2,   column3)
									= (a.column1, a.column2, a.column3)
							FROM   a
							WHERE  b.id = 123    -- optional, to update only selected row
							AND    a.id = b.id;
					opt3: plain SQL with list of columns in B
						code
							UPDATE b
							SET   (column1, column2, column3, column4)
									= (COALESCE(ab.column1, b.column1)
									 , COALESCE(ab.column2, b.column2)
									 , COALESCE(ab.column3, b.column3)
									 , COALESCE(ab.column4, b.column4)
										)
							FROM (
								 SELECT *
								 FROM   a
								 NATURAL LEFT JOIN  b -- append missing columns
								 WHERE  b.id IS NULL  -- only if anything actually changes
								 AND    a.id = 123    -- optional, to update only selected row
								 ) ab
							WHERE b.id = ab.id;
			Multiple CTE in single query id=g10425
				Multiple CTE in single query <url:file:///~/projects/study/otl/cdb.otl#r=g10425>
				https://stackoverflow.com/questions/35248217/multiple-cte-in-single-query/35249370#35249370
				ans
					code
						WITH RECURSIVE
							cte1 AS (...)  -- can still be non-recursive
						, cte2 AS (SELECT ...
											 UNION ALL
											 SELECT ...)  -- recursive term
						, cte3 AS (...)
						SELECT ... FROM cte3 WHERE ...
					points
						If RECURSIVE is specified, it allows a SELECT subquery to reference itself by name.
			Find difference between two big tables in PostgreSQL id=g10426
				Find difference between two big tables in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10426>
				https://stackoverflow.com/questions/15330403/find-difference-between-two-big-tables-in-postgresql/15333054#15333054
				ans
					opt1: EXISTS anti-semi-join:
						tbl1 is the table with surplus rows in this example:
						SELECT *
						FROM   tbl1
						WHERE  NOT EXISTS (SELECT 1 FROM tbl2 WHERE tbl2.col = tbl1.col);
					opt2: FULL OUTER JOIN
						If you don't know which table has surplus rows or both have, you can either repeat the above query after switching table names, or:
						SELECT *
						FROM   tbl1
						FULL   OUTER JOIN tbl2 USING (col)
						WHERE  tbl2 col IS NULL OR
									 tbl1.col IS NULL;
						Overview over basic techniques in a later post:
						Select rows which are not present in other table
			Guidance on using the WITH clause in SQL id=g10427
				Guidance on using the WITH clause in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10427>
				https://stackoverflow.com/questions/8721503/guidance-on-using-the-with-clause-in-sql/8725161#8725161
				ans
					ex1
						WITH x AS (
							 SELECT  psp_id
							 FROM    global.prospect
							 WHERE   status IN ('new', 'reset')
							 ORDER   BY request_ts
							 LIMIT   1
							 )
						UPDATE global.prospect psp
						SET    status = status || '*'
						FROM   x
						WHERE  psp.psp_id = x.psp_id
						RETURNING psp.*;
					ex2
						WITH x AS (
							 SELECT  psp_id
							 FROM    global.prospect
							 WHERE   status IN ('new', 'reset')
							 ORDER   BY request_ts
							 LIMIT   1
							 ), y AS (
							 UPDATE global.prospect psp
							 SET    status = status || '*'
							 FROM   x
							 WHERE  psp.psp_id = x.psp_id
							 RETURNING psp.*
							 )
						INSERT INTO z
						SELECT *
						FROM   y
			Nesting queries in SQL id=g10429
				Nesting queries in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10429>
				https://stackoverflow.com/questions/12467354/nesting-queries-in-sql/12467569#12467569
				ans
					ex: nested
						SELECT o.name AS country, o.headofstate 
						FROM   country o
						WHERE  o.headofstate like 'A%'
						AND   (
								SELECT i.population
								FROM   city i
								WHERE  i.id = o.capital
								) > 100000
					ex: join
						SELECT country.name as country, country.headofstate 
						from country
						inner join city on city.id = country.capital
						where city.population > 100000
						and country.headofstate like 'A%'
			How to display a default value when no match found in a query?
				https://stackoverflow.com/questions/8200462/how-to-display-a-default-value-when-no-match-found-in-a-query/8200473#8200473
				ans
					SELECT COALESCE((SELECT empname FROM employee WHERE id = 100), 'Unavailable')
					FROM   DUAL;
					note: SELECT inside another SELECT
			Select NOT IN multiple columns id=g10694
				Select NOT IN multiple columns <url:file:///~/projects/study/otl/cdb.otl#r=g10694>
				https://stackoverflow.com/questions/8033604/select-not-in-multiple-columns/8033700#8033700
				ans
					opt1
						SELECT * FROM friend f
						WHERE NOT EXISTS (
								SELECT 1 FROM likes l WHERE f.id1 = l.id and f.id2 = l.id2
						)
					opt2
						SELECT *
						FROM   friend f
						LEFT   JOIN likes l USING (id1, id2)
						WHERE  l.id1 IS NULL;
			Find The Missing Integer id=g10695
				Find The Missing Integer <url:file:///~/projects/study/otl/cdb.otl#r=g10695>
				http://tapoueh.org/blog/2017/05/find-the-missing-integer/
				ex
					> SELECT i FROM generate_series(1,100) as t(i)
						EXCEPT
						SELECT i FROM ints;
					 i
					----
					 55
					(1 row)
				opt: Anti join technique
					SELECT series.i
					FROM ints
					RIGHT JOIN (SELECT i
						FROM generate_series(1,100) t(i)
						) series
					ON series.i = ints.i
					WHERE ints.i IS NULL;
					 i
					----
					 55
					(1 row)
				opt: NOT EXISTS
					> SELECT i
							FROM generate_series(1,100) as t(i)
						 WHERE NOT EXISTS (SELECT 1
																 FROM ints
																WHERE ints.i = t.i);
					 i  
					----
						6
					 55
					(2 rows)
			What is the difference between LATERAL and a subquery in PostgreSQL? id=adb_006
				What is the difference between LATERAL and a subquery in PostgreSQL? <url:#r=adb_006>
				https://stackoverflow.com/questions/28550679/what-is-the-difference-between-lateral-and-a-subquery-in-postgresql#28557803
				ref
					Optimize GROUP BY query to retrieve latest record per user <url:#r=adb_007>
				LATERAL: more like a correlated subquery
					subquery has to be evaluated many times, once for each row in lhs
		syntax rules
			escaping single quotes
				https://stackoverflow.com/questions/12316953/insert-text-with-single-quotes-in-postgresql/12320729#12320729
				ans
					opt1:
						double ': ''
						'user''s log'
					opt2: dollar-quoted strings
						$$escape ' with '' $$
			Double colon (::) notation in SQL id=g10352
				Double colon (::) notation in SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10352>
				https://stackoverflow.com/questions/5758499/double-colon-notation-in-sql
				b.date_completed >  a.dc::date + INTERVAL '1 DAY 7:20:00'
				the :: converts a.dc to a date type of date.
				cast to a data type
			Are PostgreSQL column names case-sensitive? id=g10358
				Are PostgreSQL column names case-sensitive?  <url:file:///~/projects/study/otl/cdb.otl#r=g10358>
				https://stackoverflow.com/questions/20878932/are-postgresql-column-names-case-sensitive/20880247#20880247
				ans
					yes: All identifiers (including column names) that are not double-quoted are folded to lower case in PostgreSQL
					SELECT * FROM persons WHERE "first_Name" = 'xyz';
					Values (string literals) are enclosed in single quotes.
			Does PostgreSQL support "accent insensitive" collations?
				https://stackoverflow.com/questions/11005036/does-postgresql-support-accent-insensitive-collations/11007216#11007216
				q
					sql server: which means that it's possible for a query like
						SELECT * FROM users WHERE name LIKE 'João'
					to find a row with a Joao name.
				ans
					Use the unaccent module for that
						CREATE EXTENSION unaccent;
					ex
						SELECT *
						FROM   users
						WHERE  unaccent(name) = unaccent('João');
					index
						Postgres only accepts IMMUTABLE functions for indexes. If a function can return a different result for the same input, the index could silently break.
						unaccent() is only STABLE, not IMMUTABLE
					Best for now
						Create a wrapper function with the two-parameter form and "hard-wire" the schema for function and dictionary:
						CREATE OR REPLACE FUNCTION f_unaccent(text)
							RETURNS text AS
						$func$
						SELECT public.unaccent('public.unaccent', $1)  -- schema-qualify function and dictionary
						$func$  LANGUAGE sql IMMUTABLE;
			PostgreSQL IF statement id=g10383
				PostgreSQL IF statement <url:file:///~/projects/study/otl/cdb.otl#r=g10383>
				https://stackoverflow.com/questions/11299037/postgresql-if-statement/11299968#11299968
				ans
					DO
					$do$
					BEGIN
					IF EXISTS (SELECT 1 FROM orders) THEN
						 DELETE FROM orders;
					ELSE 
						 INSERT INTO orders VALUES (1,2,3);
					END IF;
					END
					$do$
					points
						A sub-select needs to be surrounded by parentheses:
						IF (SELECT count(*) FROM orders) > 0 ...
						IF (SELECT count(*) > 0 FROM orders) ...
			The forgotten assignment operator “=” and the commonplace “:=” id=g10407
				The forgotten assignment operator “=” and the commonplace “:=” <url:file:///~/projects/study/otl/cdb.otl#r=g10407>
				https://stackoverflow.com/questions/7462322/the-forgotten-assignment-operator-and-the-commonplace/22001209#22001209
				ans
					manual
						An assignment of a value to a PL/pgSQL variable is written as:
						variable { := | =  } expression;
						[...] Equal (=) can be used instead of PL/SQL-compliant :=.
					issue
						Function call with named notation:
							SELECT * FROM f_oracle(is_true := TRUE);
							Postgres identifies := as parameter assignment and all is well. However:
							SELECT * FROM f_oracle(is_true = TRUE);
							Since = is the SQL equality operator
			Using CASE in PostgreSQL to affect multiple columns at once
				interesting but hard to understand
			Difference between LIKE and ~ in Postgres id=g10420
				Difference between LIKE and ~ in Postgres <url:file:///~/projects/study/otl/cdb.otl#r=g10420>
				https://stackoverflow.com/questions/12452395/difference-between-like-and-in-postgres/12459689#12459689
				ans
					LIKE (~~) fastest
					~ is regex: more powerful
					never use: SIMILAR TO
					install pg_trgm: to use similarity operator %
					more: text search
					index
						Without pg_trgm, there is index support for left anchored search patterns
						code
							CREATE TABLE tbl(string text);
							INSERT INTO  tbl(string)
							SELECT x::text FROM generate_series(1, 10000) x;
							CREATE INDEX tbl_string_text_pattern_idx ON tbl(string text_pattern_ops);
							SELECT * FROM tbl WHERE string ~ '^1234';  -- left anchored pattern
			PostgreSQL loops outside functions. Is that possible?
				https://stackoverflow.com/questions/18340929/postgresql-loops-outside-functions-is-that-possible/18341502#18341502
				ans
					You cannot DECLARE (global) variables (well, there are ways around this) nor loop with plain SQL - with the exception of recursive CTEs as provided by @bma.
					However, there is the DO statement for such ad-hoc procedural code. Introduced with Postgres 9.0. It works like a one-time function, but does not return anything
					code
						DO
						$do$
						DECLARE
							 _counter int := 0;
						BEGIN
							 WHILE _counter < 10
							 LOOP
									_counter := _counter + 1;
									RAISE NOTICE 'The counter is %', _counter;  -- coerced to text automatically
							 END LOOP;
						END
						$do$
			Playing with Unicode
				http://tapoueh.org/blog/2017/07/playing-with-unicode/
				ergast web service and database
					http://ergast.com/mrd/
					import into pgs
						$ createdb f1db
						$ pgloader mysql://root@localhost/f1db pgsql:///f1db
						$ psql -d f1db -c 'ALTER DATABASE f1db SET search_path TO f1db, public;'
				ex
					select count(*) as victories,
								 forename, surname, nationality
						from drivers
								 left join results
												on drivers.driverid = results.driverid
											 and results.position = 1
					group by drivers.driverid
					order by victories desc
						 limit 10;
				out
						victories │ forename  │  surname   │ nationality 
					 ═══════════╪═══════════╪════════════╪═════════════
									 91 │ Michael   │ Schumacher │ German
									 56 │ Lewis     │ Hamilton   │ British
				make it flags
					create or replace function flag
					 (
						 code text
					 )
					 returns text
					language sql
					as $$
					 select    chr(  127462
												 + ascii(substring(code from 1 for 1))
												 - ascii('A'))
									|| chr(  127462
												 + ascii(substring(code from 2 for 1))
												 - ascii('A'))
					$$;
					With that it’s easier:
					select name, code, flag(code)
						from country
					 where code is not null
					 limit 10;
					And here’s a nice list of flags now:
								 name        | code | flag 
					-------------------+------+------
					 Afghanistan       | AF   | 🇦🇫
					 Albania           | AL   | 🇦🇱
				country codes
					https://github.com/datasets/country-codes
					import
						begin;
						create table public.country
						 (
							 name                             text,
							 official_name_en                 text,
							 official_name_fr                 text,
							 code                             text,
							 trigram                          text,
							 M49                              text,
							 ITU                              text,
							 MARC                             text,
							 WMO                              text,
							 DS                               text,
							 Dial                             text,
							 FIFA                             text,
							 FIPS                             text,
							 GAUL                             text,
							 IOC                              text,
							 currency_alphabetic_code         text,
							 currency_country_name            text,
							 currency_minor_unit              text,
							 currency_name                    text,
							 currency_numeric_code            text,
							 is_independent                   text,
							 Capital                          text,
							 Continent                        text,
							 TLD                              text,
							 Languages                        text,
							 Geoname                          text,
							 EDGAR                            text
						 );
						\copy public.country from 'country-codes.csv' with delimiter ',' csv header quote '"'
						commit;
		other
			configuration
				Query a parameter (postgresql.conf setting) like “max_connections” id=g10359
					Query a parameter (postgresql.conf setting) like “max_connections” <url:file:///~/projects/study/otl/cdb.otl#r=g10359>
					https://stackoverflow.com/questions/8288823/query-a-parameter-postgresql-conf-setting-like-max-connections/8288860#8288860
					ans
						opt
							SHOW max_connections;
							SHOW ALL;
							SELECT *
								FROM   pg_settings
								WHERE  name = 'max_connections';
			count
				Fast way to discover the row count of a table in PostgreSQL id=g10351
					Fast way to discover the row count of a table in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10351>
					https://stackoverflow.com/questions/7943233/fast-way-to-discover-the-row-count-of-a-table-in-postgresql/7945274#7945274
					ans
						counting can be slow
							precise number: needs full count of rows
								due to MVCC: concurrency control
						ex: exact count (slow)
							SELECT count(*) AS exact_count FROM myschema.mytable;
						ex: estimate (fast)
							SELECT reltuples::bigint AS estimate FROM pg_class where relname='mytable';
						ex: better (specify schema)
							SELECT c.reltuples::bigint AS estimate
							FROM   pg_class c
							JOIN   pg_namespace n ON n.oid = c.relnamespace
							WHERE  c.relname = 'mytable'
							AND    n.nspname = 'myschema'
						ex: better, more elegant
							SELECT reltuples::bigint AS estimate
							FROM   pg_class
							WHERE  oid = 'myschema.mytable'::regclass;
						better:
							to_regclass('myschema.mytable') 
						ex:
							Postgres actually stops counting beyond the given limit, you get an exact and current count for up to n rows (500000 in the example), and n otherwise
							SELECT count(*) FROM (SELECT 1 FROM token LIMIT 500000) t;
			etl
				COPY examples
					COPY (SELECT * FROM nyummy.cimory WHERE city = 'tokio') TO '/path/to/file.csv';
					COPY other_tbl FROM '/path/to/file.csv';
					COPY tmp_x FROM '/absolute/path/to/file' (FORMAT csv);
					COPY (SELECT * FROM ...) TO '/tmp/filename.csv' (format CSV);
					COPY client (id,name,address)
					1 Apple address_1_  
					COPY client (id,name,...) FROM STDIN (FREEZE ON); 
					1  Apple  address_1_   1   ...
					COPY book (bid,title,isbn) FROM STDIN (ENCODING 'utf-8');
					1 title_3587  9787040495126
					\COPY staging.factfinder_import FROM DEC_..csv CSV NULL AS '' DELIMITER '|' FROM somefile.txt
					\COPY (SELECT * FROM staging.factfinder_import WHERE s01 ~ E'^[0-9]+') TO '/test.tab' WITH DELIMITER E'\t' CSV HEADER
					\COPY public.country FROM 'country-codes.csv' WITH DELIMITER ',' CSV HEADER QUOTE '"'
				select into a table with different column names id=g10704
					select into a table with different column names <url:file:///~/projects/study/otl/cdb.otl#r=g10704>
					https://stackoverflow.com/questions/10560386/select-into-a-table-with-different-column-names/10560396#10560396
					ans: column names don't matter, as long as data types match
						INSERT INTO TableB (b1, b2, b3)
						SELECT a1, a2, a3
						FROM   TableA
						WHERE <some condition>;
				Export specific rows from a PostgreSQL table as INSERT SQL script id=g10361
					Export specific rows from a PostgreSQL table as INSERT SQL script <url:file:///~/projects/study/otl/cdb.otl#r=g10361>
					https://stackoverflow.com/questions/12815496/export-specific-rows-from-a-postgresql-table-as-insert-sql-script/12824831#12824831
					ans
						COPY
							For a data-only export use COPY.
							You get a file with one table row per line as plain text (not INSERT commands), it's smaller and faster:
							COPY (SELECT * FROM nyummy.cimory WHERE city = 'tokio') TO '/path/to/file.csv';
						Import 
							COPY other_tbl FROM '/path/to/file.csv';
						COPY vs pg_dump/psql
							COPY: runs on server
							pg_dump, psql: runs on client
						\copy
							run by client
							runs an SQL COPY command
							psql reads/writes
							This means that file accessibility and privileges are those of the local user, not the server, and no SQL superuser privileges are required
				How to update selected rows with values from a CSV file in Postgres? id=g10366
					How to update selected rows with values from a CSV file in Postgres?  <url:file:///~/projects/study/otl/cdb.otl#r=g10366>
					https://stackoverflow.com/questions/8910494/how-to-update-selected-rows-with-values-from-a-csv-file-in-postgres/8910810#8910810
					ans
						I would COPY the file to a temporary table and update the actual table from there. Could look like this:
							CREATE TEMP TABLE tmp_x (id int, apple text, banana text); -- but see below
							COPY tmp_x FROM '/absolute/path/to/file' (FORMAT csv);
							UPDATE tbl
							SET    banana = tmp_x.banana
							FROM   tmp_x
							WHERE  tbl.id = tmp_x.id;
							DROP TABLE tmp_x; -- else it is dropped at end of session automatically
						If the imported table matches the table to be updated exactly, this may be convenient:
							CREATE TEMP TABLE tmp_x AS SELECT * FROM tbl LIMIT 0;
				Insert data in 3 tables at a time using Postgres
					ans
						data-modifying CTE
							WITH ins1 AS (
								 INSERT INTO sample(firstname, lastname)
								 VALUES ('fai55', 'shaggk')
							-- ON     CONFLICT DO NOTHING                -- optional addition in Postgres 9.5+
								 RETURNING id AS user_id
								 )
							, ins2 AS (
								 INSERT INTO sample1 (user_id, adddetails)
								 SELECT user_id, 'ss' FROM ins1
								 -- RETURNING user_id                      -- only if used in turn
								 )
							INSERT INTO sample2 (user_id, value)         -- same here
							SELECT user_id, 'ss' FROM ins1;
						opt: provide complete data rows in one place:
							WITH data(firstname, lastname, adddetails, value) AS (
								 VALUES                                 -- provide data here
										(text 'fai55', text 'shaggk', text 'ss', text 'ss2')  -- see below
										 --  more?                          -- works for multiple input rows
								 )
							, ins1 AS (
								 INSERT INTO sample (firstname, lastname)
								 SELECT firstname, lastname FROM data   -- DISTINCT? see below
								 ON     CONFLICT DO NOTHING             -- required UNIQUE constraint
								 RETURNING firstname, lastname, id AS sample_id
								 )
							, ins2 AS (
								 INSERT INTO sample1 (sample_id, adddetails)
								 SELECT sample_id, adddetails
								 FROM   data
								 JOIN   ins1 USING (firstname, lastname)
								 RETURNING sample_id, user_id
								 )
							INSERT INTO sample2 (user_id, value)
							SELECT user_id, value
							FROM   data
							JOIN   ins1 USING (firstname, lastname)
							JOIN   ins2 USING (sample_id);
				INSERT rows into multiple tables in a single query, selecting from an involved table id=g10404
					INSERT rows into multiple tables in a single query, selecting from an involved table <url:file:///~/projects/study/otl/cdb.otl#r=g10404>
					ans
						test
							-- DROP TABLE foo; DROP TABLE bar;
							CREATE TEMP TABLE bar (
							 id serial PRIMARY KEY  -- using a serial column!
							,z  integer NOT NULL
							);
							CREATE TEMP TABLE foo (
							 id     serial PRIMARY KEY  -- using a serial column!
							,x      integer NOT NULL
							,y      integer NOT NULL
							,bar_id integer UNIQUE NOT NULL REFERENCES bar(id)
							);
							Insert values - bar first.
								INSERT INTO bar (id,z) VALUES
								 (100, 7)
								,(101,16)
								,(102,21);
								,(3,18,0,102);
							Set sequences to current values or we get duplicate key violations:
								SELECT setval('foo_id_seq', 3);
								SELECT setval('bar_id_seq', 102);
							Checks:
								-- SELECT nextval('foo_id_seq')
								-- SELECT nextval('bar_id_seq')
								-- SELECT * from bar;
								-- SELECT * from foo;
						modifying cte
							WITH a AS (
									SELECT f.x, f.y, bar_id, b.z
									FROM   foo f
									JOIN   bar b ON b.id = f.bar_id
									WHERE  x > 3
									),b AS (
									INSERT INTO bar (z)
									SELECT z
									FROM   a
									RETURNING z, id AS bar_id
									)
							INSERT INTO foo (x, y, bar_id)
							SELECT a.x, a.y, b.bar_id
							FROM   a
							JOIN   b USING (z);
						basics
							The basic form is:
								INSERT INTO foo (...)
								SELECT ... FROM foo WHERE ...
				How to export a PostgreSQL query output to a csv file id=g10415
					How to export a PostgreSQL query output to a csv file <url:file:///~/projects/study/otl/cdb.otl#r=g10415>
					https://stackoverflow.com/questions/29190632/how-to-export-a-postgresql-query-output-to-a-csv-file/29196924#29196924
					ans
						COPY (SELECT * FROM ...) TO '/tmp/filename.csv' (format CSV);
						opt:
							psql meta command \copy in a similar fashion. It writes (and reads) files local to the client and does not require superuser privileges
			functions
				Postgresql manual: format() id=adb_009 id=g10375
					Postgresql manual: format()  id=g10375 <url:file:///~/projects/study/otl/cdb.otl#r=g10375>
					Postgresql manual: format() <url:#r=adb_009>
					https://www.postgresql.org/docs/current/static/functions-string.html#FUNCTIONS-STRING-OTHER
					type (required)
						s formats the argument value as a simple string. A null value is treated as an empty string.
						I treats the argument value as an SQL identifier, double-quoting it if necessary. It is an error for the value to be null (equivalent to quote_ident).
						L quotes the argument value as an SQL literal. A null value is displayed as the string NULL, without quotes (equivalent to quote_nullable).
					%%: for literal %
					ex
						SELECT format('Hello %s', 'World');
						Result: Hello World
						SELECT format('Testing %s, %s, %s, %%', 'one', 'two', 'three');
						Result: Testing one, two, three, %
						SELECT format('INSERT INTO %I VALUES(%L)', 'Foo bar', E'O\'Reilly');
						Result: INSERT INTO "Foo bar" VALUES('O''Reilly')
						SELECT format('INSERT INTO %I VALUES(%L)', 'locations', E'C:\\Program Files');
						Result: INSERT INTO locations VALUES(E'C:\\Program Files')
				Is there something like a zip() function in PostgreSQL that combines two arrays? id=g10393
					Is there something like a zip() function in PostgreSQL that combines two arrays?  <url:file:///~/projects/study/otl/cdb.otl#r=g10393>
					https://stackoverflow.com/questions/12414750/is-there-something-like-a-zip-function-in-postgresql-that-combines-two-arrays/12414884#12414884
					ans
						x
							ex1:
								SELECT ARRAY[a,b] AS ab
								FROM  (
									 SELECT unnest('{a,b,c}'::text[]) AS a
												 ,unnest('{d,e,f}'::text[]) AS b
										) x;
								Result:
										ab
									-------
									 {a,d}
									 {b,e}
									 {c,f}
						both arrays must have the same number of elements to unnest in parallel, or you get a cross join instead.
						ex2
							CREATE OR REPLACE FUNCTION zip(anyarray, anyarray)
								RETURNS SETOF anyarray LANGUAGE SQL AS
							$func$
							SELECT ARRAY[a,b] FROM (SELECT unnest($1) AS a, unnest($2) AS b) x;
							$func$;
							Call:
								SELECT zip('{a,b,c}'::text[],'{d,e,f}'::text[]);
						ex3
							SELECT ARRAY[a,b] AS ab
							FROM   unnest('{a,b,c}'::text[] 
													, '{d,e,f}'::text[]) x(a,b);
						ex4
							SELECT array_agg('{a,b,c}'::text[],'{d,e,f}'::text[]); -- or any other array type
							-->
							{{a,d},{b,e},{c,f}}
			json
				How do I query using fields inside the new PostgreSQL JSON datatype?
					https://stackoverflow.com/questions/10560394/how-do-i-query-using-fields-inside-the-new-postgresql-json-datatype/10560761#10560761
					ans
						ex
							SELECT *
							FROM   json_array_elements(
								'[{"name": "Toby", "occupation": "Software Engineer"},
									{"name": "Zaphod", "occupation": "Galactic President"} ]'
								) AS elem
							WHERE elem->>'name' = 'Toby';
						major points:
							always use jsonb instead of json
				How to query a json column for empty objects?
					https://stackoverflow.com/questions/24292575/how-to-query-a-json-column-for-empty-objects/24296054#24296054
					ans
						select * from test where foo::text <> '{}'::text;
			null handling
				Unused index in range of dates query <url:file:///~/projects/study/otl/cdb.otl#r=g10464>
				Check if NULL exists in Postgres array id=g10350
					Check if NULL exists in Postgres array <url:file:///~/projects/study/otl/cdb.otl#r=g10350>
					https://stackoverflow.com/questions/34848009/check-if-null-exists-in-postgres-array/34848472#34848472
					opt 
						array_replace(ar, NULL, 0) <> ar
						array_remove(ar, NULL) <> ar
						array_position(ar, NULL) IS NOT NULL
					test case
						SELECT num, ar, expect
								, array_replace(ar, NULL, 0) <> ar                       AS t_93a --  99 ms
								, array_remove(ar, NULL) <> ar                           AS t_93b --  96 ms
								, cardinality(array_remove(ar, NULL)) <> cardinality(ar) AS t_94  --  81 ms
								, COALESCE(array_position(ar, NULL::int), 0) > 0         AS t_95a --  49 ms
								, array_position(ar, NULL) IS NOT NULL                   AS t_95b --  45 ms
								, CASE WHEN ar IS NOT NULL
											 THEN array_position(ar, NULL) IS NOT NULL END     AS t_95c --  48 ms
					 FROM  (
							VALUES (1, '{1,2,NULL}'::int[], true)     -- extended test case
									 , (2, '{-1,NULL,2}'      , true)
									 , (3, '{NULL}'           , true)
									 , (4, '{1,2,3}'          , false)
									 , (5, '{-1,2,3}'         , false)
									 , (6, NULL               , null)
							) t(num, ar, expect);
					Function wrapper
						For repeated use:
							CREATE OR REPLACE FUNCTION f_array_has_null (anyarray)
								RETURNS bool LANGUAGE sql IMMUTABLE AS
							 'SELECT array_position($1, NULL) IS NOT NULL';
						points
							Using a polymorphic input type this works for any array type, not just int[].
							Make it IMMUTABLE to allow performance optimization and index expressions
				Best way to check for “empty or null value” id=g10357
					Best way to check for “empty or null value” <url:file:///~/projects/study/otl/cdb.otl#r=g10357>
					https://stackoverflow.com/questions/23766084/best-way-to-check-for-empty-or-null-value/23767625#23767625
					q
						i use:
							coalesce( trim(stringexpression),'')=''
					ans
						stringexpression = '' yields:
							TRUE   .. for '' (or for any string consisting of only spaces with the data type char(n))
							NULL   .. for NULL
							FALSE .. for anything else
						So to check for: "stringexpression is either NULL or empty":
							(stringexpression = '') IS NOT FALSE
							Or the reverse approach (may be easier to read):
								(stringexpression <> '') IS NOT TRUE
						opt
							coalesce(stringexpression, '') = ''
						the opposite: stringexpression is neither NULL nor empty" is even simpler:
							stringexpression <> ''
						test
							SELECT stringexpression 
										,stringexpression = ''                    AS simple_test
										,(stringexpression = '')  IS NOT FALSE    AS test1
										,(stringexpression <> '') IS NOT TRUE     AS test2
										,coalesce(stringexpression, '') = ''      AS test_coalesce1
										,coalesce(stringexpression, '  ') = '  '  AS test_coalesce2
										,coalesce(stringexpression, '') = '  '    AS test_coalesce3
							FROM  (
								 VALUES
									 ('foo'::char(5))
								 , ('')
								 , (NULL)
								 , ('   ')                -- not different from '' in char(n)
								 ) sub(stringexpression);
				Sort NULL values to the end of a table id=g10376
					Sort NULL values to the end of a table <url:file:///~/projects/study/otl/cdb.otl#r=g10376>
					https://stackoverflow.com/questions/7621205/sort-null-values-to-the-end-of-a-table/7622046#7622046
					ans:
						NULL are sorted last in default ascending order
						in descending: NULL first
							ORDER BY col DESC NULLS LAST
				Why does PostgreSQL not return null values when the condition is <> true id=g10403
					Why does PostgreSQL not return null values when the condition is <> true <url:file:///~/projects/study/otl/cdb.otl#r=g10403>
					https://stackoverflow.com/questions/17679721/why-does-postgresql-not-return-null-values-when-the-condition-is-true/17680845#17680845
					ans
						manual
							Ordinary comparison operators yield null (signifying "unknown"), not true or false, when either input is null. For example, 7 = NULL yields null, as does 7 <> NULL. When this behavior is not suitable, use the  IS [ NOT ] DISTINCT FROM constructs:
							expression IS DISTINCT FROM expression
							expression IS NOT DISTINCT FROM expression
			performance
				How to perform a select query in a DO block?
					https://stackoverflow.com/questions/14652477/how-to-perform-a-select-query-in-a-do-block/14653151#14653151
					ans
						The DO command does not return rows
						SELECT row_number() OVER ()    AS running_month
									,extract('year'  FROM m) AS year
									,extract('month' FROM m) AS month
						FROM   generate_series('2012-04-01'::date
																	,'2016-01-01'::date
																	,'1 month'::interval) m;
			roles users groups
				Create PostgreSQL ROLE (user) if it doesn't exist id=g10367
					Create PostgreSQL ROLE (user) if it doesn't exist <url:file:///~/projects/study/otl/cdb.otl#r=g10367>
					https://stackoverflow.com/questions/8092086/create-postgresql-role-user-if-it-doesnt-exist/8099557#8099557
					ans
						DO
						$body$
						BEGIN
							 IF NOT EXISTS (
									SELECT                       -- SELECT list can stay empty for this
									FROM   pg_catalog.pg_user
									WHERE  usename = 'my_user') THEN
									CREATE ROLE my_user LOGIN PASSWORD 'my_password';
							 END IF;
						END
						$body$;
						Unlike, for instance, with CREATE TABLE there is no IF NOT EXISTS clause for CREATE ROLE (yet). And you cannot execute dynamic DDL statements in plain SQL.
				Grant all on a specific schema in the db to a group role in PostgreSQL id=g10379
					Grant all on a specific schema in the db to a group role in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10379>
					https://stackoverflow.com/questions/10352695/grant-all-on-a-specific-schema-in-the-db-to-a-group-role-in-postgresql/10353730#10353730
					ans
						GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA foo TO staff;
						not:
							GRANT ALL ON SCHEMA foo TO staff;
							GRANT ALL ON DATABASE mydb TO staff;
						note that ALL TABLES is considered to include views and foreign tables).
						for serial columns, also do:
							GRANT USAGE ON ALL SEQUENCES IN SCHEMA foo TO mygrp;
						what about new objects:
							ALTER DEFAULT PRIVILEGES IN SCHEMA foo GRANT ALL PRIVILEGES ON TABLES TO staff;
							ALTER DEFAULT PRIVILEGES IN SCHEMA foo GRANT USAGE          ON SEQUENCES TO staff;
							ALTER DEFAULT PRIVILEGES IN SCHEMA foo REVOKE ...;
				Grant privileges for a particular database in PostgreSQL
					https://stackoverflow.com/questions/24918367/grant-privileges-for-a-particular-database-in-postgresql/24923877#24923877
					ans
				Why did PostgreSQL merge users and groups into roles?
					https://stackoverflow.com/questions/8485387/why-did-postgresql-merge-users-and-groups-into-roles/8487886#8487886
					ans
						you can convert them
							ALTER ROLE myrole LOGIN;
							ALTER ROLE myrole NOLOGIN;
			temporal date data types
				Select today's (since midnight) timestamps only id=g10405
					Select today's (since midnight) timestamps only <url:file:///~/projects/study/otl/cdb.otl#r=g10405>
					https://stackoverflow.com/questions/9716868/select-todays-since-midnight-timestamps-only/9717125#9717125
					q
						need to get a list of connected users (i.e. their timestamps are u.login > u.logout):
					ans
						SELECT u.login, u.id, u.first_name
						FROM   pref_users u
						WHERE  u.login > u.logout
						AND    u.login >= now()::date + interval '1h'
						ORDER  BY u.login;
						points
							An easy way of getting only time stamps for the current day since 01:00 is to filter with CURRENT_DATE + interval '1 hour'
				Find overlapping date ranges in PostgreSQL id=g10388
					Find overlapping date ranges in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10388>
					https://stackoverflow.com/questions/4480715/find-overlapping-date-ranges-in-postgresql/15305292#15305292
					ans
						a BETWEEN x AND y translates to:
							a >= x AND a <= y
							Including the upper border, while people typically need to exclude it:
							a >= x AND a < y
						correct answer:
							SELECT DISTINCT p.* 
							FROM   contract c
							JOIN   player   p USING (name_player) 
							WHERE  c.name_team = ? 
							AND    c.date_join  <  date '2010-01-01'
							AND   (c.date_leave >= date '2009-01-01' OR c.date_leave IS NULL);
				How to convert “string” to “timestamp without time zone” id=g10400
					How to convert “string” to “timestamp without time zone” <url:file:///~/projects/study/otl/cdb.otl#r=g10400>
					https://stackoverflow.com/questions/18913236/how-to-convert-string-to-timestamp-without-time-zone/18919571#18919571
					ans
						opt1
							Use ISO 8601 format, which works the same with any locale or DateStyle setting:
							'2013-08-20 14:52:49'::timestamp
						opt2
							to_timestamp('20/8/2013 14:52:49', 'DD/MM/YYYY hh24:mi:ss')
				Get month name from number in PostgreSQL id=g10412
					Get month name from number in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10412>
					https://stackoverflow.com/questions/9094392/get-month-name-from-number-in-postgresql/9097651#9097651
					ans
						SELECT to_char(to_timestamp (4::text, 'MM'), 'TMmon')
						points
							A plain cast to text 4::text is enough, no need for to_char(..).
							Question asks for lower case "jan", there is a template pattern for that: mon.
							If you want to localize the output, prefix the template with the modifier TM.
				Selecting records between two timestamps id=g10413
					Selecting records between two timestamps <url:file:///~/projects/study/otl/cdb.otl#r=g10413>
					https://stackoverflow.com/questions/8723523/selecting-records-between-two-timestamps/8860632#8860632
					ans
						SELECT a,b,c
						FROM   table
						WHERE  xtime BETWEEN '2012-04-01 23:55:00'::timestamp
														 AND now()::timestamp;
						If you want to operate with a count of seconds as interval:
							...
							WHERE  xtime BETWEEN now()::timestamp - (interval '1s') * $selectedtimeParm
															 AND now()::timestamp;
						Note also, that the first value for the BETWEEN construct must be the smaller one. If you don't know which value is smaller use BETWEEN SYMMETRIC instead.
			PostgreSQL error: Fatal: role “username” does not exist
				https://stackoverflow.com/questions/11919391/postgresql-error-fatal-role-username-does-not-exist/11919677#11919677
				q
					Fatal: role h9uest does not exist
				ans
					use system user 'postgres' to create your db
					sudo -u postgres -i
			PostgreSQL Crosstab Query id=adb_010
				PostgreSQL Crosstab Query <url:#r=adb_010>
				https://stackoverflow.com/questions/3002499/postgresql-crosstab-query/11751905#11751905
				q
					how to create crosstab queries
					ex:
						Section    Status    Count
						A          Active    1
						A          Inactive  2
						B          Active    4
						B          Inactive  5
						-->
						Section    Active    Inactive
						A          1         2
						B          4         5
				ans
					install 'tablefunc'
						it provides crosstab()
					test case
						CREATE TEMP TABLE t (
							section   text
						, status    text
						, ct        integer  -- don't use "count" as column name.
						);
						INSERT INTO t VALUES 
							('A', 'Active', 1), ('A', 'Inactive', 2)
						, ('B', 'Active', 4), ('B', 'Inactive', 5)
																, ('C', 'Inactive', 7);  -- 'C' with 'Active' is missing
					sql
						SELECT *
						FROM   crosstab(
							 'SELECT section, status, ct
								FROM   t
								ORDER  BY 1,2'  -- needs to be "ORDER BY 1,2" here
							 ) AS ct ("Section" text, "Active" int, "Inactive" int);
						-->
						Section | Active | Inactive
						---------+--------+----------
						A       |      1 |        2
						B       |      4 |        5
						C       |      7 |           -- !!
			What are the pros and cons of performing calculations in sql vs. in your application
				https://stackoverflow.com/questions/7510092/what-are-the-pros-and-cons-of-performing-calculations-in-sql-vs-in-your-applica/7518619#7518619
				ans
					RDBMS excels at complex queries
			sql injection
				SQL Injection with user input id=g10373
					SQL Injection with user input <url:file:///~/projects/study/otl/cdb.otl#r=g10373>
					http://sqlfiddle.com/#!15/39ef7/3
					schema
						CREATE TABLE foo (id serial, data text);
						INSERT INTO foo (data) VALUES ('Important data');
						CREATE FUNCTION unsafe_add_table(text)
							RETURNS void AS
						$func$
						BEGIN
							 EXECUTE 'CREATE TABLE ' || $1 || '(item_1 int, item_2 int)';
						END
						$func$  LANGUAGE plpgsql;
					code
						TABLE foo;
						-- malicious call with SQL injection
						SELECT unsafe_add_table('bar(id int); DELETE FROM foo; CREATE TABLE baz');
						TABLE foo;
				SQL injection in Postgres functions vs prepared queries id=g10374
					SQL injection in Postgres functions vs prepared queries <url:file:///~/projects/study/otl/cdb.otl#r=g10374>
					https://dba.stackexchange.com/questions/49699/sql-injection-in-postgres-functions-vs-prepared-queries
					ans
						With SQL functions (LANGUAGE sql), the answer is generally yes. Passed parameters are treated as values and SQL-injection is not possible - as long as you don't call unsafe functions from within and pass parameters.
						With PL/pgSQL functions (LANGUAGE plpgsql), the answer is normally yes.
							However, PL/pgSQL allows for dynamic SQL where passed parameters (or parts) can be treated as identifiers or code, which makes SQL injection possible. You cannot tell from outside whether the function body deals with that properly. 
						If parameters should be treated as values or plain text in dynamic SQL with EXECUTE, use
							the USING clause. Example.
								function
									CREATE OR REPLACE FUNCTION get_stuff(_param text, _orderby text, _limit int)
										RETURNS SETOF stuff AS
									$BODY$
									BEGIN
									RETURN QUERY EXECUTE '
											SELECT *
											FROM   stuff
											WHERE  col = $1
											ORDER  BY ' || quote_ident(_orderby) || '
											LIMIT  $2'
									USING _param, _limit;
									END;
									$BODY$
										LANGUAGE plpgsql;
								Call:
									SELECT * FROM get_stuff('hello', 'col2', 100);
							format() with %L. The format specifier %s is only good for safe text, not for user input.
								ref
									Postgresql manual: format() <url:#r=adb_009>
							quote_literal() or quote_nullable()
						If parameters should be treated as identifiers, properly sanitize them with one of these tools:
							format() with %I
							quote_ident(_tbl)
							A cast to a registered type (regclass) _tbl::regclass. Example.
				PostgreSQL: SQL Injection
					http://bobby-tables.com/postgresql
					ex1: plpgsql
						CREATE OR REPLACE FUNCTION user_access (p_uname TEXT)
							RETURNS timestamp LANGUAGE plpgsql AS
						$func$
						BEGIN
								RETURN accessed_at FROM users WHERE username = p_uname;
						END
						$func$;
					ex2: sql
						CREATE OR REPLACE FUNCTION user_access (p_uname TEXT)
							RETURNS timestamp LANGUAGE sql AS
						$func$
								SELECT accessed_at FROM users WHERE username = $1
						$func$;
					ex3: dynamic sql - vulnerable
						CREATE OR REPLACE FUNCTION get_users(p_column TEXT, p_value TEXT)
							RETURNS SETOF users LANGUAGE plpgsql AS
						$func$
						DECLARE
								query TEXT := 'SELECT * FROM users';
						BEGIN
								IF p_column IS NOT NULL THEN
										query := query || ' WHERE ' || p_column
													|| $_$ = '$_$ || p_value || $_$'$_$;
								END IF;
								RETURN QUERY EXECUTE query;
						END
						$func$;
					ex4: better: USING
						CREATE OR REPLACE FUNCTION get_users(p_column TEXT, p_value TEXT)
							RETURNS SETOF users LANGUAGE plpgsql AS
						$func$
						DECLARE
								query TEXT := 'SELECT * FROM users';
						BEGIN
								IF p_column IS NOT NULL THEN
										query := query || ' WHERE ' || quote_ident(p_column) || ' = $1';
								END IF;
								RETURN QUERY EXECUTE query
								USING p_value;
						END;
						$func$;
			How to create simple fuzzy search with Postgresql only? id=g10396
				How to create simple fuzzy search with Postgresql only?  <url:file:///~/projects/study/otl/cdb.otl#r=g10396>
				https://stackoverflow.com/questions/7730027/how-to-create-simple-fuzzy-search-with-postgresql-only/7747765#7747765
					ex: levenstein
						several string comparsion functions such as soundex and metaphone. But you will want to use the levenshtein edit distance function.
							test=# SELECT levenshtein('GUMBO', 'GAMBOL');
							 levenshtein
							-------------
												 2
							(1 row)
						The 2 is the edit distance between the two words
					ex
						SELECT * 
						FROM some_table
						WHERE levenshtein(code, 'AB123-lHdfj') <= 3
						ORDER BY levenshtein(code, 'AB123-lHdfj')
						LIMIT 10
					installing
						test=# CREATE EXTENSION fuzzystrmatch;
			Is SELECT or INSERT in a function prone to race conditions?
				https://stackoverflow.com/questions/955167/return-setof-record-virtual-table-from-function/17247118#17247118
				ans
					Assuming you want to return three integer columns.
					plpgsql
						CREATE OR REPLACE FUNCTION f_foo(open_id numeric)
							RETURNS TABLE (a int, b int, c int) AS
						$func$
						BEGIN
							 -- do something with open_id?
							 RETURN QUERY VALUES
								 (1,2,3)
							 , (3,4,5)
							 , (3,4,5);
						END
						$func$  LANGUAGE plpgsql IMMUTABLE ROWS 3;
						Call:
							SELECT * FROM f_foo(1);
						points
							Use RETURNS TABLE to define an ad-hoc row type to return.
							Or RETURNS SETOF mytbl to use a pre-defined row type.
							Use RETURN QUERY to return multiple rows with one command.
							Use a VALUES expression to enter multiple rows manually. This is standard SQL and has been around for ever.
					sql
						opt1
							VALUES (1,2,3), (3,4,5), (3,4,5)
						opt2: Or, if you want (or have) to define specific column names and types:
							SELECT *
							FROM  (
								 VALUES (1::int, 2::int, 3::int)
											, (3, 4, 5)
											, (3, 4, 5)
								 ) AS t(a, b, c);
					sql function
						CREATE OR REPLACE FUNCTION foo()
							 RETURNS TABLE (a int, b int, c int) AS
						$func$
							 VALUES (1, 2, 3)
										, (3, 4, 5)
										, (3, 4, 5);
						$func$  LANGUAGE sql IMMUTABLE ROWS 3;
			Pivot on Multiple Columns using Tablefunc id=adb_011
				Pivot on Multiple Columns using Tablefunc <url:#r=adb_011>
				https://stackoverflow.com/questions/15415446/pivot-on-multiple-columns-using-tablefunc/15421607#15421607
				q
					test
						CREATE TEMP TABLE t4 (
						 timeof   timestamp
						,entity    character
						,status    integer
						,ct        integer);
						INSERT INTO t4 VALUES 
						 ('2012-01-01', 'a', 1, 1)
						,('2012-01-01', 'a', 0, 2)
						,('2012-01-02', 'b', 1, 3)
						,('2012-01-02', 'c', 0, 4);
					goal
						Section                   | Attribute | 1 | 0
						--------------------------+-----------+---+---
						2012-01-01 00:00:00       |     a     | 1 | 2
						2012-01-02 00:00:00       |     b     | 3 |  
						2012-01-02 00:00:00       |     c     |   | 4
				ans
					pivot with multiple columns
					SELECT * FROM crosstab(
								'SELECT entity, timeof, status, ct
								 FROM   t4
								 ORDER  BY 1'
								,'VALUES (1), (0)')
					 AS ct (
							"Attribute" character
						 ,"Section" timestamp
						 ,"status_1" int
						 ,"status_0" int);
			Create a pivot table with PostgreSQL id=g10431
				Create a pivot table with PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10431>
				https://stackoverflow.com/questions/20618323/create-a-pivot-table-with-postgresql/20618487#20618487
				ans
					SELECT neighborhood, bedrooms, avg(price)
					FROM   listings
					GROUP  BY 1,2
					ORDER  BY 1,2
					then feed the result to crosstab()
						PostgreSQL Crosstab Query <url:#r=adb_010>
			PostgreSQL trigger not working - neither BEFORE nor AFTER DELETE
				https://stackoverflow.com/questions/10687582/postgresql-trigger-not-working-neither-before-nor-after-delete/10687979#10687979
				ans
					Your trigger function ends with:
						RETURN NULL;
					You need to replace that with:
						RETURN OLD;
					for the system to proceed with the deletion of the row.
					The usual idiom in DELETE triggers is to return OLD
			How to Write SQL id=g10436
				How to Write SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10436>
				http://tapoueh.org/blog/2017/06/how-to-write-sql/
				writing your SQL queries in .sql files in your code repository and maintain them there
				ex: Chinook database
					https://github.com/lerocha/chinook-database
					sql
						select name as title,
									 milliseconds * interval '1ms' as duration,
									 round(  milliseconds
												 / sum(milliseconds) over ()
												 * 100, 2)
									 as pct
						 from track
						where albumid = :id
						order by trackid;
					out
															title                  │       duration       │  pct  
						═════════════════════════════════════════╪══════════════════════╪═══════
						 For Those About To Rock (We Salute You) │ @ 5 mins 43.719 secs │ 14.32
						 Put The Finger On You                   │ @ 3 mins 25.662 secs │  8.57
					call
						psql --variable "id=1" -f album.sql chinook
						opt
							> \cd path/to/my/sources
							> \set id 1
							> \i album.sql
				Dynamically building SQL queries
					ex: CASE within WHERE
						SELECT ... WHERE CASE WHEN x <> 0 THEN y/x > 1.5 ELSE false END;
					my advice is to Keep It Simple and have a SQL file for each main set of conditions
						in app: pick the SQL file
						You might have some SQL code duplication when doing so
					pro
						better modularity: it is now really easy to fix that query you discover being problematic in your production logs
						simple to replay and explain the query interactively
				Integrating SQL code in python with anosql
					sql file
						-- name: list-tracks-by-albumid
						-- List the tracks of an album, includes duration and position
							 select name as title,
											milliseconds * interval '1ms' as duration,
											round(  milliseconds
														/ sum(milliseconds) over ()
														* 100, 2)
											as pct
								from track
							 where albumid = :id
						order by trackid;
						note: name attribute
					py
						class chinook(object):
								"""Our database model and queries"""
								def __init__(self,
														 pgconnstring = "dbname=chinook application_name=cdstore"):
										self.pgconn = psycopg2.connect(pgconnstring)
										self.album  = anosql.load_queries('postgres', 'album.sql')
								def album_details(self, albumid):
										return self.album.list_tracks_by_albumid(self.pgconn, id=albumid)
						def foo(albumid):
								db = chinook()
								for (title, duration, pct) in db.album_details(albumid):
										... do something here ...
			SQL Regression Tests id=g10437
				SQL Regression Tests <url:file:///~/projects/study/otl/cdb.otl#r=g10437>
				http://tapoueh.org/blog/2017/08/sql-regression-tests/
				pgtap
					VALUES
						SELECT results_eq(
								'SELECT * FROM active_users()',
								$$
									VALUES (42, 'Anna'),
												 (19, 'Strongrrl'),
												 (39, 'Theory')
								$$,
								'active_users() should return active users'
						);
					ARRAYS:
						SELECT results_eq(
								'SELECT * FROM active_user_ids()',
								ARRAY[ 2, 3, 4, 5]
						);
				RegreSQL
			Shell script to execute pgsql commands in files id=g10703
				Shell script to execute pgsql commands in files <url:file:///~/projects/study/otl/cdb.otl#r=g10703>
					https://stackoverflow.com/questions/8594717/shell-script-to-execute-pgsql-commands-in-files/8595568#8595568
					ans
						1: don't mix psql and SQL
							make files contain only SQL
						2: don't put CREATE DATABASE in SQL files
							assuming:
								you are: system user `postgres`
								and `postgres` is superuser
								and `postgres` has paswordless access due to `IDENT` in `pg_hba.conf`
							psql postgres -c "CREATE DATABASE mytemplate1 WITH ENCODING 'UTF8'
																TEMPLATE template0"
						3: running SQL
							psql mytemplate1 -f file
						4: run single command
							psql -c 'CREATE DATABASE myDB TEMPLATE mytemplate1'
							Can be multiple commands, terminated by ; - will be executed in one transaction and only the result of the last command returned.
	Postgres DISTINCT vs DISTINCT ON
		https://stackoverflow.com/questions/50846722/postgres-distinct-vs-distinct-on
		DISTINCT
			applies to an entire tuple
				once result is computed, DISTINCT removes any duplicate tuples from the result
			SELECT DISTINCT * FROM r
			SELECT DISTINCT a,b FROM r
			you cannot issue
				SELECT a, DISTINCT b FROM r
		DISTINCT ON
			compute query, but before projection, sort result and group it according to DISTINCT ON. now do the projection using the first tuple in each group and ignore the other tuples.
			SELECT DISTINCT ON (attributeList) <rest as any query>
	PostgreSQL - set a default cell value according to another cell value id=g10705
		PostgreSQL - set a default cell value according to another cell value <url:file:///~/projects/study/otl/cdb.otl#r=g10705>
		https://stackoverflow.com/questions/16737738/postgresql-set-a-default-cell-value-according-to-another-cell-value/16754952#16754952
		ans
			not possible with `DEFAULT`
				The value is any variable-free expression
			opt1: trigger
				CREATE OR REPLACE FUNCTION trg_foo_b_default()
					RETURNS trigger AS
				$func$
				BEGIN
				-- For just a few constant options, CASE does the job:
				NEW.b :=
					 CASE NEW.a
						WHEN 'peter'  THEN 'doctor'
						WHEN 'weirdo' THEN 'shrink'
						WHEN 'django' THEN 'undertaker'
						ELSE NULL
					 END;
				/* -- For more, or dynamic options, you could use a lookup table:
				SELECT INTO NEW.b  t.b
				FROM   def_tbl t
				WHERE  t.a = NEW.a;
				*/
				RETURN NEW;
				END
				$func$ LANGUAGE plpgsql;
				CREATE TRIGGER b_default
				BEFORE INSERT ON foo
				FOR EACH ROW
				WHEN (NEW.b IS NULL AND NEW.a IS NOT NULL)
				EXECUTE PROCEDURE trg_foo_b_default();
	Computed / calculated columns in PostgreSQL
		https://stackoverflow.com/questions/8250389/computed-calculated-columns-in-postgresql/8250729#8250729
		ans
			implement similar functionality with a view.
			Or you can use functions that work and look just like computed columns.
	Truncating all tables in a Postgres database
		https://stackoverflow.com/questions/2829158/truncating-all-tables-in-a-postgres-database/12082038#12082038
	Postgresql Manual: pg_trgm: similarity of text
		text % text boolean Returns true if its arguments have a similarity that is greater than the current similarity threshold set by pg_trgm.similarity_threshold.
	Give all the permissions to a user on a DB
	Return pre-UPDATE Column Values Using SQL Only - PostgreSQL Version
	How does the search_path influence identifier resolution and the “current schema”
	PostgreSQL composite primary key
	Iterating over integer[] in PL/pgSQL
	Query for element of array in JSON column
	Discard millisecond part from timestamp
	Update a column of a table with a column of another table in PostgreSQL
	Order varchar string as numeric
	https://stackoverflow.com/questions/22736742/query-for-array-elements-inside-json-type/22737710#22737710
	Maximum characters in labels (table names, columns etc)
	Running PostgreSQL in memory only
	How to use RETURNING with ON CONFLICT in PostgreSQL?Feb 14 at 4:27 42 Return a value if no record is found
	Return zero if no record is found
	Allow null in unique column
	Now() without timezone
	Return multiple fields as a record in PostgreSQL with PL/pgSQL
	Get execution time of PostgreSQL query
	INSERT with dynamic table name in trigger functionOct 27 '11 at 11:16
	Dynamic alternative to pivot with CASE and GROUP BY
	Trim trailing spaces with PostgreSQL
	Simulate CREATE DATABASE IF NOT EXISTS for PostgreSQL?
	MySQL: Get most recent record
	Run batch file with psql command without password
	Cast syntax to convert a sum to float
	Generic Ruby solution for SQLite3 “LIKE” or PostgreSQL “ILIKE”?
	Delete rows with foreign key in PostgreSQL
	Postgres data type cast
	Query with LEFT JOIN not returning rows for count of 0
	Can I make a plpgsql function return an integer without using a variable?
	Making sense of Postgres row sizes
	PostgreSQL how to see which queries have run
	How to add a variable number of hours to a date in PostgreSQL?
	Performance Tuning : Create index for boolean column
	PostgreSQL Where count condition
	PostgreSQL: ERROR: 42601: a column definition list is required for functions returning “record”
	How do you do date math that ignores the year?
	PostgreSQL MAX and GROUP BY
	Do nullable columns occupy additional space in PostgreSQL?
	Best way to count records by arbitrary time intervals in Rails+Postgres
	GROUP BY + CASE statement
	Store the query result in variable using postgresql Stored procedure
	Return as array of JSON objects in SQL (Postgres)
	PostgreSQL: export resulting data from SQL query to Excel/CSV
	How to convert empty to null in PostgreSQL?
	select into a table with different column names
	Optional argument in PL/pgSQL stored procedureJul 22 '12 at 21:41
	Determining the OID of a table in Postgres 9.1?
	PostgreSQL next value of the sequences?
	Postgres - Transpose Rows to Columns
	Postgres accent insensitive LIKE search in Rails 3.1 on Heroku
	How to retrieve the comment of a PostgreSQL database?
	Postgis install
	How much disk-space is needed to store a NULL value using postgresql DB?
	What's the proper index for querying structures in arrays in Postgres jsonb?
	Getting results between two dates in PostgreSQL
	How to grant all privileges on views to arbitrary user
	BREAK statement in PL/pgSQL
	Why can I create a table with PRIMARY KEY on a nullable column?
	Safely and cleanly rename tables that use serial primary key columns in Postgres?
	PostgreSQL cannot begin/end transactions in PL/pgSQL
	Keep PostgreSQL from sometimes choosing a bad query plan
	PostgreSQL - DB user should only be allowed to call functions
	Postgres query optimization (forcing an index scan)
	Renaming multiple columns in one statement with PostgreSQL
	How to perform update operations on columns of type JSONB in Postgres 9.4
	Rails Migrations: tried to change the type of column from string to integer
	Define table and column names as arguments in a plpgsql function?
	Calculate working hours between 2 dates in PostgreSQL
	Can PostgreSQL have a uniqueness constraint on array elements?
	Avoid division by zero in PostgreSQL
	Drop sequence and cascade
	How do I do large non-blocking updates in PostgreSQL?
	How to get function parameter lists (so I can drop a function)
	PostgreSQL date() with timezone
	PostgreSQL delete all content
	Difference between GiST and GIN index
	PostgreSQL Index on JSON
	Is there any difference between integer and bit(n) data types for a bitmask?
	How to check if an array is empty in Postgres
	Update top N values using PostgreSQL
	psql: no relations found despite existing relations
	Primary & Foreign Keys in pgAdmin
	Split column into multiple rows in Postgres
	Run a query with a LIMIT/OFFSET and also get the total number of rows
	postgresql index on string column
	Concatenate strings in Oracle SQL without a space in between?
	How to create a temporary function in PostgreSQL?
	How to reset postgres' primary key sequence when it falls out of sync?
	Composite PRIMARY KEY enforces NOT NULL constraints on involved columns
	PostgreSQL - Writing dynamic sql in stored procedure that returns a result set
	Does the order of columns in a postgres table impact performance?
	Case insensitive unique model fields in Django?
	Passing column names dynamically for a record variable in PostgreSQL
	Incrementing a number in a loop in plpgsql
	PostgreSQL table variable
	PostgreSQL - set a default cell value according to another cell value
	Does not using NULL in PostgreSQL still use a NULL bitmap in the header?
	PostgreSQL convert columns to rows? Transpose?
	How to add column if not exists on PostgreSQL?
	Dynamic SQL (EXECUTE) as condition for IF statement
	Cursor based records in PostgreSQL
	Foreign key contraints in many-to-many relationships
	PostgreSQL index not used for query on range
	Update multiple rows in a table from another table when condition exists
	PostgreSQL: between with datetime
	Creating a PostgreSQL sequence to a field (which is not the ID of the record)
	Optimize Postgres timestamp query range
	Is there a shortcut for SELECT * FROM in psql?
	PostgreSQL: Create an index for fields within a composite type?
	Connection refused (PGError) (postgresql and rails)
	Migrate datetime w. timezone in PostgreSQL to UTC timezone to use Django 1.4
	How to get the date and time from timestamp in PostgreSQL select query?
	Check statistics targets in PostgreSQL
	Convert date from long time postgres
	Insert SQL statements via command line without reopening connection to remote database
	How to add a conditional unique index on PostgreSQL
	Search in integer array in Postgres
	drop all tables sharing the same prefix in postgres
	XML data to PostgreSQL database
	How to use ANY instead of IN in a WHERE clause with Rails?
	Cut string after first occurrence of a character
	SQLAlchemy: how to filter on PgArray column types?
	PostgreSQL parameterized Order By / Limit in table function
	How does PostgreSQL enforce the UNIQUE constraint / what type of index does it use?
	Adding a non-nullable column to existing table fails. Is the “value” attribute being ignored?
	Does setting “NOT NULL” on a column in postgresql increase performance?
	PostgreSQL CASE usage in functions
	Postgres - how to return rows with 0 count for missing data?
	PostgreSQL loops outside functions. Is that possible?
	Do stored procedures run in database transaction in Postgres?
	Why do NULL values come first when ordering DESC in a PostgreSQL query?
	Atomic UPDATE .. SELECT in Postgres
	PostgreSQL Update trigger
	Postgres 9.2 - add conditional constraint check
	How to use a SQL window function to calculate a percentage of an aggregate
	How to display a default value when no match found in a query?
	Cannot cast type numeric to boolean
	Dropping column in Postgres on a large dataset
	pgAdmin shortcuts to execute scripts
	Nesting queries in SQL
	COPY function in PostgreSQL
	Exporting MYSQL data into Excel/CSV via php
	Perform this hours of operation query in PostgreSQL
	Optimize query with OFFSET on large table
	Any downsides of using data type “text” for storing strings?
	PostgreSQL aggregate or window function to return just the last value
	Update Redshift table from query
	Create a pivot table with PostgreSQL
	PostgreSQL trigger not working - neither BEFORE nor AFTER DELETEMay 21 '12 at 15:16
	Passing a record as function argument PL/pgSQL
	SELECT INTO with more than one attribution
	How to check if a row exists in a PostgreSQL stored procedure?
	Change varchar to boolean in PostgreSQL
	Within a trigger function, how to get which fields are being updated
	Does setting “NOT NULL” on a column in postgresql increase performance?
	PostgreSQL CASE usage in functions
	Postgres - how to return rows with 0 count for missing data?
	Do stored procedures run in database transaction in Postgres?
	Why do NULL values come first when ordering DESC in a PostgreSQL query?
	Atomic UPDATE .. SELECT in Postgres
	PostgreSQL Update trigger
	Postgres 9.2 - add conditional constraint check
	Dropping column in Postgres on a large dataset
	pgAdmin shortcuts to execute scripts
	COPY function in PostgreSQL
	Exporting MYSQL data into Excel/CSV via php
	Perform this hours of operation query in PostgreSQL
	Optimize query with OFFSET on large table
	Any downsides of using data type “text” for storing strings?
	Update Redshift table from query
	CONSTRAINT to check values from a remotely related table (via join etc.)
	MySQL pivot table
	Find possible duplicates in two columns ignoring case and special characters
	Preventing adjacent/overlapping entries with EXCLUDE in PostgreSQL
	Full-text search in CouchDB
	How to execute PostgreSQL RAISE command dynamically
	plpgsql error “RETURN NEXT cannot have a parameter in function with OUT parameters” in table-returning function
	Login Failed with Existing User on PostgreSQL
	How to delete duplicate rows without unique identifier
	PostgreSQL: performance impact of extra columns
	ERROR: failed to find conversion function from unknown to text
	Time zone storage in PostgreSQL timestamps
	postgresql syntax check without running the query
	What's the difference between a tuple and a row in Postgres?
	How to check role of current PostgreSQL user from Qt application?
	Change column type from timestamp WITHOUT time zone to timestamp WITH time zone
	PostgreSQL store value returned by RETURNING
	Loop on tables with PL/pgSQL in Postgres 9.0+
	Slow PostgreSQL query in production - help me understand this explain analyze output
	Join a count query on a generate_series in postgres and also retrieve Null-values as “0”
	Can't delete database
	How to make PostgreSQL insert a row into a table when deleted from another table?
	How do I determine the last day of the previous month using PostgreSQL?
	SQLite query with multiple joins
	sum() vs. count()Feb 21 '13 at 12:24
postgrest id=g10337
	ref - bütünleştirici dok postgrest id=g10762
		ref - bütünleştirici dok postgrest <url:file:///~/projects/study/otl/cdb.otl#r=g10762>
		Rmd files
			~/projects/study/db/study_postgrest.Rmd <url:file:///~/projects/study/otl/cdb.otl#r=g10633>
			~/projects/study/db/tutorial_postgrest.Rmd <url:file:///~/projects/study/otl/cdb.otl#r=g10765>
			~/projects/itr/vrp_doc/study/study_vrp.Rmd <url:file:///~/gdrive/mynotes/content/code/cjs/cjs.md#r=g10766>
			~/projects/study/js/study_notes_cyclejs.Rmd <url:file:///~/gdrive/mynotes/content/code/cjs/cjs.md#r=g10767>
		bütün opsiyonlar: ~/projects/study/db/study_postgrest.Rmd
		Upload data to postgrest with cyclejs <url:file:///~/projects/study/db/study_postgrest.Rmd#r=g10758>
			Use R to upload data <url:file:///~/projects/study/db/study_postgrest.Rmd#r=g10759>
		~/projects/study/r/study_postgrestR.R
		<url:/Users/mertnuhoglu/projects/itr/vrp/r/pkg/vrpdata/R/vrp_api.R#tn=call_java = function>
		007.05 Upload big data <url:file:///~/projects/itr/vrp_doc/study/study_vrp.Rmd#r=g10757>
		007.01 Upload Data to R and Java Servers and Run Optimization <url:file:///~/projects/itr/vrp_doc/study/study_vrp.Rmd#r=g10753>
		bzq
			~/projects/bizqualify/BQ-data-run/datarun/make_api_ready_for_creditsafe_20181219.md
			~/projects/study/db/tutorial_postgrest.Rmd
		software:
			R + java servers:
				~/projects/itr/vrp/docker-compose3.yml
			postgrest server:
				~/projects/itr/vrp_psk01/docker-compose.yml
			our vrp data:
				~/projects/itr/vrp_psk01/db/src/sample_data/sample_data_vrp.sql
		options
			üç farklı tip için de hazırla bunları:
				opt01: make a query + upload bulk data
				opt02: curl, js, R
				opt03: default data + vrp data + bzq data
			konular
				wt_yuml2data
				wt_vrp
			doklar
				Edi
				Ecdb
				Ecvrp
			projeler
				/wt_vrp/dm 
				/vrp_psk01 
				from scratch 
		relationships between walkthrough projects
			ref: overview - vrp <url:file:///~/projects/itr/vrp_doc/doc_itr.md#r=g10581>
			study_psk:
				input: psk tutorial repo
			yuml2data01:
				input: yuml/datamodel_*.md
				output: yuml2data01/*.sql
			vrp_psk01:
				input: yuml2data01/*.sql
				output: psk database
				walkthrough_vrp reposunun bir örneği vrp_psk01
		walkthrough study_psk <url:file:///~/projects/study/otl/cdb.otl#r=g10522>
		walkthrough yuml2data01 <url:file:///~/projects/itr/vrp_doc/doc_itr.md#r=g10550>
		walkthrough vrp_psk01 <url:file:///~/projects/itr/vrp_doc/doc_itr.md#r=g10551>
	content pgr
		issues/errors
			error: sample_data update olmuyor
				ref
					no output inside psql: if no semi-colon is used <url:file:///~/projects/study/otl/cdb.otl#r=g10493>
				solution
					opt1
						tüm sql satırlarının sonuna noktalı virgül `;` koy
					opt2
						call reset.sql explicitly
							psql -f "db/src/sample_data/reset.sql"
			error: permission denied for relation clients
				export etmemişim
				export JWT_TOKEN=...
		points pgr
			bulk/batch data nasıl import edilir
				ref
					~/projects/study/db/study_postgrest.Rmd <url:file:///~/projects/study/otl/cdb.otl#r=g10633>
					Bulk Insert <url:file:///~/projects/study/otl/cdb.otl#r=g10630>
	notes pgr
		studies psk
			ref
				Tutorial - postgrest starter kit <url:file:///~/gdrive/mynotes/content/articles/articles_db.md#r=g10159>
			refcard psk id=g10492
				refcard psk <url:file:///~/projects/study/otl/cdb.otl#r=g10492>
				steps psk 01
					1. basic REST request
						$ curl http://localhost:8080/rest/todos?select=id
						[{"id":1},{"id":3},{"id":6}]
					2. authorized REST request
						$ export JWT_TOKEN=...
						$ curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/todos?select=id,todo
					3. Edit `sample_data/data.sql`:
						COPY client (id,name,address)
						1 Apple address_1_  
					4. Create `data/todo.sql`
						create table client ( id serial primary key,...
					5. Edit `data/schema.sql`: `\ir todo.sql` 
					6. Create `api/todo.sql` 
						create or replace view clients as...
					7. Edit `api/schema.sql`: `\ir todo.sql`
					7. Edit `reset.sql`
						truncate data.client restart identity cascade;
					8. Edit `authorization/privileges.sql` imported from `init.sql`
						grant select, insert, update, delete ...
					9. Make request to new API:
						$ curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/clients?select=id,name
				sql files psk 01
					sample_data/reset.sql 
						truncate plan ...
						sample_data/data.sql 
							COPY plan (id,name,...)
							1,plan_1,...
					init.sql
						data/schema.sql
							ddl.sql
								CREATE TABLE enum_var (...)
						api/schema.sql
							views.sql
								CREATE VIEW clients as ...
						authorization/privileges.sql
							grant select ... on api.clients
						sample_data/data.sql
							COPY plan (id,name, ...)
				sql files psk 01
					reset.sql 
						truncate plan ...
						sample_data/data.sql 
					init.sql
						data/schema.sql
							ddl.sql
						api/schema.sql
							views.sql
						authorization/privileges.sql
						sample_data/data.sql
			tutorial postgrest_starter_kit (psk) id=g10318
				postgrest_starter_kit (psk) <url:file:///~/projects/study/otl/cdb.otl#r=g10318>
				db/src/data/tables.sql
					create table client (...)
					create index client_user_id_index on client(user_id);
				db/src/data/schema.sql
					\ir tables.sql
				db/src/api/views_and_procedures.sql
					create view clients as
						select id, ... from data.client;
				db/src/api/schema.sql
					\ir views_and_procedures.sql
				db/src/sample_data/data.sql
					\echo # client
					COPY client (id,name,...) FROM STDIN (FREEZE ON); 
					1  Apple  address_1_   1   ...
					ALTER SEQUENCE client_id_seq RESTART WITH 4;
				db/src/sample_data/reset.sql
					\ir data.sql
				curl -H ".." http:.../clients?select=id,name
					# {"permission error"}
				db/src/authorization/privileges.sql
					grant select, insert, update, delete on api.clients, api.projects, ...  to webuser;
					grant usage on sequence data.client_id_seq to webuser;
				curl -H ".." http:.../clients?select=id,name
					# [{"id":1,...}] 
					# all rows
				db/src/api/views_and_procedures.sql
					alter view clients owner to api;
				db/src/authorization/privileges.sql
					create policy access_own_rows on client to api
					using ( request.user_role() = 'webuser' and request.user_id() = user_id )
					with check ( request.user_role() = 'webuser' and request.user_id() = user_id);
				curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/clients?select=id,name
					# [{"id":1,"name":"Apple"},{"id":2,"name":"Microsoft"}]
					# my own rows
				db/src/data/tables.sql
					user_id      int not null references "user"(id) default request.user_id(),
				create table client (
					check (length(name)>2 and length(name)<100),
					check (updated_on is null or updated_on > created_on)
				test:
					curl ... -d '{"name":"A"}'  
					# {..."message":"new row for relation \"client\" violates check constraint \"client_name_check\""}
				db/src/libs/util/mutation_comments_trigger.sql
					create function mutation_comments_trigger() returns trigger as $$
							elsif (tg_op = 'UPDATE') then
									if (new.parent_type = 'task') then
											update data.task_comment 
											set 
													body = coalesce(new.body, old.body)
											where id = old.id
											returning * into c;
											return (c.id, c.body);
				db/src/libs/util/schema.sql
					\ir mutation_comments_trigger.sql;
				db/src/init.sql
					\ir libs/util/schema.sql
				db/src/api/views_and_procedures.sql
					create trigger comments_mutation
						instead of insert or update or delete on comments
						for each row execute procedure util.mutation_comments_trigger();
				curl \
					-H "Content-Type: application/json" \
					-H "Accept: application/vnd.pgrst.object+json" \
					-d '{"email":"alice@email.com","password":"pass"}' \
					http://localhost:8080/rest/rpc/login
					# response:
						{
							"me":{"id":1,"name":"alice","email":"alice@email.com","role":"webuser"},
							"token":"xxxxxxxxxxxxx"
						}
				curl ... | python -mjson.tool
				curl ...  /projects
					--data-urlencode select="id,name,tasks(id,name)" \
					--du client_id="eq.1" \
					--du tasks.completed="eq.false"
				cloc --include-lang=SQL db/src/api/views_and_procedures.sql db/src/data/tables.sql db/src/authorization/privileges.sql db/src/libs/util/
			postgrest_starter_kit (psk): source code review id=g10319
				postgrest_starter_kit (psk): source code review <url:file:///~/projects/study/otl/cdb.otl#r=g10319>
				file structure
					.env
					docker-compose.yml
					openresty: reverse proxy and lua code
						lualib/user_code: application lua code
						nginx/
							conf
							html
						tests
							rest: rest interface tests
							common.js: helper functions
						Dockerfile
						entrypointsh: custom entrypoint
					postgrest
						tests: bash based integration tests
							all.sh assert.sh
					db: 
						src: schema definition
							data: definition of source tables
								schema.sql tables.sql
							api: api entities
								schema.sql views_and_procedures.sql
							libs:
								auth pgjwt rabbitmq request settings util
							authorization: roles and privileges
								privileges.sql roles.sql
							sample_data 
								data.sql reset.sql
							init.sql: entry point
						tests: pgTap tests
							rls.sql structure.sql
				db
					src
						init.sh
							~/codes/pg/khumbuicefall/db/src/init.sh
						init.sql
							~/codes/pg/khumbuicefall/db/src/init.sql
						api
				function call hierarchy
					\ir libs/auth/schema.sql
						\ir ../pgjwt/schema.sql
							pgjwt--0.0.1.sql
								FUNCTION url_encode
								FUNCTION url_decode
								FUNCTION algorithm_sign
								FUNCTION sign
								FUNCTION verify
						encrypt_pass
						sign_jwt
						get_jwt_payload
						set_auth_endpoints_privileges
					\ir libs/request/schema.sql
						create or replace function request.env_var(v text) returns text as $$
						create or replace function request.jwt_claim(c text) returns text as $$
						create or replace function request.cookie(c text) returns text as $$
						create or replace function request.header(h text) returns text as $$
						create or replace function request.user_id() returns int as $$
						create or replace function request.user_role() returns text as $$
					\ir libs/rabbitmq/schema.sql
						create or replace function rabbitmq.send_message(
						create or replace function rabbitmq.on_row_change() returns trigger as $$
					\ir libs/util/schema.sql
						\ir mutation_comments_trigger.sql;
							create or replace function mutation_comments_trigger() returns trigger as $$
					\ir data/schema.sql
						\ir ../libs/auth/data/user_role_type.sql
							create type user_role as enum ('webuser');
						\ir ../libs/auth/data/user.sql
							create table "user" (
							create trigger user_encrypt_pass_trigger
						\ir todo.sql
							create table todo (
							create trigger send_change_event
						\ir tables.sql
							create table client (
							create table project (
							create table task (
							create table project_comment (
							create table task_comment (
					\ir api/schema.sql
						\ir ../libs/auth/api/user_type.sql
							create type "user" as (id int, name text, email text, role text);
						\ir ../libs/auth/api/all.sql
							\ir session_type.sql
								create type session as (me json, token text);
							\ir login.sql
								create or replace function login(email text, password text) returns session as $$
							\ir refresh_token.sql
								create or replace function refresh_token() returns text as $$
							\ir signup.sql
								create or replace function signup(name text, email text, password text) returns session as $$
							\ir me.sql
								create or replace function me() returns "user" as $$
						\ir todos.sql
							create or replace view todos as
						\ir views_and_procedures.sql
							create or replace view clients as
							create or replace view projects as
							create or replace view tasks as
							create or replace view comments as
							create trigger comments_mutation
					\ir authorization/roles.sql
						create role :"authenticator" with login password :'authenticator_pass';
						create role :"anonymous";
						create or replace function _temp_create_application_roles("authenticator" text, "roles" text[]) returns void as $$
					\ir authorization/privileges.sql
						create policy todo_access_policy on data.todo to api 
						create policy access_own_rows on client to api
						create policy access_own_rows on project to api
						create policy access_own_rows on task to api
						create policy access_own_rows on project_comment to api
						create policy access_own_rows on task_comment to api
					\ir sample_data/data.sql
						COPY data.user (id,name,email,"password") FROM STDIN (FREEZE ON);
						COPY data.todo (id,todo,private,owner_id) FROM STDIN (FREEZE ON);
						COPY client (id,name,address,user_id,created_on,updated_on) FROM STDIN (FREEZE ON);
						COPY project (id,name,client_id,user_id,created_on,updated_on) FROM STDIN (FREEZE ON);
						COPY task (id,name,completed,project_id,user_id,created_on,updated_on) FROM STDIN (FREEZE ON);
						COPY project_comment (id,body,project_id,user_id,created_on,updated_on) FROM STDIN (FREEZE ON);
						COPY task_comment (id,body,task_id,user_id,created_on,updated_on) FROM STDIN (FREEZE ON);
				function signatures
					type session as (me json, token text)
					login(email, password): session
					refresh_token(): text token
					signup(name text, email text, password text): session
					me(): user
			study_postgrest_starter_kit.Rmd: SQL functions: array_to_json array_agg row_to_json id=g10340
				study_postgrest_starter_kit.Rmd: SQL functions: array_to_json array_agg row_to_json <url:file:///~/projects/study/otl/cdb.otl#r=g10340>
				ref
					<url:/Users/mertnuhoglu/projects/study/db/study_postgrest_starter_kit.Rmd#tn=In order to understand this code, I simplified it to its core:>
				step 0: base
					SELECT  "api"."todos"."id", "api"."todos"."todo" FROM  "api"."todos"
						id |  todo  
						 1 | item_1
						 3 | item_3
						 6 | item_6
					row_to_json(pg_source)
						# convert each row to json
						WITH pg_source AS (SELECT  "api"."todos"."id", "api"."todos"."todo" FROM  "api"."todos") SELECT row_to_json(pg_source) AS body FROM pg_source
							body           
							{"id":1,"todo":"item_1"}
							{"id":3,"todo":"item_3"}
							{"id":6,"todo":"item_6"}
					array_agg(row_to_json(pg_source))
						# aggregate result set into an array
						WITH pg_source AS (SELECT  "api"."todos"."id", "api"."todos"."todo" FROM  "api"."todos") SELECT array_agg(row_to_json(pg_source)) AS body FROM pg_source
							body                                                 
							{"{\"id\":1,\"todo\":\"item_1\"}","{\"id\":3,\"todo\":\"item_3\"}","{\"id\":6,\"todo\":\"item_6\"}"}
					array_to_json(array_agg(row_to_json(pg_source)))
						# convert array to json
						WITH pg_source AS (SELECT  "api"."todos"."id", "api"."todos"."todo" FROM  "api"."todos") SELECT array_to_json(array_agg(row_to_json(pg_source))) AS body FROM pg_source
							body
							[{"id":1,"todo":"item_1"},{"id":3,"todo":"item_3"},{"id":6,"todo":"item_6"}]
		tests psk id=g10494
			tests psk <url:file:///~/projects/study/otl/cdb.otl#r=g10494>
			ref
				Tutorial PSK <url:file:///~/projects/study/otl/cdb.otl#r=g10159>
				move git tag to another commit <url:file:///~/gdrive/mynotes/content/code/ccode.md#r=g10518>
				<url:file:///~/projects/itr/vrp_doc/study/db_vrp.Rmd>
			psk admin id=g10513
				psk admin <url:file:///~/projects/study/otl/cdb.otl#r=g10513>
				docker exec -it khumbuicefall_db_1 bash
				accessing postgresql logs - psk id=g10544
					accessing postgresql logs - psk <url:file:///~/projects/study/otl/cdb.otl#r=g10544>
					ref
						database logs - postgresql <url:file:///~/projects/study/otl/cdb.otl#r=g10515>
						<url:file:///~/codes/pg/postgrest-docs/admin.rst>
					opt1: manually
						psql
							show data_directory;
						cd /var/lib/postgresql/data
						cat postgresql.conf
					opt2: docker-compose.yml
						db:
							volumes:
								- "./postgresql/data:/var/lib/postgresql/data"
						cat postgresql/data/postgresql.conf
			walkthrough study_psk id=g10522
				walkthrough study_psk <url:file:///~/projects/study/otl/cdb.otl#r=g10522>
				ref
					https://docs.subzero.cloud/installation/
					https://github.com/mertnuhoglu/study_psk 
					/Users/mertnuhoglu/codes/pg/study_psk01
				navigating steps
					git checkout test05
					fshow
					fcoc
					git checkout step01
					git checkout step02
					git checkout step03
					git checkout step04
					git checkout step05
				step 0: new project
					docker pull subzerocloud/subzero-cli-tools
					npm install -g subzero-cli
					subzero --version
					subzero base-project
					docker-compose up -d
					subzero dashboard
					curl http://localhost:8080/rest/todos?select=id,todo
						[{"id":1,"todo":"item_1"},
						 {"id":3,"todo":"item_3"},
						 {"id":6,"todo":"item_6"}]
				step 01: authorized request and update sample_data
					edit
						.env
					export JWT_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoxLCJyb2xlIjoid2VidXNlciJ9.uSsS2cukBlM6QXe4Y0H90fsdkJSGcle9b7p_kMV1Ymk
					curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/todos?select=id,todo
						[{"id":1,"todo":"item_1"},{"id":2,"todo":"item_2"},{"id":3,"todo":"item_3"},{"id":6,"todo":"item_6"}]
					edit  
						f:$STUDY_PSK01/db/src/sample_data/data.sql
					# wait till subzero dashboard reloads sql files
					curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/todos?select=id,todo
					[{"id":1,"todo":"updated"},...
				step 02: update data schema
					edit
						f:$STUDY_PSK01/db/src/data/tables.sql
						f:$STUDY_PSK01/db/src/data/schema.sql
					vim
						:DB g:prod select * from api.clients
							id | name | address | user_id | created_on | updated_on 
							----+------+---------+---------+------------+------------
							(0 rows)
				step 03: update api schema
					edit
						f:$STUDY_PSK01/db/src/api/views_and_procedures.sql
						f:$STUDY_PSK01/db/src/api/schema.sql
					vim
						DB g:prod select * from api.clients
							id | name | address | created_on | updated_on 
							----+------+---------+------------+------------
							(0 rows)
				step 04: update sample data for new tables
					edit  
						f:$STUDY_PSK01/db/src/sample_data/data.sql
						f:$STUDY_PSK01/db/src/sample_data/reset.sql
					vim
						DB g:prod select * from api.clients
						# 3 rows
						DB g:prod select * from data.client
						# 3 rows
					curl
						curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/clients\?select\=id,name
						# permission denied
				step 05: access rights to api entities
					edit
						f:$STUDY_PSK01/db/src/authorization/privileges.sql
					curl
						curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/clients\?select\=id,name
						# [{"id":1,"name":"Apple"}, {"id":2,"name":"Microsoft"}, {"id":3,"name":"Amazon"}]
				step 06: row level security
					edit
						f:$STUDY_PSK01/db/src/api/views_and_procedures.sql
						f:$STUDY_PSK01/db/src/authorization/privileges.sql
						f:$STUDY_PSK01/db/src/data/tables.sql
					curl
						curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/clients?select=id,name
						# [{"id":1,"name":"Apple"},{"id":2,"name":"Microsoft"}]
				step 07: authentication
					curl
						curl \
							-H "Content-Type: application/json" \
							-H "Accept: application/vnd.pgrst.object+json" \
							-d '{"email":"alice@email.com","password":"pass"}' \
							http://localhost:8080/rest/rpc/login
						# { "me":{"id":1,"name":"alice","email":"alice@email.com","role":"webuser"} }
				step 08: input validation
					edit
						f:$STUDY_PSK01/db/src/data/tables.sql
						f:$STUDY_PSK01/db/src/authorization/privileges.sql
					curl
						curl -H "Authorization: Bearer $JWT_TOKEN" -H "Content-Type: application/json" -H 'Prefer: return=representation' -d '{"name":"A"}'  http://localhost:8080/rest/clients
						# {"hint":null,"details":null,"code":"23514","message":"new row for relation \"client\" violates check constraint \"client_name_check\""}
						curl -H "Authorization: Bearer $JWT_TOKEN" -H "Content-Type: application/json" -H 'Prefer: return=representation' -d '{"name":"Uber"}' http://localhost:8080/rest/clients 
						# [{"id":5,"name":"Uber","address":null,"created_on":"2018-09-15T08:41:44.497925+00:00","updated_on":null}]
				step 09: mutations on complex views
					edit
						f:$STUDY_PSK01/db/src/libs/util/schema.sql
						f:$STUDY_PSK01/db/src/libs/util/mutation_comments_trigger.sql
						f:$STUDY_PSK01/db/src/init.sql
						f:$STUDY_PSK01/db/src/api/views_and_procedures.sql
					curl: insert
						curl -s -X POST \
							-H "Authorization: Bearer $JWT_TOKEN" \
							-H "Content-Type: application/json" \
							-H "Accept: application/vnd.pgrst.object+json" \
							-H 'Prefer: return=representation' \
							-d '{"body": "Hi there!","parent_type": "task","task_id":1}'  \
							http://localhost:8080/rest/comments | \
							python -mjson.tool
						# { "id":3, "body":"Hi there!", "parent_type":"task", "parent_id":1, "project_id":null, "task_id":1, "created_on":"2017-08-29T02:04:29.35094+00:00", "updated_on":null }
					curl: update
						curl -s -X PATCH \
							-H "Authorization: Bearer $JWT_TOKEN" \
							-H "Content-Type: application/json" \
							-H "Accept: application/vnd.pgrst.object+json" \
							-H 'Prefer: return=representation' \
							-d '{"body":"This is going to be awesome!"}'  \
							"http://localhost:8080/rest/comments?id=eq.1&parent_type=eq.project" | \
							python -mjson.tool
						# { "id":1, "body":"This is going to be awesome!", "parent_type":"project", "parent_id":1, "project_id":1, "task_id":null, "created_on":"2017-07-18T11:31:12+00:00", "updated_on":null }
				step 10: progress
					curl
						curl -s -G \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/clients \
							--data-urlencode select="id,name,projects(id,name)" | \
							python -mjson.tool
					response
						[
								{
										"id": 1,
										"name": "Apple",
										"projects": [
												{
														"id": 1,
														"name": "MacOS"
												},
												{
														"id": 3,
														"name": "IOS"
												}
										]
								},
								{
										"id": 2,
										"name": "Microsoft",
										"projects": [
												{
														"id": 2,
														"name": "Windows"
												}
										]
								}
						]
					curl
						curl -s -G \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/clients \
							--data-urlencode select="id,name,projects(id,name,comments(body))" | \
							python -mjson.tool
					response
						{
								"message": "Could not find foreign keys between these entities, No relation found between projects and comments"
						}
					curl
						curl -s -G \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/clients \
							--data-urlencode select="id,name,projects(id,name,tasks(id,name))" | \
							python -mjson.tool
					response
						[
								{
										"id": 1,
										"name": "Apple",
										"projects": [
												{
														"id": 1,
														"name": "MacOS",
														"tasks": [
																{
																		"id": 1,
																		"name": "Design a nice UI"
																},
																{
																		"id": 2,
																		"name": "Write some OS code"
																}
														]
												},
												{
														"id": 3,
														"name": "IOS",
														"tasks": [
																{
																		"id": 4,
																		"name": "Get everybody to love it"
																}
														]
												}
										]
								},
								{
										"id": 2,
										"name": "Microsoft",
										"projects": [
												{
														"id": 2,
														"name": "Windows",
														"tasks": [
																{
																		"id": 3,
																		"name": "Start aggressive marketing"
																}
														]
												}
										]
								}
						]
					curl
						curl -s -G \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/tasks \
							--data-urlencode select="id,name" | \
							python -mjson.tool
					curl
						curl -s -G \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/tasks \
							--data-urlencode select="id,name,comments(id,body)" | \
							python -mjson.tool
					response
						no foreign key between tasks and comments
					curl
						curl -s -G \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/tasks \
							--data-urlencode select="id,name,task_comments(id,body)" | \
							python -mjson.tool
					response
						"message": "permission denied for relation task_comments"
				step 11: permission for task_comments
					edit
						f:$STUDY_PSK01/db/src/api/views_and_procedures.sql
						f:$STUDY_PSK01/db/src/authorization/privileges.sql
					curl
						curl -s -G \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/task_comments \
							--data-urlencode select="id,body" | \
							python -mjson.tool
					curl
						curl -s -G \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/tasks \
							--data-urlencode select="id,name,task_comments(id,body)" | \
							python -mjson.tool
				step 12: rest
					curl
						curl -s -G -X GET \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/projects \
							--data-urlencode select="id,name,tasks(id,name)" \
							--data-urlencode client_id="eq.1" \
							--data-urlencode tasks.completed="eq.false"  | \
							python -mjson.tool
					curl
						curl -s -X POST \
							-H "Authorization: Bearer $JWT_TOKEN" \
							-H "Content-Type: application/json" \
							-H "Accept: application/vnd.pgrst.object+json" \
							-H 'Prefer: return=representation' \
							-d '{"name":"Google","address":"Mountain View, California, United States"}'  \
							http://localhost:8080/rest/clients?select=id,created_on | \
							python -mjson.tool
					curl
						curl -s -X PATCH \
							-H "Authorization: Bearer $JWT_TOKEN" \
							-H "Content-Type: application/json" \
							-H "Accept: application/vnd.pgrst.object+json" \
							-H 'Prefer: return=representation' \
							-d '{"name":"Updated name"}'  \
							"http://localhost:8080/rest/projects?select=id,name,created_on,updated_on&id=eq.1" | \
							python -mjson.tool
					response
						{
							"id": 1,
							"name": "Updated name",
							"created_on": "2017-07-18T11:31:12+00:00",
							"updated_on": null
						}
					note: updated_on is still null
				step 13: fix updated_on
					edit
						f:$STUDY_PSK01/db/src/data/tables.sql
					curl
						curl -s -X PATCH \
							-H "Authorization: Bearer $JWT_TOKEN" \
							-H "Content-Type: application/json" \
							-H "Accept: application/vnd.pgrst.object+json" \
							-H 'Prefer: return=representation' \
							-d '{"name":"Updated name"}'  \
							"http://localhost:8080/rest/projects?select=id,name,created_on,updated_on&id=eq.1" | \
							python -mjson.tool
					response
						{
							"id": 1,
							"name": "Updated name",
							"created_on": "2017-07-18T11:31:12+00:00",
							"updated_on": "2018-09-15T10:19:23.277106+00:00"
						}
				step 14: delete
					curl
						curl -s -X DELETE \
							-H "Authorization: Bearer $JWT_TOKEN" \
							-H 'Prefer: return=representation' \
							"http://localhost:8080/rest/tasks?select=id,created_on&id=eq.2" | \
							python -mjson.tool
	postgrest official manual 
		postgrest official manual id=g10317
			postgrest official manual <url:file:///~/projects/study/otl/cdb.otl#r=g10317>
			manual v5.0
				http://postgrest.org/en/v5.0/intro.html
				intro
					ref
						<url:file:///~/codes/pg/postgrest-docs/intro.rst>
					motivation 
						alternative to manual CRUD programming
						business logic: duplicates, ignores or hobbles database structure
						orm is leaky abstraction
							leads to slow imperative code
						goal: single declarative source of truth = data itself
						declarative programming
						leak-proof abstraction
							database admin can create an API
						embracing the relational model
							codd criticized hierarchical model
							hierarchical databases are similar to nested http routes
							postgrest: flexible filtering, embedding rather than nested routes
						one thing well
							pgs has focused scope
						shared improvements
					Ecosystem
						client-side libraries
							js admin-on-rest
							js: generating query URLs
							js superagent
							js client
							typescript: ui generator on angular
							angular: editing data
						external notification
							expose web sockets
							rabbitmq
							amazon kinesis
						example apps
							postgrest starter kit
							handsontable + postgrest
				tutorial 0
					ref
						<url:file:///~/codes/pg/postgrest-docs/tutorials/tut0.rst>
					create database
						sudo docker exec -it tutorial psql -U postgres
						psql:
							create schema api;
							create table api.todos (
								id serial primary key,
								done boolean not null default false,
								task text not null,
								due timestamptz
							);
							insert into api.todos (task) values
								('finish tutorial 0'), ('pat self on back');
							create role web_anon nologin;
							grant web_anon to postgres;
							grant usage on schema api to web_anon;
							grant select on api.todos to web_anon;
					run postgrest
						tutorial.conf
							db-uri = "postgres://postgres:mysecretpassword@localhost/postgres"
							db-schema = "api"
							db-anon-role = "web_anon"
						$ ./postgrest tutorial.conf
						$ curl http://localhost:3000/todos
							[
								{
									"id": 1,
									"done": false,
									"task": "finish tutorial 0",
									"due": null
								},
								{
									"id": 2,
									"done": false,
									"task": "pat self on back",
									"due": null
								}
							]
						insert
							$ curl http://localhost:3000/todos -X POST \
									 -H "Content-Type: application/json" \
									 -d '{"task": "do bad thing"}'
							error: permission denied
				tutorial 1
					ref
						<url:file:///~/codes/pg/postgrest-docs/tutorials/tut1.rst>
					add a trusted user
						psql:
							create role todo_user nologin;
							grant todo_user to postgres;
							grant usage on schema api to todo_user;
							grant all on api.todos to todo_user;
							grant usage, select on sequence api.todos_id_seq to todo_user;
					make a secret
						$ export LC_CTYPE=C
						$ < /dev/urandom tr -dc A-Za-z0-9 | head -c32
						tutorial.conf:
							jwt-secret = "<the password you made>"
					sign a token
						jwt.io:
							create a token 
					make a request - insert
						$ export TOKEN=<token>
						$ curl http://localhost:3000/todos -X POST \
								 -H "Authorization: Bearer $TOKEN"   \
								 -H "Content-Type: application/json" \
								 -d '{"task": "learn how to auth"}'
					update request 
						$ curl http://localhost:3000/todos -X PATCH \
								 -H "Authorization: Bearer $TOKEN"    \
								 -H "Content-Type: application/json"  \
								 -d '{"done": true}'
				api
					ref
						<url:file:///~/codes/pg/postgrest-docs/api.rst>
					horizontal filtering (rows)
						ref
							<url:/Users/mertnuhoglu/codes/pg/postgrest-docs/api.rst#tn=Horizontal Filtering (Rows)>
						ex
							GET /people?age=lt.13 HTTP/1.1
							GET /people?age=gte.18&student=is.true HTTP/1.1
							GET /people?or=(age.gte.14,age.lte.18) HTTP/1.1
							GET /people?and=(grade.gte.90,student.is.true,or(age.gte.14,age.is.null)) HTTP/1.1
						operators
							eq neq
							gt gte lt lte
							like ilike
							in
								?a=in.(1,2,3)
								?a=in.("hi","yes")
							is
							fts full-text search @@
							cs contains @>
							cd contained in <@
							ov overlap &&
					vertical filtering (columns)
						ref
							<url:/Users/mertnuhoglu/codes/pg/postgrest-docs/api.rst#tn=Vertical Filtering (Columns)>
						ex
							GET /people?select=first_name,age HTTP/1.1
							GET /people?select=fullName:full_name,birthDate:birth_date HTTP/1.1
								rename columns new_name:db_column
							GET /people?select=full_name,salary::text HTTP/1.1
								casting columns ::
							GET /people?select=id,json_data->>blood_type,json_data->phones HTTP/1.1
								json path -> ->>
							GET /people?select=id,json_data->phones->0->>number HTTP/1.1
					computed columns
						ex
							CREATE FUNCTION full_name(people) RETURNS text AS $$
								SELECT $1.fname || ' ' || $1.lname;
							$$ LANGUAGE SQL;
							GET /people?full_name=fts.Beckett HTTP/1.1
							GET /people?select=*,full_name HTTP/1.1
					ordering
						ex
							GET /people?order=age.desc,height.asc HTTP/1.1
							GET /people?order=age HTTP/1.1
							GET /people?order=age.nullsfirst HTTP/1.1
							GET /people?order=age.desc.nullslast HTTP/1.1
					limits and pagination
						ex: opt1: use HTTP headers
							response
								HTTP/1.1 200 OK
								Range-Unit: items
								Content-Range: 0-14/*
							request
								GET /people HTTP/1.1
								Range-Unit: items
								Range: 0-19
							response
								HTTP/1.1 200 OK
								Range-Unit: items
								Content-Range: 0-17/*
						ex: opt2: use query parameters
							GET /people?limit=15&offset=30 HTTP/1.1
					Response Format
						ex
							GET /people HTTP/1.1
							Accept: application/json
						other options:
							\*/\*
							text/csv
							application/json
							application/openapi+json
							application/octet-stream
					Singular or Plural
						by default: returns json array
						ex: return json object
							GET /items?id=eq.1 HTTP/1.1
							Accept: application/vnd.pgrst.object+json
					Resource Embedding - pgr
						ref 
							<url:/Users/mertnuhoglu/codes/pg/postgrest-docs/api.rst#tn=Resource Embedding>
						ex
							GET /films?select=title HTTP/1.1
							GET /films?select=title,directors(id,last_name) HTTP/1.1
						when fk changes in database schema
							refresh pgr schema cache
					Embedded Operations
						ref
							<url:/Users/mertnuhoglu/codes/pg/postgrest-docs/api.rst#tn=Embedded Operations>
						ex
							GET /films?select=*,actors(*)&actors.order=last_name,first_name HTTP/1.1
							GET /films?select=*,roles(*)&roles.character=in.(Chico,Harpo,Groucho) HTTP/1.1
					Stored Procedures
						ex
							POST /rpc/function_name HTTP/1.1
							CREATE FUNCTION add_them(a integer, b integer)
							RETURNS integer AS $$
							 SELECT $1 + $2;
							$$ LANGUAGE SQL IMMUTABLE STRICT;
							POST /rpc/add_them HTTP/1.1
								{ "a": 1, "b": 2 }
							GET /rpc/add_them?a=1&b=2 HTTP/1.1
					Insertions / Updates
						Bulk Insert id=g10630
							Bulk Insert <url:file:///~/projects/study/otl/cdb.otl#r=g10630>
							http://postgrest.org/en/v5.0/api.html#bulk-insert
							opt01: json
								POST /people HTTP/1.1
								Content-Type: application/json
								[
									{ "name": "J Doe", "age": 62, "height": 70 },
									{ "name": "Janus", "age": 10, "height": 55 }
								]
							opt02: csv
								POST /people HTTP/1.1
								Content-Type: text/csv
								name,age,height
								J Doe,62,70
								Jonas,10,55
					Explicit Qualification
				admin
					ref
						<url:file:///~/codes/pg/postgrest-docs/admin.rst>
					debugging
						ref
							<url:/Users/mertnuhoglu/codes/pg/postgrest-docs/admin.rst#tn=Debugging>
						server version
						http requests
							sniff incoming HTTP requests
								sudo ngrep -d lo0 port 3000
						database logs - postgresql id=g10515
							database logs - postgresql <url:file:///~/projects/study/otl/cdb.otl#r=g10515>
							where are db logs?
								psql:
									show data_directory;
								postgresql.conf:
									# note: always use '' never ""
									log_destination = 'stderr'
									logging_collector = on
									log_directory = 'pg_log'		# directory where log files are written,
									log_filename = 'postgresql-%Y-%m-%d.log'
									log_statement = 'all'
								restart then
						schema reloading
							opt1: manual SIGHUP signal
								$ killall -HUP postgrest
							opt2: automatic 
								step01: db trigger
									CREATE OR REPLACE FUNCTION public.notify_ddl_postgrest()
										RETURNS event_trigger
									 LANGUAGE plpgsql
										AS $$
									BEGIN
										NOTIFY ddl_command_end;
									END;
									$$;
									CREATE EVENT TRIGGER ddl_postgrest ON ddl_command_end
										 EXECUTE PROCEDURE public.notify_ddl_postgrest();
								step02: send SIGHUP when that event occurs
									$ pg_listen <db-uri> ddl_command_end "killall -HUP postgrest"
			manual v4.3
				https://postgrest.com/en/v4.3/intro.html
				steps01
					sudo docker run --name tutorial -p 5432:5432 -e POSTGRES_PASSWORD=mysecretpassword -d postgres
				steps
					sudo docker run --name tutorial -p 5432:5432 ... -d postgres
					CREATE SCHEMA api;
						CREATE TABLE api.todos (..)
						INSERT INTO api.todos (task) VALUES ..
					CREATE ROLE web_anon NOLOGIN;
						GRANT web_anon TO postgres;
						GRANT usage ON SCHEMA api TO web_anon;
						GRANT select ON api.todos TO web_anon;
					tutorial.conf
						db-uri = "postgres://postgres:mysecretpassword@localhost/postgres"
						db-schema = "api"
						db-anon-role = "web_anon"
					postgrest tutorial.conf
					curl http://localhost:3000/todos
					curl http://localhost:3000/todos -X POST ..  -d '{"task": "do bad thing"}'
						# error
					CREATE ROLE todo_user NOLOGIN;
						GRANT ALL ON api.todos TO todo_user;
						GRANT USAGE, SELECT ON SEQUENCE api.todos_id_seq TO todo_user;
					tutorial.conf
						jwt-secret = "<secret>"
					jwt.io:
						{ "role": "todo_user" }
					export TOKEN="..."
					curl http://localhost:3000/todos -X POST ..  
						-H "Authorization: Bearer $TOKEN"   \
				admin
					nginx.conf
						...
						upstream postgrest {
							server localhost:3000;
						}
						server {
							location /api/ {
								proxy_pass http://postgrest/;
								add_header Content-Location /api/$upstream_http_content_location;
								...
					DELETE /logs
						sudo -E pgxn install safeupdate
						postgresql.conf:
							shared_preload_libraries='safeupdate';
					sudo ngrep -d lo0 port 3000
					postgresql.conf
						log_destination = 'stderr'
						log_directory = 'pg_log'
						log_statement = 'all'
				API
					GET /entity?param1=op.value&...
						GET /people HTTP/1.1
						verbs: OPTIONS, GET, POST, PATCH, DELETE
						GET /people?age=gte.18&student=is.true
						GET /people?and=(grade.gte.90,student.is.true,or(age.gte.14,age.is.null))
						?a=in."hi there","yes"
						?tags=cs.{example, new}
					GET /view
						CREATE VIEW fresh_stories AS
							SELECT *
								FROM stories
							 WHERE pinned = true
									OR published > now() - interval '1 day'
							ORDER BY pinned DESC, published DESC;
							-->
							GET /fresh_stories HTTP/1.1
					GET /entity?function /entity?order
						GET /people?select=*,full_name
						CREATE FUNCTION full_name(people) RETURNS text AS $$
							SELECT $1.fname || ' ' || $1.lname;
						CREATE INDEX people_full_name_idx ON people
							USING GIN (to_tsvector('english', full_name(people)));
							-->
							GET /people?full_name=fts.Beckett
						GET /people?order=age.desc,height.asc
					Range pagination
						GET /people HTTP/1.1
							Range-Unit: items
							Range: 0-19
						GET /people?limit=15&offset=30
					single item ?id=eq.1 Accept: vnd.pgrst
						/items?id=eq.1
							[ {"id":1} ]
						/items?id=eq.1
							Accept: application/vnd.pgrst.object+json
							{"id":1}
					joins items?select=id,subitems(id,field)
						curl http://localhost:8080/rest/items?id=gt.1&select=id,name,subitems(id,name)
						/films?select=title,directors(id,last_name)
						GET /films?select=*,actors(*)&actors.order=last_name,first_name
					POST /rpc/function {args}
						POST /rpc/function_name
						CREATE FUNCTION add_them(a integer, b integer)
							-->
							POST /rpc/add_them
								{"a":1, "b":2}
					request.header.XYZ
						SELECT current_setting('request.header.origin', true);
					insert = POST, update = PATCH, delete = DELETE
						POST /table_name 
							{"col1": "value", "col2": "value"}
						PATCH /people?age=lt.13
							{"category": "child"}
						POST /people
							Content-Type: text/csv
							name,age
							J Doe,62 
							Jonas,10
						POST /people
							Content-Type: application/json
							[
								{"name":"J Doe", "age":62},
								..
							]
				Authentication
					GRANT user123 TO authenticator;
		Postgrest Manual id=g10158
			Postgrest Manual <url:file:///~/projects/study/otl/cdb.otl#r=g10158>
				https://postgrest.com/en/v4.3/intro.html
			Introduction
				Motivation
					turns db into REST API
						constraints and permissions in db determine API endpoints and operations
					alternative to CRUD programming
						solves logic duplication problem:
							business logic duplicates, ignores db structure
						orm is leaky abstraction and slow
					single declarative source of truth: data itself
				Declarative Programming
					easier to ask psql
						to join data
						to loop through rows
						to manage permissions
						to set constraints
				Leak-proof Abstraction
					no ORM
					creating new views happens in SQL
					db admin can create API
				Embracing the Relational Model
					1970 Codd: criticized hierachical model of db
						similarity bw hierarchical db and nested http routes
					prest: uses flexible filtering and embedding rather than nested routes
				One Thing Well
				Shared Improvements
				Ecosystem
					postgrestR
						https://github.com/clesiemo3/postgrestR
			Tutorials
				Tutorial 0: Get it Running id=g10154
					ref
						~/projects/study/pg/postgrest01/
					intro
						pgr turns db into API
						endpoints and permissions come from db objects (tables, views ...)
					run postgresql
						sudo docker run --name tutorial -p 5432:5432 \
							-e POSTGRES_PASSWORD=mysecretpassword \
							-d postgres
						volume to persist (optional)
							docker volume create --name=tutorialdb
							sudo docker run --name tutorial -p 5432:5432 \
								-e POSTGRES_PASSWORD=mysecretpassword \
								-v tutorialdb:/var/lib/postgresql/data \
								-d postgres
					install postgrest
						wget https://github.com/begriffs/postgrest/releases/download/v0.4.3.0/postgrest-v0.4.3.0-osx.tar.xz
						tar xfJ postgrest.tar.xz
						mv postgrest /usr/local/bin
						postgrest
					create database for api
						docker exec -it tutorial psql -U postgres
						psql commands
							CREATE SCHEMA api;
							CREATE TABLE api.todos (
								id SERIAL PRIMARY KEY,
								done BOOLEAN NOT NULL DEFAULT FALSE,
								task TEXT NOT NULL,
								due TIMESTAMPTZ
							);
							INSERT INTO api.todos (task) VALUES
								('finish tutorial 0'), ('pat self on back');
						create role for web requests
							CREATE ROLE web_anon NOLOGIN;
							GRANT web_anon TO postgres;
							GRANT usage ON SCHEMA api TO web_anon;
							GRANT select ON api.todos TO web_anon;
						web_anon can access 
							api SCHEMA
							api.todos TABLE
					run postgrest
						create file: tutorial.conf
							db-uri = "postgres://postgres:mysecretpassword@localhost/postgres"
							db-schema = "api"
							db-anon-role = "web_anon"
						cd ~/projects/study/pg/postgrest01/
						postgrest tutorial.conf
							❯ postgrest tutorial.conf
							Listening on port 3000
						test
							curl http://localhost:3000/todos
							[{"id":1,"done":false,"task":"finish tutorial 0","due":null},{"id":2,"done":false,"task":"pat self on back","due":null}]
						but we cannot add new record
							curl http://localhost:3000/todos -X POST \
								-H "Content-Type: application/json" \
								-d '{"task": "do bad thing"}'
				Tutorial 1: The Golden Key
					step 1: add a trusted user
						previously: web_anon role
							executes anonymous web requests
						new role: todo_user for authenticated users
							psql
								CREATE ROLE todo_user NOLOGIN;
								GRANT todo_user TO postgres;
								GRANT USAGE ON SCHEMA api TO todo_user;
								GRANT ALL ON api.todos TO todo_user;
								GRANT USAGE, SELECT ON SEQUENCE api.todos_id_seq TO todo_user;
					step 2: make a secret
						create a secret password
							openssl rand -base64 32
						error: {"message":"JWSError JWSInvalidSignature"}
							use openssl to create secret password for jwt
						touch tutorial1.conf. add:
							jwt-secret = "<secret>"
							jwt-secret = "T8k0JBUifFC6UevcbjpJi1Jc7mJfCVg3eEfoQi7IpuI"
						run pgr
							postgrest tutorial1.conf
					step 3: sign a token
						go to jwt.io and make a token
							payload:
								{ "role": "todo_user" }
							jwt:
								eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoidG9kb191c2VyIn0.nZ1wYdWinlOGkzh_FE6CP0cJo4W8IWETc6LVtnEO2P0
							note: token is signed but not encrypted. it is easy to see inside
					step 4: make a request
						http header will contain authentication token
						ex
							export TOKEN="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoidG9kb191c2VyIn0.nZ1wYdWinlOGkzh_FE6CP0cJo4W8IWETc6LVtnEO2P0"
							curl http://localhost:3000/todos -X POST \
								-H "Authorization: Bearer $TOKEN"   \
								-H "Content-Type: application/json" \
								-d '{"task": "learn how to auth"}'
					step 5: add expiration
						currently jwt token is valid endlessly
						expiration timestamp: use "exp" claim
							postgrest treats two jtw claims specially:
								role: database role
								exp: expiration timestamp in "unix epoch time"
									number of seconds since 1970.01.01
						find epoch value of five minutes from now. in psql:
							SELECT extract(epoch from now() + '5 minutes'::interval) :: integer;
								1505825517
							payload of jwt
								"exp": <epoch>
							token
								eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoidG9kb191c2VyIiwiZXhwIjoxNTA1ODI1NTE3fQ.Tf-tihwQzyrFK40Gqn_YKTlyDjxTP_b5rvxZh805bKw
								export NEW_TOKEN=".."
							request
								curl http://localhost:3000/todos \
									-H "Authorization: Bearer $NEW_TOKEN"
					bonus: immediate revocation
						payload:
							"email": "disgruntled@mycompany.com"
						token
							eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoidG9kb191c2VyIiwiZW1haWwiOiJkaXNncnVudGxlZEBteWNvbXBhbnkuY29tIn0.4t6agb-sv1PCG0zWehWFkhnDPpy77mkgqCXDsOGnkcQ
							export WAYWARD_TOKEN=".."
						psql: stored procedure
							CREATE SCHEMA auth;
							GRANT USAGE ON SCHEMA auth TO web_anon, todo_user;
							CREATE OR REPLACE FUNCTION auth.check_token() RETURNS void
								LANGUAGE PLPGSQL
								AS $$
							BEGIN
								IF current_setting('request.jwt.claim.email', true) =
									 'disgruntled@mycompany.com' THEN
									raise insufficient_privilege
										using hint = 'Nope, we are on to you';
								END IF;
							END
							$$;
						touch tutorial2.conf: add
							pre-request = "auth.check_token"
						postgrest tutorial2.conf
						request
							# this request still works
							curl http://localhost:3000/todos \
									 -H "Authorization: Bearer $TOKEN"
							# this one is rejected
							curl http://localhost:3000/todos \
									 -H "Authorization: Bearer $WAYWARD_TOKEN"
			Installation
				Docker
					docker-compose.yml
						# docker-compose.yml
						server:
							image: postgrest/postgrest
							ports:
								- "3000:3000"
							links:
								- db:db
							environment:
								PGRST_DB_URI: postgres://app_user:password@db:5432/app_db
								PGRST_DB_SCHEMA: public
								PGRST_DB_ANON_ROLE: app_user
						db:
							image: postgres
							ports:
								- "5432:5432"
							environment:
								POSTGRES_DB: app_db
								POSTGRES_USER: app_user
								POSTGRES_PASSWORD: password
			Administration
				Hardening PostgREST
					proxy requests to PostgREST server
						nginx.conf
							http {
								...
								# upstream
								upstream postgrest {
									server localhost:3000;
									keepalive 64; 
								}
								...
								server {
									...
									# expose to outside
									location /api/ {
										proxy_pass http://postgrest/;
										add_header Content-Location /api/$upstream_http_content_location;
										...
									}
									..
					Block Full-Table Operations
						prevent accidental DELETE
							DELETE /logs
						use: pg-safeupdate extension from PGXN network
							sudo -E pgxn install safeupdate
							# add to postgresql.conf:
								shared_preload_libraries='safeupdate';
						use RLS (row level security)
							ref
								5.7. Row Security Policies <url:#r=adb_002>
					Count-Header DoS
					HTTPS
					Rate Limiting
				Debugging
					Server Version
						"Server" http response header
					HTTP Requests
						full information about client requests and SQL commands
							# watch network
							sudo ngrep -d lo0 port 3000
					Database Logs
						postgresql.conf
							where is it
								show data_directory; 
							log_destination = 'stderr'
							log_directory = 'pg_log'
							log_statement = 'all'
					Schema Reloading
				Alternate URL Structure
					singular or plural object response
			API id=g10439
				API <url:file:///~/projects/study/otl/cdb.otl#r=g10439>
				Tables and Views id=g10440
					Tables and Views <url:file:///~/projects/study/otl/cdb.otl#r=g10440>
					intro
						ex
							GET /people HTTP/1.1
						no nested routes
							such as: /films/1/director
							instead: resource embedding
								handles 1-n and n-n relationships
						verbs: OPTIONS, GET, POST, PATCH, DELETE
					horizontal filtering (rows) id=g10441
						horizontal filtering (rows) <url:file:///~/projects/study/otl/cdb.otl#r=g10441>
						people aged under 13
							GET /people?age=lt.13
						multiple parameters
							GET /people?age=gte.18&student=is.true
						complex logic
							GET /people?and=(grade.gte.90,student.is.true,or(age.gte.14,age.is.null))
						operators
							eq gt gte lt lte neq 
							like iilike 
								use * in place of %
							in 
								?a=in.1,2,3
								?a=in."hi there","yes"
							is
								null, true, false
							fts
								full text search
								@@
							cs
								contains
								?tags=cs.{example, new}
								@>
							cd
								contained in
								?values=cd.{1,2,3}
								<@
							ov
								overlap
								?period=ov.[2017-01-01,2017-06-30]
								&&
							sl
								strictly left of
								?range=sl.(1,10)
								<<
							sr
							nxr
								does not extend to the right
								?range=nxr.(1,10)
								&<
							nxl
							adj
								is adjacent to
								?range=adj.(1,10)
								-|-
							not
								negates another operator
								?a=not.eq.2
								?not.and=(a.get.0,a.lte.100)
							for more complicated filters
								create a new view or stored procedure
								ex
									CREATE VIEW fresh_stories AS
									SELECT *
										FROM stories
									 WHERE pinned = true
											OR published > now() - interval '1 day'
									ORDER BY pinned DESC, published DESC;
									-->
									GET /fresh_stories HTTP/1.1
					vertical filtering (columns) id=g10442
						vertical filtering (columns) <url:file:///~/projects/study/otl/cdb.otl#r=g10442>
						GET /people?select=fname,age
						computed columns
							computed columns are not in output by default
								to include them
									GET /people?select=*,full_name
							ex
								CREATE TABLE people (
									fname text,
									lname text
								);
								CREATE FUNCTION full_name(people) RETURNS text AS $$
									SELECT $1.fname || ' ' || $1.lname;
								$$ LANGUAGE SQL;
								-- (optional) add an index to speed up anticipated query
								CREATE INDEX people_full_name_idx ON people
									USING GIN (to_tsvector('english', full_name(people)));
								-->
								GET /people?full_name=fts.Beckett
					ordering
						GET /people?order=age.desc,height.asc
						GET /people?order=age
					limits and pagination id=g10443
						limits and pagination <url:file:///~/projects/study/otl/cdb.otl#r=g10443>
						postgrest uses http range headers
							to describe size of results
						ex: response
							HTTP/1.1 200 OK
							Range-Unit: items
							Content-Range: 0-14/*
						opt1: specify range in request:
							ex: request
								GET /people HTTP/1.1
								Range-Unit: items
								Range: 0-19
							ex: response
								HTTP/1.1 200 OK
								Range-Unit: items
								Content-Range: 0-14/*
							no limit:
								Range: 10-
						opt2: specify with query params:
							ex
								GET /people?limit=15&offset=30
							useful for embedded resources
						get total count
							ex: request header
								Prefer: count=exact
							ex: response header
								Content-Range: 0-24/434902
					response format id=g10444
						response format <url:file:///~/projects/study/otl/cdb.otl#r=g10444>
						same API endpoint can respond in different formats
						use: "Accept"
						ex
							GET /people
							Accept: application/json
						possibilities
							*/*
							text/csv 
							application/json
							application/openapi+json
							application/octet-stream
					singular or plural
						by default returns json in an array
							/items?id=eq.1
							returns:
								[
									{"id":1}
								]
						to return first result as an object without array:
							/items?id=eq.1
							Accept: application/vnd.pgrst.object+json
							returns:
								{"id":1}
					binary output
						ex
							GET /items?select=bin_data&id=eq.1
							Accept: application/octet-stream
				unicode
					use percent encoding
					ex
						http://localhost:3000/%D9%85...
				resource embedding id=g10445
					resource embedding <url:file:///~/projects/study/otl/cdb.otl#r=g10445>
					allows related sources to be included in single api call
					server uses fk to determine which tables are returned
					ex:
						Actors 1-n Roles n-1 Films n-1 Directors
						Films 1-n Nominations n-1 Competitions
					ex: titles of all fimls
						/films?select=title
					ex: include directors names
						/films?select=title,directors(id,last_name)
						returns:
							[
								{ "title": "..",
									"directors": {
										"id": 2,
										"last_name": ".."
									}
								},
								...
							]
					must specify: pk of related (embedded) table
					ex: table name alias
						/films?select=title,director:directors(id,last_name)
					ex: films of all directors
						/directors?select=films(title,year)
					embedded filters and order
						ex: order actors in each film
							GET /films?select=*,actors(*)&actors.order=last_name,first_name
				stored procedures id=g10446
					stored procedures <url:file:///~/projects/study/otl/cdb.otl#r=g10446>
					accessible under /rpc
						POST /rpc/function_name
					named arguments: supplied as json
						ex:
							CREATE FUNCTION add_them(a integer, b integer)
							...
							-->
							POST /rpc/add_them
							{"a":1, "b":2}
					accessing request headers/cookies
						ex:
							request.header.XYZ
							SELECT current_setting('request.header.origin', true);
					raising errors
				insertions/updates id=g10447
					insertions/updates <url:file:///~/projects/study/otl/cdb.otl#r=g10447>
					ex
						POST /table_name 
						{"col1": "value", "col2": "value"}
					response: includes Location header
						to find the new object
					note: when inserting, you must post a not quoted JSON
						{"col1": "value"}
						note: no single quotes '{}' around
							'{"col1": "value"}'
					update: PATCH
						PATCH /people?age=lt.13
						{"category": "child"}
					prevent full table updates:
						Block full table operations
					bulk insert
						provide json array or csv
							csv is much faster
						post with: Content-Type: text/csv
						ex: csv
							POST /people
							Content-Type: text/csv
							name,age
							J Doe,62 
							Jonas,10
						empty field: ,,
							coerced to empty string
						NULL is mapped to sql null
						ex: json
							POST /people
							Content-Type: application/json
							[
								{"name":"J Doe", "age":62},
								..
							]
				deletions
					DELETE /user?active=is.false
				openapi support
					use: Swagger UI
				http error codes
			Authentication
				Overview of role System
					intro
						authenticate: verify that a user is who he says he is
						authorize: user has permissions to do db operations
					Authentication Sequence id=g10448
						Authentication Sequence <url:file:///~/projects/study/otl/cdb.otl#r=g10448>
						three types of roles
							authenticator
								switches to actual user roles
							anonymous
							user
						db admin creates them
						ex
							jwt:
								{"role": "user123"}
							switch to db role:
								SET LOCAL Role user123;
							note: db admin must allow authenticator role to switch into this user
								GRANT user123 TO authenticator;
					Users and Groups
						concept of roles:
							either: user, group of users
						roles for each web user
							role is available to SQL through: current_user variable
					Custom Validation
			Points
				Object Identifier Types (OID)
					used internally as pk for system tables
					alias types for oid: regproc, regclass, regtype, regrole ...
		official repo
			https://github.com/begriffs/postgrest
			intro
				Using PostgREST is an alternative to manual CRUD programming. Custom API servers suffer problems. Writing business logic often duplicates, ignores or hobbles database structure. Object-relational mapping is a leaky abstraction leading to slow imperative code. 
				The PostgREST philosophy establishes a single declarative source of truth: the data itself.
	postgrest starter kit
		Postgrest Starter Kit Manual id=adb_003
			Postgrest Starter Kit Manual <url:~/projects/study/otl/cdb.otl#r=adb_003>
			https://github.com/subzerocloud/postgrest-starter-kit/wiki
			intro
				postgrest is a solid candidate for production
				features
					items
						docker environment
						standard directory structure
						debugging and live code
						authentication
						unit tests
						reverse proxy configuration
							to add custom logic at any step of http request
						CI scripts
					any production project needs these
					but this is not to learn postgrest and how it works
			Architecture and Project Structure
				why multiple languages
					look at frontend:
						css, html, sass, js, npm, webpack, rest, graphql etc.
				use nginx as integration point
					use it for routing requests based on runtime logic
				use postgresql
					for more than dumb data store
					complicated data questions: as view, procedure
					rules for who can access data
				use rabbitmq
					email: trigger on signup
				define your API do not write them
			Project Structure
				docker-compose.yml
				.env
				openresty: reverse proxy and lua code
					lualib/user_code: application lua code
					nginx/
						conf
						html
					tests
						rest: rest interface tests
						common.js: helper functions
					Dockerfile
					entrypointsh: custom entrypoint
				postgrest
					tests: bash based integration tests
				db: 
					src: schema definition
						data: definition of source tables
						api: api entities
						libs:
						authorization: roles and privileges
						sample_data
						init.sql: entry point
					tests: pgTap tests
			HTTP Request Flow
				/Users/mertnuhoglu/gdrive/public/img/ss-225.png
				OpenResty
					OpenResty = nginx + lua
					routes requests
					you can alter shape of request
				PostgREST
				PostgreSQL
				RabbitMQ
					lua hooks: run code synchronously
					rabbit: run code async
						ex: email, log event, live updates
					from db, you can generate an event
						then route it to consumers (clients)
						then do live updates
			PostgREST Crash Course
				get one item
					curl http://localhost:8080rest/items/1
				get a filtered list
					curl http://localhost:8080rest/items?id=gt.1
				specify columns
					curl http://localhost:8080rest/items?id=gt.1&select=id,name
				embed related entities
					curl http://localhost:8080rest/items?id=gt.1&select=id,name,subitems(id,name)
				apply filters to embedded items
					curl http://localhost:8080rest/items?id=gt.1&select=id,name,subitems(id,name)&subitems.name=like.%subitem%
				insert one item and return its id
					curl -s -X POST \
					-H 'Prefer: return=representation' \
					-d '{"name":"New Item"}'  \
					http://localhost:8080/rest/items?select=id
				update: PATCH
				delete: DELETE
			PostgreSQL Concepts
				this tool will let you become proficient in postgresql
					we tricked you into being a DBA
					90% of the work will be in db
				Tables
				Constraints
				Triggers
					server programming
				Roles, Grants, and Row Level Security
					application users defined as database users
					ref
						Application users vs. Row Level Security <url:#r=adb_004>
				Views
				Stored Procedures
			Authentication Authorization Flow
				Json Web Tokens JWT
			Iterative Development Workflow id=g10449
				Iterative Development Workflow <url:file:///~/projects/study/otl/cdb.otl#r=g10449>
				Overview
					big problem: logic moves from a file (stored in git) to environment (database)
					subzero-cli solves this problem
					also does this for nginx config as well
				Install
				Workflow
					step01
						docker-compose -d up db postgrest openresty
						subzero dashboard
					step02
						curl http://localhost:8080/rest/todos?select=id 09:26:01odo
					step03: change todos view
						#before
						select id, todo, private, (owner_id = request.user_id()) as mine from data.todo;
						#after
						select ('#' || id::text) as id, ('do this: ' || todo) as todo, private, (owner_id = request.user_id()) as mine from data.todo;
			Production Infrastructure
				Overview
				Familiarize yourself with ECS concepts
				Install AWS CLI
				ECS Cluster
					save cluster name
						export CLUSTER_NAME=mycluster
					get cluster's cloudformation stack name
						aws cloudformation list-stacks --output table --query 'StackSummaries[*].[StackName,TemplateDescription]'
					save stack name to env
						export STACK_NAME=EC2ContainerService-mycluster
					extract stack configuration info
					save cluster region in env
						export AWS_REGION=`echo $Cluster_Param_VpcAvailabilityZones | cut -d',' -f1 | head --bytes -2`
						echo $AWS_REGION
				SSL Certificate
					# create a certificate in AWS Certificate Manager
					aws acm list-certificates
					# save arn
					export CERTIFICATE_ARN="arn:aws:acm:us-east-1:CHANGE-WITH-YOURS:certificate/CHANGE-WITH-YOURS"
				Loadbalancer
				Image Repository
					store docker image in EC2 Container Registry
				Database (RDS)
		Tutorial PSK id=g10159
			Tutorial PSK <url:file:///~/projects/study/otl/cdb.otl#r=g10159>
			Tutorial PSK (postgrest starter kit) v01
				ref
					~/codes/pg/example-api/
				Overview
					What We Are Building
						app: todo app for project management
						tables: users, clients, projects, tasks, comments
						production ready app
							a few hours of work
							eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoxLCJyb2xlIjoid2VidXNlciJ9.vAN3uJSleb2Yj8RVPRsb1UBkokqmKlfl6lJ2bg3JfFg
				API Core id=g10450
					API Core <url:file:///~/projects/study/otl/cdb.otl#r=g10450>
					Schemas  id=g10451
						Schemas  <url:file:///~/projects/study/otl/cdb.otl#r=g10451>
						Data Schema
							schema: data
							sql file: db/src/data/tables.sql
								create table client (...)
								create index client_user_id_index on client(user_id);
								create table project (...)
								create index project_user_(d_index on project(user_id);
								create index project_client_id_index on project(client_id);
								create table task (...)
								create index task_user_id_index ...
								create index task_project_id_index ...
								create table project_comment (...)
								...
								create table task_comment (...)
							"user_id" in each table
								it will help to enforce access rights for each row
									instead of complicated joins
							two tables for comments
								not good
								but will show how to decouple API from tables
							sql file: db/src/data/schema.sql
								...
								-- import application models
								\ir todo.sql
								\ir tables.sql
						Api Schema
							schema: api
								used for exposing REST
								contains only views and functions
							sql file: db/src/api/views_and_procedures.sql
								create view clients as
									select id, ... from data.client;
								create view projects as
									select id, ... from data.project;
								...
							sql file: db/src/api/schema.sql
								-- ...
								-- our endpoints
								\ir todos.sql
								\ir views_and_procedures.sql
							note:
								we expose only fields and tables we want
						Sample Data
							tool: datafiller to generate data
							sql file: db/src/sample_data/data.sql
								...
								set search_path = data,public;
								\echo # client
								COPY client (id,name,...) FROM STDIN (FREEZE ON); 
								1  Apple  address_1_   1   ...
								2  ...
								...
								ALTER SEQUENCE client_id_seq RESTART WITH 4;
								...
								ANALYZE client;
								...
							sql file: db/src/sample_data/reset.sql
								BEGIN;
								\set QUIET on
								...
								truncate todo restart identity cascade;
								...
								\ir data.sql
								COMMIT;
							test it
								curl -H ".." http:.../clients?select=id,name
								# {"permission error"}
					Securing your API id=g10452
						Securing your API <url:file:///~/projects/study/otl/cdb.otl#r=g10452>
						Access rights to API entities
							goal: give authenticated API users the ability to access endpoints
								ie. grant them rights to views in api schema
							sql file: db/src/authorization/privileges.sql
								-- ...
								grant select, insert, update, delete
								on api.clients, api.projects, ...
								to webuser;
							test it
								curl -H ".." http:.../clients?select=id,name
								# [{"id":1,...}]
							issue: any user can modify existing data and see all of it
						The magic "webusers" role and mystery $JWT token
							"webuser" role is created in roles.sql
						Row Level Security
							code
								views_and_procedures.sql
									# change owners of views to "api" role
									...
									alter view clients owner to api;
								privileges.sql
									" restrict rows a user can access
									...
									grant select, ... on client to api;
									create policy access_own_rows on client to api
									using (request.user_role() = 'webuser' and request.user_id() = user_id )
									with check (request.user_role() = 'webuser' and request.user_id() = user_id);
									...
							test it: returns only my rows
								curl -H ".." http:.../clients?select=id,name
								# [{"id":1,...}]
							when data inserted:
								give a default value for "user_id"
									user_id      int not null references "user"(id) default request.user_id(),
						Authentication
							how do users log in?
								functions come into play
							we use functions from "auth" lib
							ex:
								curl -H .. -d '{"email":..}' http://../rpc/login
						Input Validation
							ex
								create table client (
									..
									check (length(name)>2 and length(name)<100),
									check (updated_on is null or updated_on > created_on)
						Mutations on complex views
							ex
								create or replace function mutation_comments_trigger() returns trigger as $$
								...
										elsif (tg_op = 'UPDATE') then
												if (new.parent_type = 'task' or old.parent_type = 'task') then
														update data.task_comment 
														set 
																body = coalesce(new.body, old.body),
																task_id = coalesce(new.task_id, old.task_id)
														where id = old.id
														returning * into c;
														if not found then return null; end if;
														return (c.id, c.body, 'task'::text, c.task_id, null::int, c.task_id, c.created_on, c.updated_on);
												elsif (new.parent_type = 'project' or old.parent_type = 'project') then
						Progress
							ex:
								curl ...
								http://localhost:8080/rest/clients \
								--data-urlencode select="id,name,projects(id,name,comments(body),tasks(id,name,comments(body)))" | \
								python -mjson.tool
				Using the API id=g10453
					Using the API <url:file:///~/projects/study/otl/cdb.otl#r=g10453>
					REST
						ex: get projects of a client and all active tasks
							curl ...
							/projects
							--data-urlencode select="id,name,tasks(id,name)" \
							--du client_id="eq.1" \
							--du tasks.completed="eq.false"
						ex: add a client and return its new id 
							curl ...
							-d '{"name":"Google",..}'
							/clients?select=id,created_on
						ex: update projects name
							-d {"name":"updated name"}
							/projects?select=id,name&id=eq.1
				Beyond Data Access
					Overview
						we are used to a specific design of backend:
							dumb webserver and dumb database
							we assume: single place for logic = simple design
								this is not true
						this tool: holistic approach
							leverage all features of underlying components
						think of these components
							not as separate processes
							but as modules in your code
					Restricting filtering capabilities
						ex: prevent filtering by a column that is not part of an index
							hooks.lua
								local function check_filters()
								end
					Reacting to database events 
						%90 of what an API does is CRUD
						how it works
							whenever an event occurs
							you send a message to Message queue (events table)
								insert a row
							another system (a script) reads rows
								performs additional tasks
			Tutorial steps - postgrest starter kit v02
				https://github.com/subzerocloud/postgrest-starter-kit/wiki/Tutorial-Overview
				steps only
					installation
						git clone --single-branch https://github.com/subzerocloud/postgrest-starter-kit khumbuicefall
						cd khumbuicefall
					edit .env
						COMPOSE_PROJECT_NAME=khumbuicefall
					run and test
						$ docker-compose up -d # wait for 5-10s before running the next command
						$ curl http://localhost:8080/rest/todos?select=id
						[{"id":1},{"id":3},{"id":6}]
					install and run subzero dashboard
						npm install -g subzero-cli
						subzero dashboard
					check subzero
						$ curl http://localhost:8080/rest/todos?select=id
					make authorized request
						$ export JWT_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoxLCJyb2xlIjoid2VidXNlciJ9.uSsS2cukBlM6QXe4Y0H90fsdkJSGcle9b7p_kMV1Ymk
						$ curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/todos?select=id,todo
						[{"id":1,"todo":"item_1"},{"id":3,"todo":"item_3"},{"id":6,"todo":"item_6"}]
					edit db/src/sample_data/data.sql
						1 item_1  FALSE 1
						->
						1 item_1_updated  FALSE 1
					run request
						$ curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/todos?select=id,todo
						[{"id":1,"todo":"item_1_updated"},{"id":3,"todo":"item_3"},{"id":6,"todo":"item_6"}]
				generated sql 
					$ curl http://localhost:8080/rest/todos?select=id
					$ curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/todos?select=id,todo
				cd khumbuicefall
					.env
					docker-compose.yml
						- "postgresstore:/var/lib/postgresql/data"
						volumes:
							postgresstore:
					docker-compose up -d 
					curl http://localhost:8080/rest/todos?select=id
					# install subzero-cli
					npm install -g subzero-cli
					subzero dashboard
				connect db
					credentials - datagrip intellij
						DB_NAME=app
						DB_SCHEMA=api
						SUPER_USER=superuser
						SUPER_USER_PASSWORD=superuserpass
					psql
						docker exec -it postgreststarterkit_db_1 bash
						docker exec -it postgreststarterkit_db_1 psql -d app -h localhost -p 5432 -U superuser
						psql -d app -h localhost -p 5432 -U superuser
					DataGrip
						connection > Schemase
							app > api
					pgAdmin
						connection > Databases > app > Schemas > api
					alchemysql
						%%sql postgresql://user:passwd@localhost/db
				pretty json
					curl -s ... | python -mjson.tool
				authenticated call
					export JWT_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoxLCJyb2xlIjoid2VidXNlciJ9.uSsS2cukBlM6QXe4Y0H90fsdkJSGcle9b7p_kMV1Ymk
					curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/todos?select=id,todo
				~/codes/pg/khumbuicefall/db/src/sample_data/data.sql
					change: item_1 to updated
				Schemas
					Data Schema
						add new tables to schema "data"
							db/src/data/tables.sql: 
								create table client (
									id           serial primary key,
									...
							db/src/data/schema.sql:
								\ir tables.sql
					Api Schema
						add new views to schema "api"
							db/src/api/views_and_procedures.sql
								create or replace view clients as
								select id, name, address, created_on, updated_on from data.client;
								...
							db/src/api/schema.sql
								\ir views_and_procedures.sql
						# so we decoupled implementation details (underlying source tables) from API we expose using views
					Sample Data
						db/src/sample_data/data.sql
							COPY client (id,name,address,user_id,created_on,updated_on) FROM STDIN (FREEZE ON);
							1 Apple address_1_  1 2017-07-18 11:31:12 \N
						db/src/sample_data/reset.sql
						test
							curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/clients?select=id,name
							# permission denied
								we need to state who can access what resource
				Securing Your API
					Access rights to API entities
						db/src/authorization/privileges.sql
							grant select, insert, update, delete 
							on api.clients, api.projects, api.tasks, api.comments
							to webuser;
						test
							curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/clients?select=id,name
							# response correct
					The magic "webusers" role and the mystery $JWT token
						"webuser" role created in roles.sql
							db/src/authorization/roles.sql
							differentiate between logged-in users "webuser" and rest "anonymous"
					Row Level Security
						limit access of users to their rows only
							db/src/api/views_and_procedures.sql
								alter view clients owner to api;
								...
							db/src/authorization/privileges.sql
								create policy access_own_rows on client to api
								using ( request.user_role() = 'webuser' and request.user_id() = user_id )
								with check ( request.user_role() = 'webuser' and request.user_id() = user_id);
								...
							test
								curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/clients?select=id,name
								# [{"id":1,"name":"Apple"},{"id":2,"name":"Microsoft"}]
							default value for user_id:
								user_id      int not null references "user"(id),
								-->
								user_id      int not null references "user"(id) default request.user_id(),
					Authentication
						how do users signup and log in?
							stored procedures defined in "api" schema
							use functions when sth cannot be expressed using a single query
						login:
							curl \
							-H "Content-Type: application/json" \
							-H "Accept: application/vnd.pgrst.object+json" \
							-d '{"email":"alice@email.com","password":"pass"}' \
							http://localhost:8080/rest/rpc/login
						response:
							{
								"me":{"id":1,"name":"alice","email":"alice@email.com","role":"webuser"},
								"token":"xxxxxxxxxxxxx"
							}
					Input Validation
						check if rules are applied:
							curl \
							-H "Authorization: Bearer $JWT_TOKEN" \
							-H "Content-Type: application/json" \
							-H 'Prefer: return=representation' \
							-d '{"name":"A"}'  \
							http://localhost:8080/rest/clients
							# error: permission denied for sequence client_id_seq
						db/src/authorization/privileges.sql
							grant usage on sequence data.client_id_seq to webuser;
							...
						test:
							curl ... http://localhost:8080/rest/clients
							# new row
						add constraints
							db/src/data/tables.sql
								check (length(name)>2 and length(name)<100),
								check (updated_on is null or updated_on > created_on)
						test:
							curl ... http://localhost:8080/rest/clients
							# {"hint":null,"details":null,"code":"23514","message":"new row for relation \"client\" violates check constraint \"client_name_check\""}
						test2:
							curl ... -d '{"name":"Uber"}'  
					Mutations on complex views
						db/src/libs/util/schema.sql
							drop schema if exists util cascade;
							create schema util;
							set search_path = util, public;
						db/src/libs/util/mutation_comments_trigger.sql
							create or replace function mutation_comments_trigger() returns trigger as $$
								..
								elsif (tg_op = 'UPDATE') then
										if (new.parent_type = 'task' or old.parent_type = 'task') then
												update data.task_comment 
												set 
														body = coalesce(new.body, old.body),
														task_id = coalesce(new.task_id, old.task_id)
												where id = old.id
												returning * into c;
												if not found then return null; end if;
												return (c.id, c.body, 'task'::text, c.task_id, null::int, c.task_id, c.created_on, c.updated_on);
						db/src/libs/util/schema.sql
							\ir mutation_comments_trigger.sql;
						db/src/init.sql
							-- add this line just below the place where rabbitmq lib is included
							\ir libs/util/schema.sql
						db/src/api/views_and_procedures.sql
							create trigger comments_mutation
							instead of insert or update or delete on comments
							for each row execute procedure util.mutation_comments_trigger();
						test
							curl -s -X POST \
							-H "Authorization: Bearer $JWT_TOKEN" \
							-H "Content-Type: application/json" \
							-H "Accept: application/vnd.pgrst.object+json" \
							-H 'Prefer: return=representation' \
							-d '{"body": "Hi there!","parent_type": "task","task_id":1}'  \
							http://localhost:8080/rest/comments| \
							python -mjson.tool
						response:
							{
								"id": 3,
								"body": "Hi there!",
								..
					Progress
						get comments
							curl -s -G \
							-H "Authorization: Bearer $JWT_TOKEN" \
							http://localhost:8080/rest/clients \
							--data-urlencode select="id,name,projects(id,name,comments(body))" | \
							python -mjson.tool
						how much code did we write 
							cloc --include-lang=SQL db/src/api/views_and_procedures.sql db/src/data/tables.sql db/src/authorization/privileges.sql db/src/libs/util/
				next
	articles
		Stop calling PostgREST “MAGIC”! id=g10338
			Stop calling PostgREST “MAGIC”!  <url:file:///~/projects/study/otl/cdb.otl#r=g10338>
			https://medium.freecodecamp.org/stop-calling-postgrest-magic-8f3e1d5e5dd1
			Whats in a URL
				ex
					GET /items/1
					SELECT * FROM items WHERE id=1
				postgrest :: Schema -> HTTP -> SQL
				ex
					GET /items?select=id,name&id=gt.10&order=name
					SELECT id, name FROM items WHERE id > 10 ORDER BY name
			The three magic ingredients
				JSON encoding
					ex: postgrest generates:
						WITH essence AS (
							SELECT id, name FROM items WHERE id > 10 ORDER BY name
						)
						SELECT 
							coalesce(
								array_to_json(array_agg(row_to_json(response))),
								'[]'
							)::character varying AS BODY
						FROM (SELECT * FROM essence) response
				Authentication / Authorization - postgrest id=g10339
					Authentication / Authorization - postgrest <url:file:///~/projects/study/otl/cdb.otl#r=g10339>
					pgrest connects using a role "authenticator"
						the role has no privileges aside from login
					when an authenticated request comes in, and there is an Authorization header that contains a JWT token, pgrest decodes the token, verify it is valid, and look at it payload for field "role"
						say role has "alice"
						ie. this request will be executed with privileges of "alice"
							to do this, pgs switches the current user
								SET ROLE alice
						ex:
							BEGIN;
							SET LOCAL role TO 'alice'; 
							-- query
							COMMIT;
						for this, you say that "authenticator" has the right to assume "alice" role:
							GRANT alice TO authenticator;
					benefits:   
						no imperative code to check authorization
						no latency due to this
					pgs has full picture:
						who is issuing the query (role)
						his privileges (grants)
						his restrictions (RLS)
					query planner can do much better job
						get faster over time
				Resource Embedding
					pgrest can do basic CRUD and more
					trick: &select= parameter
					ex
						GET /items?select=id,name,subitems(id,name)
					similar to GraphQL
						replace () with {}
					how does this work?
						at boot time:
							pgrest runs a bunch of queries
								to understand 
									what entities live
									relations between them
										based on fks
							when you say "subitems(...)
								it knows table "items" is related to "subitems" through fk "item_id"
								thus it can generate join query
					ex:
						SELECT items.id, items.name,
							COALESCE(
								( 
									SELECT array_to_json(array_agg(row_to_json(subitems)))
									FROM (
										SELECT subitems.id, subitems.name
										FROM subitems
										WHERE subitems.item_id = items.id
									) subitems
								),
								'[]'
							) AS subitems
						FROM items
			But why (do we need PostgREST)? id=g10341
				But why (do we need PostgREST)?  <url:file:///~/projects/study/otl/cdb.otl#r=g10341>
				Do one thing well
				You'll no longer write APIs, you'll be defining and configuring them
			Beyond REST
		HN: PostgREST – A fully RESTful API from any existing PostgreSQL database  id=g10342
			HN: PostgREST – A fully RESTful API from any existing PostgreSQL database  <url:file:///~/projects/study/otl/cdb.otl#r=g10342>
			https://news.ycombinator.com/item?id=13959156
			q: how to write complex queries
				to define richer endpoints you create views in the database and stored procedures. It can follow foreign key and m2m through a select url param
				select=id,foreignkey(name)
			q: big picture
				not PostgREST alone that accomplishes this "big claim" of eliminating the need for custom APIs. 
				It's the combination of using openresty(nginx)/postgrest/postgres/rabbitmq together that gives you the possibility of "defining" apis rather then "manually coding" apis.
			q: how to do CRUD with a little extra
				ex: "create this object and kick off a Stripe payment"
				trigger external actions by psql pubsub
					https://postgrest.com/en/v0.4/intro.html#external-notification
			q: Odd choice to push JSON serialization onto the DB while touting horizontal scaling
				ans: this is the only way to extract tree like data from db
					very small burden on db
						still 99% never outgrow a single db
					traversing trees in postgres: "WITH RECURSIVE"
						starting from this point in the tree, recursively return all of its children
					protocol can onyl represent 2D array
			q: comparison to ORM
				shifts the work of writing a basic CRUD API (a task for which you would probably use an ORM) to declaring a SQL schema
		Swagger id=g10343
			Swagger <url:file:///~/projects/study/otl/cdb.otl#r=g10343>
			1. run swagger-ui
				docker run -p 80:8080 swaggerapi/swagger-ui
			2. open swagger
				http://localhost
			3. swagger > explore > 
				localhost:3000
					enter url of postgrest api uri
Tools id=g10175
	sample data id=g10454
		sample data <url:file:///~/projects/study/otl/cdb.otl#r=g10454>
		ddlgenerator: generate ddl from data id=g10455
			ddlgenerator: generate ddl from data <url:file:///~/projects/study/otl/cdb.otl#r=g10455>
			https://github.com/catherinedevlin/ddl-generator
			install
				pip install ddlgenerator
			ex: from file
				$ ddlgenerator postgresql mydata.yaml > mytable.sql
			ex: csv
				ddlgenerator postgresql sample_data01.csv > sample_data01.sql
				# sample_data01.csv
					product_id,title,added
					1,laptop x101,2017-02-03
					2,nvidia p80,2016-05-04
				# sample_data01.sql
					CREATE TABLE sample_data01 (
						product_id INTEGER NOT NULL, 
						title VARCHAR(11) NOT NULL, 
						added TIMESTAMP WITHOUT TIME ZONE NOT NULL
					);
			ex: python data
				$ ddlgenerator -i postgresql '[{"Name": "Alfred", "species": "wart hog", "kg": 22}]'
				DROP TABLE generated_table;
				CREATE TABLE generated_table (
								name VARCHAR(6) NOT NULL,
								kg INTEGER NOT NULL,
								species VARCHAR(8) NOT NULL
				)
				;
				INSERT INTO generated_table (kg, Name, species) VALUES (22, 'Alfred', 'wart hog');
			formats: csv, yaml, json
			options
				-t --text: TEXT instead of VARCHAR
				-k --key KEY: primary key field
				-i --inserts: include INSERT
			alternatives
				csvsql
				prequel
				dplyr
		csvsql: generate ddl/insert from csv data
			https://csvkit.readthedocs.io/en/0.7.3/scripts/csvsql.html
			ex: generate ddl
				csvsql -i postgresql data.csv
			ex: create table and import data into db
				csvsql --db postgresql:///test --table fy09 --insert data.csv
		datafiller: sample data generator id=g10456
			datafiller: sample data generator <url:file:///~/projects/study/otl/cdb.otl#r=g10456>
			https://www.cri.ensmp.fr/people/coelho/datafiller.html
			Tutorial
				ref
					<url:file:///~/projects/study/pg/datafiller01/run_datafiller_01.sh>
					<url:file:///~/projects/study/pg/datafiller01/library.sql>
					<url:file:///~/projects/study/pg/datafiller01/library_test_data.sql>
				relative or absolute size of relations
					ex: 100 Book per Reader. 1.5 Borrowed Book (on average) per Reader.
						CREATE TABLE Reader( 
						...
						CREATE TABLE Book( -- df: mult=100.0
						...
						CREATE TABLE Borrow( --df: mult=1.5
					default multiplier: 1.0
					run:
						datafiller --size=100 library.sql > library_test_data.sql
					out:
						\echo # filling table book (10000)
						COPY book (bid,title,isbn) FROM STDIN (ENCODING 'utf-8');
						1 title_3587  9787040495126
						2 title_752 9786895831394
						...
						COPY reader (rid,firstname,lastname,born,gender,phone) FROM STDIN (ENCODING 'utf-8');
						1 firstnam  lastname_6  2017-09-20  FALSE phone_20_20_20_
						2 firstname_  lastname_80_80_ 2017-08-08  FALSE phone_49
						...
						COPY borrow (borrowed,rid,bid) FROM STDIN (ENCODING 'utf-8');
						2017-11-07 15:07:20 7 1
						2017-11-07 17:27:20 14  2
				Directives in Comments
					ex:
						-- this directive sets the default overall size
							-- df: size=10
						-- this directive defines a macro named "fn"
							-- df fn: word=/path/to/file-containing-words
						-- this directive applies to table "Foo"
						CREATE TABLE Foo( -- df: mult=10.0
							-- this directive applies to attribute "fid"
							fid SERIAL -- df: offset=1000
							-- use defined macro, choose "stuff" from the list of words
						, stuff TEXT NOT NULL -- df: use=fn
						);
						-- ... much later
						-- this directive applies to attribute "fid" in table "Foo"
						-- df T=Foo A=fid: null=0.8
				A Simple Library Example
					ex: library.sql
						CREATE TABLE Book( 
							bid SERIAL PRIMARY KEY,
							title TEXT NOT NULL, 
							isbn ISBN13 NOT NULL 
						);
						CREATE TABLE Reader( 
							rid SERIAL PRIMARY KEY,
							firstname TEXT NOT NULL, 
							lastname TEXT NOT NULL, 
							born DATE NOT NULL, 
							gender BOOLEAN NOT NULL, 
							phone TEXT 
						);
						CREATE TABLE Borrow( 
							borrowed TIMESTAMP NOT NULL, 
							rid INTEGER NOT NULL REFERENCES Reader,
							bid INTEGER NOT NULL REFERENCES Book, 
							PRIMARY KEY(bid) -- a book is borrowed once at a time!
						);
					options:
						CREATE TABLE Book( -- df: mult=100.0
						...
						CREATE TABLE Borrow( --df: mult=1.5
						The default multiplier is 1.0, it does not need to be set on Reader. Then you can generate a data set with:
						sh> datafiller --size=1000 library.sql > library_test_data.sql
				Improving Generated Values
					take words from a dictionary
						title TEXT NOT NULL
						-- df English: word=/etc/dictionaries-common/words
						-- df: text=English length=4 lenvar=3
					prevent collisions by increasing size of population
						isbn ISBN13 NOT NULL -- df: size=1000000000
					birth date range
						birth DATE NOT NULL, -- df: start=1923-01-01 end=2010-01-01
					gender ratio
						gender BOOLEAN NOT NULL, -- df: rate=0.25
					timestamp: spread on a period of 50 days: 24 * 60 * 50 = 72000 min, precision is 60 sec
						borrowed TIMESTAMP NOT NULL -- df: size=72000 prec=60
					phone TEXT
						-- these directives could be on a single line
						-- df: chars='0-9' length=10 lenvar=0
						-- df: null=0.01 size=1000000
					fill directly to database
						datafiller --size=10 --filter library.sql | psql library
						datafiller --size=10 --filter library03.sql | psql library
					hataları düzelt
					tablo içeriklerini incele
					test
						SELECT firstname, COUNT(*) AS cnt FROM Reader GROUP BY firstname ORDER BY cnt DESC LIMIT 3;
				Advanced Features
					pattern generator: for email ex.
						email TEXT NOT NULL CHECK(email LIKE '%@%')
						-- df: pattern='[a-z]{3,8}\.[a-z]{3,8}@(gmail|yahoo)\.com'
					weighted distribution: chars or alt
						-- define two macros
						-- df librarian: inet='10.1.0.0/16'
						-- df reader: inet='10.2.0.0/16'
						ip INET NOT NULL
						-- df: alt=reader:8,librarian:2
						-- This would do as well: --df: alt=reader:4,librarian
		pg_sample: take a sample of rows from a database
			https://github.com/mla/pg_sample
			ex
				createdb sampledb
				pg_sample mydb | psql sampledb
		rdbms-subsetter: filter some sample data from existing database 
			https://github.com/18F/rdbms-subsetter
			it preserves existing constraints
			use cases:
				create test database
			ex
				rdbms-subsetter postgresql://:@/bigdb postgresql://:@/littledb 0.05
				# 0.05: fraction of data
		pgcli
Articles SQL  id=g10684
	Modern SQL id=g10465
		Modern SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10465>
		https://www.slideshare.net/MarkusWinand/modern-sql
		Modern SQL in PostgreSQL-nFfS1HmiWCM.webm
		LATERAL id=g10466
			LATERAL <url:file:///~/projects/study/otl/cdb.otl#r=g10466>
			ref
				What is the difference between LATERAL and a subquery in PostgreSQL? <url:#r=adb_006>
			before:
				SELECT ...
					, (SELECT column1 
						FROM t1)
					...
				wrong: SELECT column1, column2
				wrong: more than one row returned 
			after:
				SELECT ..., ldt.*
					FROM t2
					LEFT JOIN LATERAL 
						(SELECT column_1, column_2
						FROM t1
						WHERE t1.x = t2.y
						) AS ldt
						ON (true) ...
				new:
					lift both limitations
					and can be correlated
			before:
				inline views can't refer to outside view:
					SELECT *
						FROM t1
						JOIN (SELECT *
							FROM t2
							WHERE t2.x = t1.x
							) inline view
				WHERE t2.x = t1.x
					this refers to outside view
					this was wrong
				solution was:
					SELECT *
						FROM t1
						JOIN (SELECT *
							FROM t2
							) inline view
						ON (inline_view.x = t1.x)
			now:
				inline views can refer to outside view
					SELECT *
						FROM t1
						JOIN LATERAL 
							(SELECT *
								FROM t2
								WHERE t2.x = t1.x
								) inline view
						ON (true)   -- useless but still required
			why? where to use?
				usecase1: calling table functions with args from previously mentioned tables
					SELECT t1.id, tf.*
						FROM t1
						JOIN LATERAL table_function(t1.id) tf
						ON (true)
					LATERAL is optional here
				usecase2: limiting data set
					SELECT top_products.*
						FROM categories c
						JOIN LATERAL 
							(SELECT *
								FROM products p
								WHERE p.cat = c.cat
								ORDER BY p.rank DESC
								LIMIT 3
								) top_products
					no way to do without limit
					ex: get 10 most recent news
						SELECT n.*
							FROM news n
							JOIN subscriptions s
							ON (n.topic = s.topic)
							WHERE s.user = ?
							ORDER BY n.created DESC
							LIMIT 10
						# this joins 900 K rows. we only need 10 recent subscriptions
						SELECT n.*
							FROM subscriptions s
							JOIN LATERAL 
								(SELECT *
									FROM news n
									WHERE n.topic = s.topic
									ORDER BY n.created DESC
									LIMIT 10
									) top_news ON (true)
							WHERE s.user_id = ?
							ORDER BY n.created DESC
							LIMIT 10
			nutshell
				LATERAL is "for each" loop of SQL
					executed predefined number of times
				plays well with outer joins
				great for TOP-N suqueries
				can join table functions (unnest)
		GROUPING SETS id=g10467
			GROUPING SETS <url:file:///~/projects/study/otl/cdb.otl#r=g10467>
			before:
				only one GROUP BY operation at a time
				SELECT year, month, sum(revenue)
					FROM tbl
					GROUP BY year, month
				SELECT year
					, sum(revenue)
					FROM tbl
					GROUP BY year
				both together:
					SELECT year, month, sum(revenue)
						FROM tbl
						GROUP BY year, month
					UNION ALL
					SELECT year
						, null
						, sum(revenue)
						FROM tbl
						GROUP BY year
			after:
				SELECT year, month, sum(revenue)
					FROM tbl
					GROUP BY 
						GROUPING SETS (
							(year, month)
							, (year)
							)
			nutshell
				multiple GROUP BY
				() "empty brackets" build a group over all rows
				Permutations can be created using ROLLUP and CUBE
					(ROLLUP(a,b,c) = GROUPING SETS ((a,b,c), (a,b),(a),()))
					https://stackoverflow.com/questions/25274879/when-to-use-grouping-sets-cube-and-rollup#25276123
						CUBE is the same of GROUPING SETS with all possible combinations
						if you don't really need all combinations, you should use GROUPING SETS rather than CUBE
						GROUP BY CUBE (C1, C2, C3, ..., Cn-2, Cn-1, Cn)
						===
						GROUP BY GROUPING SETS (
								 (C1, C2, C3, ..., Cn-2, Cn-1, Cn) -- All dimensions are included.
								,( , C2, C3, ..., Cn-2, Cn-1, Cn) -- n-1 dimensions are included.
								,(C1, C3, ..., Cn-2, Cn-1, Cn)
								…
		WITH (common table expressions CTE) id=g10468
			WITH (common table expressions CTE) <url:file:///~/projects/study/otl/cdb.otl#r=g10468>
			nested subqueries are hard to read
			syntax
				WITH
					a (c1, c2) -- column names optional
					AS (SELECT c1, c2 FROM ...)
			before:
				nested queries are hard to read:
					SELECT ...
						FROM (SELECT ...
							FROM t1
							JOIN (SELECT ... FROM ...
								) a ON (...)
						) b
						...
			after
				CTEs are statement-scoped views
					WITH
						a (c1, c2, c3)
						AS (SELECT c1, c2, c3 FROM ...),
						b (c4, ...)
						AS (SELECT c4, ... FROM ... JOIN a ON (...)),
					SELECT ...
						FROM b JOIN a ON (...)
			nutshell
				they are like private methods of SQL
				views can be referred
				allows chaining instead of nesting
				allowed where SELECT is allowed
					INSERT INTO tbl
					WITH ... SELECT ...
			but
				pgs: WITH views are like materialized views
			INSERT, UPDATE, DELETE inside WITH
			WITH deleted_rows AS (
				DELETE FROM source_tbl
				RETURNING *
				)
				INSERT INTO destination_tbl
				SELECT * FROM deleted_rows;
			issue: performance 
				CTE doesn't know about outer filters
		WITH RECURSIVE id=g10469
			WITH RECURSIVE <url:file:///~/projects/study/otl/cdb.otl#r=g10469>
			this also part of making SQL, turing complete
			before:
				too hard
			after
				WITH RECURSIVE cte (n)
					AS 
						(SELECT 1
							UNION ALL
							SELECT n+1
							FROM cte
							WHERE n < 3)
					SELECT * FROM cte;
			use cases
				row generators
				for graph processing
				loops that
					pass data to next iteration
					need a dynamic abort condition
			nutshell
				WITH RECURSIVE is while of SQL
				supports infinite loops
		OVER and PARTITION BY id=g10470
			OVER and PARTITION BY <url:file:///~/projects/study/otl/cdb.otl#r=g10470>
			together with recursive CTE SQL became turing complete with over feature
			/Users/mertnuhoglu/gdrive/public/img/ss-249.png
				| sql                                          | merge rows | aggregate |
				| select * from t                              | false      | false     |
				| select distinct c1 from t                    | true       | false     |
				| select * from t join (select .. group by ..) | false      | true      |
				| select sum(c1) from t group by ..            | true       | true      |
				better:
					select * from t join (select .. group by ..) 
					-->
					select sum(c2) over (partition by c1) from t
			before:
				WITH total_salary_by_department
					AS (SELECT dep, SUM(salary) total
						FROM emp
						GROUP BY dep)
					SELECT dep, emp_id, salary,
						salary/ts.total*100 "% of dep"
						FROM emp
						JOIN total_salary_by_department ts
						ON (emp.dep = ts.dep)
			what if we add:
				WHERE emp.dep = ?
			before: there was no chance to collect aggregates but not collapse rows
			after:
				SELECT dep, emp_id, salary,
					salary/SUM(salary) OVER (PARTITION BY dep) * 100 "% of dep"
					FROM emp
			notes:
				PARTITION BY seggregates like GROUP BY does
				OVER () aggregates over all result set
			nutshell
				any aggregate function ok
		OVER and ORDER BY id=g10471
			OVER and ORDER BY <url:file:///~/projects/study/otl/cdb.otl#r=g10471>
			before: calculating a running total
				SELECT txid, value,
					(SELECT SUM(value)
						FROM transactions tx2
						WHERE tx2.acnt = tx1.acnt
						AND tx2.txid <= tx1.txid) bal
					FROM transactions tx1
					WHERE acnt = ?
					ORDER BY txid
				# issues
					requires a scalar subselect or selfjoin
					poor maintenance
					poor performance
					better: do it in application
			after:
				SELECT txid, value,
					SUM(value)
						OVER(ORDER BY txid
							ROWS
							BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) bal
					FROM transactions tx1
					WHERE acnt = ?
					ORDER BY txid
			new functions makes sense with OVER(ORDER BY ...)
				ROW_NUMBER
				ranking:
					RANK, DENSE_RANK, CUME_DIST
			use cases:
				aggregates without GROUP BY
				running totals, moving averages
				ranking
					top-n per group
				avoiding self-joins
		WITHIN GROUP id=g10472
			WITHIN GROUP <url:file:///~/projects/study/otl/cdb.otl#r=g10472>
			grouped rows cannot be ordered prior aggregation
				how to get middle value (median) of a set
			before:
				...
			after:
				SELECT PERCENTILE_DISC 0.5
					WITHIN GROUP (ORDER BY val)
				FROM data
			hypothetical set-functions
				SELECT RANK(123)
					WITHIN GROUP (ORDER BY val)
				FROM data
		OVER id=g10473
			OVER <url:file:///~/projects/study/otl/cdb.otl#r=g10473>
			before: calculate difference to previous row
				WITH numbered_data AS (
					SELECT *,
						ROW_NUMBER() OVER(ORDER BY x) rn
						FROM data)
					SELECT cur.*, cur.balance - prev.balance
						FROM numbered_data cur
						LEFT JOIN numbered_data prev 
						ON (cur.rn = prev.rn - 1)
			after:
				SELECT *, balance - LAG(balance) OVER (ORDER BY x)
					FROM data
			other functions
				LEAD/LAG
				FIRST_VALUE / LAST_VALUE
				NTH_VALUE(col, n) FROM FIRST/LAST RESPECT/IGNORE NULLS
		FETCH FIRST id=g10474
			FETCH FIRST <url:file:///~/projects/study/otl/cdb.otl#r=g10474>
			like LIMIT but SQL standard
			before:
				SELECT *
					FROM (SELECT *,
						ROW_NUMBER() OVER(ORDER BY x) rn
						FROM data) numbered_data
					WHERE rn <= 10
			after:
				SELECT *
					FROM data
					ORDER BY x
					FETCH FIRST 10 ROWS ONLY
		OFFSET id=g10475
			OFFSET <url:file:///~/projects/study/otl/cdb.otl#r=g10475>
			OFFSET is EVIL
				comparison to sleep
				problem: renumbering rows 
			before: skip 10 rows, then get next 10
				SELECT *
					FROM (SELECT *,
						ROW_NUMBER() OVER(ORDER BY x) rn
						FROM data
						FETCH FIRST 20 ROWS ONLY
					) numbered_data
					WHERE rn > 10
			after:
				don't use offset
			alternative to OFFSET:
				http://use-the-index-luke.com/no-offset
					db still fetches these rows and bring them in order before it can send the following ones
					use where clause that filters the data you look for
					sql
						SELECT ...
						FROM ...
						WHERE ...
						AND id < ?last_seen_id
						ORDER BY id DESC
						FETCH FIRST 10 ROWS ONLY
				http://use-the-index-luke.com/sql/partial-results/fetch-next-page
					how to do proper pagination
					opt1: offset
						SELECT *
							FROM sales
						 ORDER BY sale_date DESC
						OFFSET 10
						 FETCH NEXT 10 ROWS ONLY
						costs
							db counts all rows from beginning until requested page
								pages drift when inserting new rows
								response time increases when browsing further back
					opt2: seek
						SELECT *
							FROM sales
						 WHERE sale_date < ?
						 ORDER BY sale_date DESC
						 FETCH FIRST 10 ROWS ONLY
						benefits:
							doesn't get already shown pages
							where condition uses index
							you get stable results if new rows are inserted
		AS OF id=g10476
			AS OF <url:file:///~/projects/study/otl/cdb.otl#r=g10476>
			before: changing data was destructive
			after: tables can be system versioned
				CREATE TABLE t (...,
					start_ts TIMESTAMP(9) GENERATED ALWAYS AS ROW START,
					end_ts TIMESTAMP(9) GENERATED ALWAYS AS ROW END,
					PERIOD FOR SYSTEM TIME (start_ts, end_ts)
				) WITH SYSTEM VERSIONING
			ex:
				INSERT ... (ID, DATA) VALUES (1, 'x')
					| ID | Data | start_ts | end_ts |
					| 1  | x    | 10:00    |        |
				UPDATE ... SET DATA = 'y' ...
					| ID | Data | start_ts | end_ts |
					| 1  | x    | 10:00    | 11:00  |
					| 1  | y    | 11:00    |        |
			with AS OF you can query:
				SELECT *
					FROM t FOR SYSTEM_TIME AS OF
						TIMESTAMP '2015-04-02 10:30:00'
					| ID | Data | start_ts | end_ts |
					| 1  | x    | 10:00    | 11:00  |
		WITHOUT OVERLAPS
			how to avoid overlapping periods
				| id | begin | end   |
				| 1  | 9:00  | 11:00 |
				| 1  | 10:00 | 12:00 |
			before:
				using triggers 
			after:
				PRIMARY KEY (id, period WITHOUT OVERLAPS)
				===
				EXCLUDE USING gist (id WITH =, period WITH &&)
		Temporal/Bi-Temporal SQL
			next
		LIST_AGG id=g10477
			LIST_AGG <url:file:///~/projects/study/otl/cdb.otl#r=g10477>
			ex:
				| grp | val |
				| 1   | B   |
				| 1   | A   |
				| 1   | C   |
				| 2   | X   |
				SELECT grp
					, LIST_AGG(val, ', ')
						WITHIN GROUP (ORDER BY val)
					FROM t
					GROUP By grp
				-->
				| grp | val     |
				| 1   | A, B, C |
				| 2   | X       |
		Temporal Features in Sql 2011
			http://cs.ulb.ac.be/public/_media/teaching/infoh415/tempfeaturessql2011.pdf
			Introduction
			Temporal data support
				Periods
					an interval on timeline
					period definitions are metadata to tables
						pair of columns: period start and period end time
		Assign Names to Columns Without Known Name
			http://modern-sql.com/use-case/naming-unnamed-columns
			table functions produce columns with no names:
				ex: unnest, values 
			opt1: using aliases in the from Clause
				SELECT COUNT(c1)
						 , COUNT(*)
					FROM (VALUES (1)
										 , (NULL)
							 ) t1(c1)
				t1: table alias
					c1: column alias for first column
			opt2: Using CTE
				WITH t1 (c1) AS (
						 VALUES (1)
									, (NULL)
				)
				SELECT COUNT(c1)
						 , COUNT(*)
					FROM t1
		Literate SQL id=g10478
			Literate SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10478>
			http://modern-sql.com/use-case/literate-sql
			WITH has two properties:
				names come first
				subqueries are unnested
			Names first
				meaningful names: very important
				ex: name is out of sight
					SELECT ...
						FROM (SELECT ...
										FROM ...
								 ) intention_revealing_name
						 ...
				after: name put before the code (like programming languages)
					WITH intention_revealing_name AS (
							 SELECT ...
								 FROM ...
							 )
					SELECT ...
						FROM intention_revealing_name irn
						 ...
			Order of human logic
				turn nesting into chaining
				with is a prefix for SELECT
		Unit Tests on Transient Data
			http://modern-sql.com/use-case/unit-tests-on-transient-data
			opt1: Open Transactions
			opt2: with and values
				1. with creates table with the same name
				2. with generates test data
				ex
					WITH cart (product_id, qty) AS (
							 VALUES (1, 2)
					)
					SELECT ...
						FROM cart
						 ...
		with — Organize Complex Queries
			http://modern-sql.com/feature/with
			you can use 'with' as prefix for select everywhere:
				WITH query_name (column_name1, ...) AS
						 (SELECT ...)
				SELECT ...
			WITH is like CREATE VIEW
		case — Conditional Expressions id=g10479
			case — Conditional Expressions <url:file:///~/projects/study/otl/cdb.otl#r=g10479>
			http://modern-sql.com/feature/case
			code
				CASE WHEN <condition> THEN <result>
						[WHEN <condition> THEN <result>
						 ...]
						[ELSE <result>]
				END
			handling NULL
				note: null = null is not true
					thus 'when null' never applies
				opt1: coalesce
					COALESCE(a, b)
					===
					CASE WHEN a IS NOT NULL THEN a
							 ELSE b
					END
				opt2: nullif
					x / NULLIF(y, 0)
					===
					x / CASE WHEN y = 0 THEN null 
									 ELSE y
							END
		EXTRACT expression id=g10480
			EXTRACT expression <url:file:///~/projects/study/otl/cdb.otl#r=g10480>
			http://modern-sql.com/feature/extract
			code
				EXTRACT(<field> FROM <expression>)
			fields
				Year  YEAR
				Month MONTH
				Day of month  DAY
				24 hour HOUR
				Minute  MINUTE
				Seconds (including fractions) SECOND
				Time zone hour  TIMEZONE_HOUR
				Time zone minute  TIMEZONE_MINUTE
			related:
				EXTRACT gets single field
				to extract multiple fields, CAST can be used
			CAST
				CAST(<timestamp> AS [DATE|TIME])
			anti-patterns
				string formatting functions
				inappropriate use in WHERE
					ex: consider
						WHERE EXTRACT(YEAR FROM some_date) = 2016
						last time unit is not known
						resolution unknown
						no index can be utilized
					better:
						WHERE some_date >= DATE'2016-01-01'
							AND some_date <  DATE'2017-01-01'
		FILTER clause id=g10481
			FILTER clause <url:file:///~/projects/study/otl/cdb.otl#r=g10481>
			http://modern-sql.com/feature/filter
			ex:
				SUM(<expression>) FILTER (WHERE <condition>)
				SUM(<expression>) FILTER (WHERE <condition>) OVER (...)
		The FILTER clause in Postgres 9.4 id=g10482
			The FILTER clause in Postgres 9.4 <url:file:///~/projects/study/otl/cdb.otl#r=g10482>
			https://medium.com/little-programming-joys/the-filter-clause-in-postgres-9-4-3dd327d3c852
			before:
				SELECT
					COUNT(*) AS unfiltered,
					SUM( CASE WHEN i < 5 THEN 1 ELSE 0 END ) AS filtered
				FROM generate_series(1,10) AS s(i);
			after:
				SELECT
					COUNT(*) AS unfiltered,
					COUNT(*) FILTER (WHERE i < 5) AS filtered
				FROM generate_series(1,10) AS s(i);
			ex: sellers sales counters based on purchase states
				SELECT purchases.seller_id,
					SUM(CASE WHEN state IN ('authorized', 'reversed') THEN 1 ELSE 0 END) AS sales_count,
					SUM(CASE WHEN state = 'authorized' THEN 1 ELSE 0 END) AS successful_sales_count
				FROM purchases
				GROUP BY purchases.seller_id
				-->
				SELECT purchases.seller_id,
					COUNT(1) FILTER (WHERE state IN ('authorized', 'reversed')) AS sales_count,
					COUNT(1) FILTER (WHERE state = 'authorized') AS successful_sales_count
				FROM purchases
				GROUP BY purchases.seller_id
		Table Column Aliases id=g10485
			Table Column Aliases <url:file:///~/projects/study/otl/cdb.otl#r=g10485>
			http://modern-sql.com/feature/table-column-aliases
			code
				FROM … [AS] alias [(<derived column list>)]
			issues:
				you have to alias all columns in the table in order
			use case: unnest, values table functions
			ex: WITH clause can be used to name columns
				WITH t1 (c1) AS (
						 VALUES (1)
									, (NULL)
				)
				SELECT COUNT(c1)
						 , COUNT(*)
					FROM t1
		listagg Function
			http://modern-sql.com/feature/listagg
			rows to delimited strings
			used to denormalize rows into a string of csv values
			issue: no way to escape separator
			...
		match_recognize Clause
			http://modern-sql.com/feature/match_recognize
		values Clause   
			http://modern-sql.com/feature/values
			can be used where SELECT is allowed
		SELECT without FROM id=g10486
			SELECT without FROM <url:file:///~/projects/study/otl/cdb.otl#r=g10486>
			instead of non-conforming select without from:
				SELECT CURRENT_DATE
			use VALUES without INSERT
				VALUES (CURRENT_DATE)
			better because: can return multiple rows 
				SELECT without FROM can return 1 row
			opt: conforming
				SELECT *
					FROM (VALUES (1,2)
										 , (3,4)
							 ) t1 (c1, c2)
		The Three-Valued Logic of SQL id=g10488
			The Three-Valued Logic of SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10488>
			http://modern-sql.com/concept/three-valued-logic
			three valued logic:
				true, false, unknown
				why?
					because null marks absent data
				if null affects result, then result is neither true or false but unknown
			Comparisons to null
				null means: can be anything
				nothing equals null
					not even null equals null
					because each null can be different
					this is rationale for:
						IS NULL
						IS NOT DISTINCT FROM
							treats two null as same
			Logical Operations Involving Unknown
				in logical connections (and, or)
					unknown behaves like null in comparisons
					result is unknown if it depends on unknown operand
			General Rule: where, having, when, etc.
				odd: NOT IN (NULL) is never true
					WHERE 1 NOT IN (NULL)
					never allow NULL in lists
					for subqueries: use NOT EXISTS instead of NOT IN or remove NULL inside WHERE
			Exception: Check Constraints
				they reject false, thus they accept true and unknown
				ex
					CREATE TABLE t (
							a NUMERIC CHECK (a >= 0),
							b NUMERIC CHECK (b >= 0),
							CHECK ( a + b <= 10 )
					)
	Use the Index Luke id=g10487
		Use the Index Luke <url:file:///~/projects/study/otl/cdb.otl#r=g10487>
		http://use-the-index-luke.com/sql/table-of-contents
		Preface — Why is indexing a development task?
			some say: sql is inherently slow
				why is performance problems commonplace
			sql: most successful 4GL programming language
				separates: what and how
					SELECT date_of_birth
						FROM employees
					 WHERE last_name = 'WINAND'
				side effect: we need to know "how" to solve performance problems
			only thing you need to learn: how to index
				database indexing: a development task
				because it depends on how application queries the data
			most important index type: B-tree index
				works identically in most db
			Chapter 1: fundamental structure of an index
				essential don't skip it
		Anatomy of an Index — What does an index look like?
			intro
				index: a distinct structure in db
					built using: "create index" statement
					has own disk space
					holds a copy of indexed table data
					thus it is pure redundancy
						does not change table data
						create new data structure that refers to table
				it is like: phonebook directory
					difference: insert, delete, update keeps index order without moving data
			The Leaf Nodes — A doubly linked list
				main purpose of index: ordered representation of indeexd data
					not possible to store data sequentially
						because insert would need to move following entries
					solution: build a logical order independent of physical order
				logical order
					built via doubly linked list
					each node
						has links to two neighboring entries
							like a chain
					new nodes: inserted between two existing nodes
						by updating their links
					doubly linked: node refers to preceding and following node
					doubly linked lists:
						java.util.LinkedList
						databases use them for index leaf nodes
				leaf node:
					stored in a database block/page
						smallest storage unit
						a few kb
				structure:
					doubly linked list of leaf nodes
						index entries inside each leaf node
				/Users/mertnuhoglu/gdrive/public/img/ss-250.png
			The B-Tree — It's a balanced tree
				leaf nodes are in arbitrary order
					position in disk is independent of logical position of index order
					it is like shuffled pages
				how to find entry among shuffled pages?
					a balanced search tree: B-tree
				/Users/mertnuhoglu/gdrive/public/img/ss-251.png
					root node
						branch nodes
							leaf nodes
					branch nodes: 
						each branch node entry: corresponds to biggest value in respective leaf node
					each layer built similarly
						repeats until all keys fit into a single node: root node
					structure is balanced search tree
						because tree depth is equal at every position
						unlike a binary tree
				/Users/mertnuhoglu/gdrive/public/img/ss-252.png
					search for the key: 57
				tree traversal
					very efficient
					first power of indexing
						because: tree balance and logarithmic growth of tree depth
							ie. tree depth grows very slowly
			Slow Indexes, Part I — Two ingredients make the index slow
				still there are cases where an index lookup is slow
					myth of "degenerated index"
						solution: rebuilding index
						this has no effect in fact
				first factor for slowness: leaf node chain
					ex: search for 57 in previous example
						two entries are same
						db must read next leaf node to see if there are any more matching entries
				second factor: accessing the table
					there is additional table access for each hit
				index lookup consists of 3 steps:
					1. tree traversal
					2. following leaf node chain
					3. fetching table data
					only tree traversal has upper bound
		The Where Clause — Indexing to improve search performance
			intro
				poorly written where clause => slowness
			The Equals Operator — Exact key lookup
				Primary Keys — Verifying index usage
					simplest where clause: primary key lookup
					ex:
						CREATE TABLE employees (
							 employee_id   NUMBER         NOT NULL,
							 first_name    VARCHAR2(1000) NOT NULL,
							 last_name     VARCHAR2(1000) NOT NULL,
							 date_of_birth DATE           NOT NULL,
							 phone_number  VARCHAR2(1000) NOT NULL,
							 CONSTRAINT employees_pk PRIMARY KEY (employee_id)
						)
					db automatically creates an index for pk
					ex:
						SELECT first_name, last_name
							FROM employees
						 WHERE employee_id = 123
					execution plan:
						Index Scan using employees_pk on employees 
							 (cost=0.00..8.27 rows=1 width=14)
							 Index Cond: (employee_id = 123::numeric)
				Concatenated Keys — Multi-column indexes
					if the key consists of multiple columns
					/Users/mertnuhoglu/gdrive/public/img/ss-253.png
					thus ordering of columns for index is important
					ex:
						CREATE UNIQUE INDEX EMPLOYEES_PK 
								ON EMPLOYEES (SUBSIDIARY_ID, EMPLOYEE_ID)
						this index is used for it as well:
							where SUBSIDIARY_ID = 20
				Slow Indexes, Part II — The first ingredient, revisited
		Execution Plans
			PostgreSQL
				Getting an Execution Plan
					put "explain" command in front of a sql statement
						for bind parameters:
							put "prepare" first
							ex:
								PREPARE stmt(int) AS SELECT $1
									"$n" used for bind parameters
								EXPLAIN EXECUTE stmt(1)
					for other statements:
						EXPLAIN SELECT 1
					out
						Result  (cost=0.00..0.01 rows=1 width=0)
							operation name: Result
							related cost
								two cost values:
									first: cost for startup
									second: execution cost if all rows are retrieved
							row count estimate
							expected row width
					opt2: ANALYZE
						it executes actually
							so data is modified for INSERT, UPDATE, DELETE
						to prevent it:
							enclose in transaction
							perform a rollback afterwards
						ex:
							BEGIN 
							EXPLAIN ANALYZE EXECUTE stmt(1)
							ROLLBACK
						out
							Result  (cost=0.00..0.01 rows=1 width=0)
										 (actual time=0.002..0.002 rows=1 loops=1)
							Total runtime: 0.020 ms
						prints estimated+actual cost
					prepared statements must be closed:
						DEALLOCATE stmt
				Operations
					Index and Table Access
						Seq Scan
							scans entire relation (table) as stored on disk
						Index Scan
							performs a b-tree traversal
							fetches corresponding table data
					Join Operations
						"table" ≅ "intermediate result"
						nested loops
							query other table for each row from the first
						hash join / hash
							"Hash" in plan
							loads candidate records into a hash table
							probes for each record of the other side
						merge join
							both sides must be presorted
					Sorting and Grouping
						sort / sort key
							needs large memory to materialize intermediate result
						GroupAggregate
						HashAggregate
					Top-N Queries
						Limit
						WindowAgg
				Distinguishing Access and Filter-Predicates
					3 different methos to apply where clauses (predicates)
					Access predicate ("Index Cond")
						express start and stop conditions of leaf node traversal
					Index Filter Predicate ("Index Cond")
						applied during leaf node traversal only
					Table level filter predicate ("Filter")
						for unindexed columns
		Bind Parameters
			Parameterized Queries
		Tips
			Visualizing index
				code
					SELECT <INDEX COLUMN LIST> 
						FROM <TABLE>  
					 ORDER BY <INDEX COLUMN LIST>
					 FETCH FIRST 100 ROWS ONLY
				get a sample from the index
	davisjeff id=g10685
		davisjeff <url:file:///~/projects/study/otl/cdb.otl#r=g10685>
		What is the deal with NULLs? id=g10680
			What is the deal with NULLs?  <url:file:///~/projects/study/otl/cdb.otl#r=g10680>
			http://thoughts.davisjeff.com/2009/08/02/what-is-the-deal-with-nulls/
			sql's approach to missing information is unique
				null is not a value:
					this is contradiction to sql standard
					sql uses three-valued logic (3VL)
				null means unknown: third truth value
					this is contradiction too
				null is false like
					it is misleading
					in WHERE: null predicate is treated like FALSE
					in CHECK constraint: NULL is treated like TRUE
				oh that makes sense
					you see a pattern of null behaviour
					this is often wrong
					null semantics are a mix of behaviors
					lots of special cases
				p OR NOT p
					this is not always true in sql
				SUM() vs +
					SUM of 1 and NULL = 1
					1 + NULL = NULL
				aggregates ignore NULLs
					aggregates ignore nulls but why not + operator?
				aggregates return null
					they return null when they have no non-null input
					ex: SUM over zero tuples: shouldn't it be zero?
				sql breaks its own rules
					these aggregate rules don't work for COUNT()
					COUNT(*)
						breaks aggregates ignore nulls rule
						breaks aggregates return null rule
					COUNT(x)
						breaks aggregates return null rule
					ARRAY_AGG() 
						breaks first but not the latter
					SUM()
						follows both
				NULLs appear even when you have complete information
					because of outer join and aggregates, nulls can appear anywhere
				WHERE NOT IN (SELECT ...)
					if subselect returns any NULLs, then NOT IN can only evaluate to FALSE or NULL
							thus you get no tuples
				x >= 10 or x <= 10
					not a tautology in sql
				x IS NULL AND x IS DISTINCT FROM NULL
					this expression can eval to TRUE
						if x = ROW(NULL)
				NOT x IS NULL
					this is not the same as x IS NOT NULL
						if x is ROW(1,NULL)
							former: TRUE
							latter: FALSE
				NOT x IS NULL AND NOT x IS NOT NULL
					do you have ROW(1,NULL)?
		Taking a step back from ORMs
			I then made the quantum leap to databases
				why quantum leap?
					if you are debugging the application, you can quickly see what’s going wrong by seeing what the application sees right before the bug is hit
					cost to ORMs is that developers are tempted to remain ignorant of the SQL DBMS
				misleadings
					learning SQL is harder than learning an ORM
					help eliminate boilerplate in some common situations
				we should be able to do so without something as invasive as an ORM
					ex
						# 'conn' is a PG::Connection object
						def sqlinsert(conn, table, rec)
							table     = conn.quote_ident(table)
							rkeys     = rec.keys.map{|k| conn.quote_ident(k.to_s)}.join(",")
							positions = (1..rec.keys.length).map{|i| "$" + i.to_s}.join(",")
							query     = "INSERT INTO #{table}(#{rkeys}) VALUES(#{positions})"
							conn.exec(query, rec.values)
						end
		SQL: the successful cousin of Haskell
			tough problems that haskell would solve are already solved in so many cases by SQL
		Database for a Zoo: the problem and the solution id=g10681
			Database for a Zoo: the problem and the solution <url:file:///~/projects/study/otl/cdb.otl#r=g10681>
			http://thoughts.davisjeff.com/2011/09/21/database-for-a-zoo-the-problem-and-the-solution/
			problem:
				you cannot mix animals of different types within a single zoo cage
			how to enforce it?
				opt01: procedurally
					check if any animals are already assigned to the cage
					if so, make sure they are the same type
				opt02: exclusion constraints
					semantics: if any two rows are compared on some columns/expresions
						not all of the comparisons will return TRUE
					how?
						install btree_gist
							CREATE EXTENSION btree_gist
						table
							CREATE TABLE zoo
							(
								animal_name TEXT,
								animal_type TEXT,
								cage        INTEGER,
								UNIQUE      (animal_name),
								EXCLUDE USING gist (animal_type WITH <>, cage WITH =)
							);
						ex: never evaluates to TRUE
							TupleA.animal_type <> TupleB.animal_type AND
							TupleA.cage        =  TupleB.cage
			design
				1: constraint is declarative
				2: immune from race Conditions
		Building SQL Strings Dynamically, in 2011
			http://thoughts.davisjeff.com/2011/07/09/building-sql-strings-dynamically-in-2011/
			ex:
				SELECT first_name, last_name, subsidiary_id, employee_id
				FROM employees
				WHERE ( subsidiary_id    = :sub_id OR :sub_id IS NULL )
					AND ( employee_id      = :emp_id OR :emp_id IS NULL )
					AND ( UPPER(last_name) = :name   OR :name   IS NULL )
			01: if you pass a NULL, that condition becomes obsolete
		Why PostgreSQL Already Has Query Hints
			http://thoughts.davisjeff.com/2011/02/05/why-postgresql-already-has-query-hints/
		Exclusion Constraints are generalized SQL UNIQUE id=g10682
			Exclusion Constraints are generalized SQL UNIQUE <url:file:///~/projects/study/otl/cdb.otl#r=g10682>
			http://thoughts.davisjeff.com/2010/09/25/exclusion-constraints-are-generalized-sql-unique/
			ex: The first requirement you’ll encounter is that no two reservations may overlap
			ex: identical to UNIQUE constraint
				DROP TABLE IF EXISTS a;
				CREATE TABLE a(i int);
				ALTER TABLE a ADD EXCLUDE (i WITH =);
			ex: multi-column: identical to UNIQUE on (a.i, a.j)
				DROP TABLE IF EXISTS a;
				CREATE TABLE a(i int, j int);
				ALTER TABLE a ADD EXCLUDE (i WITH =, j WITH =);
			you can use any operator other than "="
				operator that is: commutative, boolean, searchable by index access method (btree, hash, gist)
				for BTree and Hash: only "="
				other data types: PERIOD, CIRCLE, BOX are searchable using GiST
					overlaps &&
			ex: overlapping periods
				DROP TABLE IF EXISTS b;
				CREATE TABLE b (p PERIOD);
				ALTER TABLE b ADD EXCLUDE USING gist (p WITH &&);
				INSERT INTO b VALUES('[2009-01-05, 2009-01-10)');
				INSERT INTO b VALUES('[2009-01-07, 2009-01-12)'); -- causes ERROR
			ex: overlapping reservations
				CREATE TABLE reservation(room TEXT, professor TEXT, during PERIOD);
				-- enforce the constraint that the room is not double-booked
				ALTER TABLE reservation
						ADD EXCLUDE USING gist
						(room WITH =, during WITH &&);
				-- enforce the constraint that the professor is not double-booked
				ALTER TABLE reservation
						ADD EXCLUDE USING gist
					 (professor WITH =, during WITH &&);
		Flexible Schemas and PostgreSQL
			http://thoughts.davisjeff.com/2010/05/06/flexible-schemas-and-postgresql/
			flexible schema: data model that permits changes in data structures
			problem: ALTER TABLE a major performance problem
		Temporal PostgreSQL Roadmap
			http://thoughts.davisjeff.com/2010/03/09/temporal-postgresql-roadmap/
			why are temporal extensions important?
			what is already done?
				PERIOD data type
				Temporal Keys: called Exclusion Constraints
		Choosing Data Types
			http://thoughts.davisjeff.com/2009/09/30/choosing-data-types/
			database design: starts from understanding the business
			book: C.J.Date An Introduction to Database Systems
		Why DBMSs are so complex
			http://thoughts.davisjeff.com/2008/08/03/why-dbmss-are-so-complex/
			wrong approach: use oo design to manage data instead of relational model
				assumes data is highly independent
				constraints: interdependent data
				they put complexity into the system
		Data Labels and Predicates id=g10683
			Data Labels and Predicates <url:file:///~/projects/study/otl/cdb.otl#r=g10683>
			http://thoughts.davisjeff.com/2008/02/24/data-labels-and-predicates/
			ex:
				username | realname   | phone
				---------+------------+----------
				jdavis   | Jeff Davis | 555-1212
				jsmith   | John Smith | 555-2323
			Predicate P1: There exists a user with user name [username] and real name [realname] and phone number [phone].
				we can freely omit parts of the predicate
			Predicate P2: There exists (or existed) a user with user name [username] and real name [realname] and phone number [phone] during the interval of time [during].
				we cannot freely remove "during" portion
			in general: attributes are not independent except in the simplest cases
				predicates are a much more complete way to represent the meaning of data
					there is no requirement that the attributes be independent
				attribute names (ie. labels): used as a reminder not as a representation of the actual meaning
		Parallelism in Postgres 11
			https://speakerdeck.com/macdice/parallelism-in-postgresql-11
			Thomas Munro
	craigkerstiens id=g10686
		craigkerstiens <url:file:///~/projects/study/otl/cdb.otl#r=g10686>
		Postgres Hidden Gems id=g10665
			Postgres Hidden Gems <url:file:///~/projects/study/otl/cdb.otl#r=g10665>
			http://www.craigkerstiens.com/2018/01/31/postgres-hidden-gems/
			messages
				saving a tree using ltree
				working with time
				lateral joins
				row_number() over(partition ... order by ...)
				index access method and custom FDWs
				set-returning functions and custom aggregate functions
				psql -x
				unicode table borders
				notify/listen
				GIN indices make LIKE run fast even if % isn't at the end of string
				statistics system to plan queries
				vim editor in psql
				psql: \x auto: wide table displayed vertically
				\watch
		How I Work With Postgres – Psql, My PostgreSQL Admin id=g10667
			How I Work With Postgres – Psql, My PostgreSQL Admin <url:file:///~/projects/study/otl/cdb.otl#r=g10667>
			http://www.craigkerstiens.com/2013/02/13/How-I-Work-With-Postgres/
			tips
				Set your default EDITOR then use \e
				On postgres 9.2 and up \x auto is your friend
				Set history to unlimited
				\d all the things
			Editor
				\e
					export EDITOR=vim
					export PSQL_EDITOR=vim # better
					psql
					\e 
					# use vim for sql editing
				\x auto
					# most intelligible format to read results
				psql history
					export HISTFILESIZE=
					export HISTSIZE=
				\d
					\d
						listing all relations
					\dt
						list only all tables
					\d <relation>
						describe a relation
		Working With Time in Postgres
			http://www.craigkerstiens.com/2017/06/08/working-with-time-in-postgres/
			Date math
				'1 day'::interval
				'1 week'::interval
				ex:
					WHERE created_at >= now() - '1 week'::interval
			Date functions
				ex: to find the count of users that signed up per week:
					SELECT date_trunc('week', created_at), count(*)
					FROM users
					GROUP BY 1
					ORDER BY 1 DESC;
				ex: above query doesn't show 0 counts:
					SELECT generate_series('2017-01-01'::date, now()::date, '1 week'::interval) weeks
					WITH weeks AS (
						SELECT week
						FROM generate_series('2017-01-01'::date, now()::date, '1 week'::interval) week
					)
					SELECT weeks.week, count(*)
					FROM weeks, users
					WHERE users.created_at > weeks.week
						AND users.created_at <= (weeks.week - '1 week'::interval)
					GROUP BY 1
					ORDER BY 1 DESC
			Timestamp vs. Timestamptz
				just use timestamptz
			More
				extract
				dow: day of the week
		A Tour of Postgres' Foreign Data Wrappers
			http://www.craigkerstiens.com/2016/09/11/a-tour-of-fdws/
			your data isn't always in a SQL database
			Overview
				FDW allows you to connect to a remote system
		Writing More Legible SQL id=g10672
			Writing More Legible SQL <url:file:///~/projects/study/otl/cdb.otl#r=g10672>
			http://www.craigkerstiens.com/2016/01/08/writing-better-sql/
			One thing per line
				ex:
					SELECT foo,
						bar
					FROM baz
			Align your projections and conditions
				SELECT foo,
					bar
				FROM baz
				WHERE foo > 3
					AND bar = 'craig.kerstiens@gmail.com'
			Use column names when grouping/ordering
				ORDER BY 1 
				->
				ORDER BY SUM(foo)
			Comments
			Casing
			CTEs
		Understanding Postgres Performance
			http://www.craigkerstiens.com/2012/10/01/understanding-postgres-performance/
		Why I Blog
			http://www.craigkerstiens.com/2013/03/31/why-i-blog/
			Becoming replaceable
				to become heavily replaceable
			Lots can be shared
			Distilling information
			Economies of scale
		Postgres 11 - a First Look
			http://www.craigkerstiens.com/2018/09/20/postgresql-11-a-first-look/
			Quitting Postgres
				quit exit ^d \q
			Fear column addition no more
				no more table lock when adding new columns
			Performance
				Parallelism
					parallel hash joins, append, index creation
				JIT
			Statistics
				in 10: CREATE STATISTICS
			Keeping standbys warm
		Getting More Out of Psql (the PostgreSQL CLI) id=g10677
			Getting More Out of Psql (the PostgreSQL CLI) <url:file:///~/projects/study/otl/cdb.otl#r=g10677>
			http://www.craigkerstiens.com/2013/02/21/more-out-of-psql/
			Feel yourself at home
				CLI: most efficient user interface ever
				font:
					\pset linestyle unicode
					\pset border 2
				colors in the prompt:
					\set PROMPT1 '%[%033[33;1m%]%x%[%033[0m%]%[%033[1m%]%/%[%033[0m%]%R%# '
				wrapping:
					\pset format wrapped
				bash like aliases:
					ex:
						\set show_slow_queries 
						'SELECT 
							(total_time / 1000 / 60) as total_minutes, 
							(total_time/calls) as average_time, query 
						FROM pg_stat_statements 
						ORDER BY 1 DESC 
						LIMIT 100;'
					use:
						:show_slow_queries
			Psql at your fingertips
				\?
				\h alter table
				interacting with editor
					\ef my_function
			others
				.pgpass
				chmod 600 .pgpass
				env variables
				piping into psql: echo ... | psql
		Upsert Lands in PostgreSQL 9.5 – a First Look id=g10679
			Upsert Lands in PostgreSQL 9.5 – a First Look <url:file:///~/projects/study/otl/cdb.otl#r=g10679>
			http://www.craigkerstiens.com/2015/05/08/upsert-lands-in-postgres-9.5/
			upsert: create this new row, but if a conflict exists update it
			ex
				INSERT INTO products (
						upc, 
						title, 
						description, 
						link) 
				VALUES (
						123456789, 
						‘Figment #1 of 5’, 
						‘THE NEXT DISNEY ADVENTURE IS HERE - STARRING ONE OF DISNEY'S MOST POPULAR CHARACTERS! ’, 
						‘http://www.amazon.com/dp/B00KGJVRNE?tag=mypred-20’
						)
				ON CONFLICT DO UPDATE SET description=excluded.description;
	Debugging your PL_pgSQL code-pOb-7JZQoW4.webm
		pgAdmin
			breakpoints inside functions
		plprofiler
			runs a query inside profiler
	SQL and Business Logic
		http://tapoueh.org/blog/2017/06/sql-and-business-logic/
		pgloader:
			load csv, mysql etc files like copy
	SQL Joins with On or Using id=g10460
		SQL Joins with On or Using <url:file:///~/projects/study/otl/cdb.otl#r=g10460>
		https://lornajane.net/posts/2012/sql-joins-with-on-or-using
		why: you use ON for most things, but USING is a handy shorthand for the situation where the column names are the same
		ex:
			select owners.name as owner, pets.name as pet, pets.animal 
				from owners join pets USING (owners_id);
			===
			select owners.name as owner, pets.name as pet, pets.animal 
				from owners join pets ON (pets.owners_id = owners.owners_id);
	Becoming a SQL Guru-cL8QZ2yyFCM.mp4
		Set Operations id=g10461
			Set Operations <url:file:///~/projects/study/otl/cdb.otl#r=g10461>
			Union vs union all
				union: distinct rows only
				ex:
					select city from customers
					UNION ALL
					select city from suppliers
				ex:
					select city from customers
					UNION 
					select city from suppliers
			Except vs intersect
				ex:
					select city from customers
					EXCEPT
					select city from suppliers
				ex:
					select city from customers
					INTERSECT
					select city from suppliers
		Filtered Aggregates id=g10462
			Filtered Aggregates <url:file:///~/projects/study/otl/cdb.otl#r=g10462>
			before:
				select 
					sum(revenue) as total_revenue,
					sum(CASE
						WHEN country = 'USA'
							THEN revenue
						ELSE 0
						END) as USA_revenue
				from suppliers
			after:
				select 
					sum(revenue) as total_revenue,
					sum(revenue) FILTER (where country = 'USA') as USA_revenue
				from suppliers
	Grouping Sets, Cube, Rollup
		like pivot tables
		Grouping sets: allows creation of sets wherein a subtotal is calculated for each set
		Rollup: allows for creating of hierarchical 
		Cube: 
	Subquery Examples id=g10463
		Subquery Examples <url:file:///~/projects/study/otl/cdb.otl#r=g10463>
		http://www.zentut.com/sql-tutorial/sql-subquery/
			WHERE set
				SELECT ... WHERE field IN (SELECT field2 FROM ...)
			expression
				to substitute an expression in SQL
					SELECT field1, 
						(SELECT AVG(unitprice) FROM products AS 'avg_price'),
						(unitprice - (SELECT AVG(unitprice) FROM products)) as diff
					FROM products
					WHERE category_id = 1
			these subqueries execute independently
				alternative: correlated subquery
					executed dependently of some outer query
		http://www.zentut.com/sql-tutorial/understanding-correlated-subquery/
			in SELECT clause
				for each row of Customers table runs the subquery
				SELECT 
						companyname,
						city,
						(SELECT SUM(unitprice * quantity)
						 FROM orders
						 INNER JOIN orderdetails ON orderdetails.orderid = orders.orderid
						 WHERE orders.customerid = customers.customerid) AS total
				FROM customers
				ORDER BY total DESC
				LIMIT 5;
			in WHERE clause
				SELECT companyname, city
				FROM customers
				WHERE 100000 < (
								SELECT SUM(unitprice * quantity)
								FROM orders
								INNER JOIN orderdetails ON orderdetails.orderid = orders.orderid
								WHERE orders.customerid = customers.customerid);
			in HAVING clause
				SELECT t1.categoryID, categoryName
				FROM products t1
				INNER JOIN categories c ON c.categoryID = t1.categoryID
				GROUP BY categoryID
				HAVING MAX(unitprice) > ALL (
					 SELECT  2 * AVG(unitprice)
					 FROM products t2
					 WHERE t1.categoryID = t2.categoryID)
	PostgreSQL’s Powerful New Join Type: LATERAL id=g10668
		PostgreSQL’s Powerful New Join Type: LATERAL <url:file:///~/projects/study/otl/cdb.otl#r=g10668> 
		https://heapanalytics.com/blog/engineering/postgresqls-powerful-new-join-type-lateral
		doc:
			The LATERAL key word can precede a sub-SELECT FROM item. This allows the sub-SELECT to refer to columns of FROM items that appear before it in the FROM list. (Without LATERAL, each sub-SELECT is evaluated independently and so cannot cross-reference any other FROM item.
			it means that a LATERAL join is like a SQL foreach loop, in which PostgreSQL will iterate over each row in a result set and evaluate a subquery using that row as a parameter
		ex:
			we have a table of click events
				CREATE TABLE event (
					user_id BIGINT,
					event_id BIGINT,
					time BIGINT NOT NULL,
					data JSON NOT NULL,
					PRIMARY KEY (user_id, event_id)
				)
			problem: how many people view our homepage and then enter a credit card within twoo weeks
			code
				SELECT 
					user_id,
					view_homepage,
					view_homepage_time,
					enter_credit_card,
					enter_credit_card_time
				FROM (
					-- Get the first time each user viewed the homepage.
					SELECT
						user_id,
						1 AS view_homepage,
						min(time) AS view_homepage_time
					FROM event
					WHERE
						data->>'type' = 'view_homepage'
					GROUP BY user_id
				) e1 LEFT JOIN LATERAL (
					-- For each row, get the first time the user_id did the enter_credit_card
					-- event, if one exists within two weeks of view_homepage_time.
					SELECT
						1 AS enter_credit_card,
						time AS enter_credit_card_time
					FROM event
					WHERE
						user_id = e1.user_id AND
						data->>'type' = 'enter_credit_card' AND
						time BETWEEN view_homepage_time AND (view_homepage_time + 1000*60*60*24*14)
					ORDER BY time
					LIMIT 1
				) e2 ON true
	Speeding Up PostgreSQL With Partial Indexes id=g10670
		Speeding Up PostgreSQL With Partial Indexes <url:file:///~/projects/study/otl/cdb.otl#r=g10670>
		ex: create partial index
			CREATE INDEX event_signups ON event (time)
			WHERE (data->>'type') = 'submit' AND (data->>'path') = '/signup/'
		ex: use
			SELECT COUNT(*)
			FROM event
			WHERE
				(data->>'type') = 'submit' AND
				(data->>'path') = '/signup/' AND
				time BETWEEN 1409554800000 AND 1410159600000
	Postgres Window Functions id=g10671
		Postgres Window Functions <url:file:///~/projects/study/otl/cdb.otl#r=g10671>
		https://robots.thoughtbot.com/postgres-window-functions
		oracle: analytic functions
		goal: get each post's three most recent comments
			ex: using join table and then filter in app
				SELECT posts.id AS post_id, comments.id AS comment_ids, comments.body AS body
				FROM posts LEFT OUTER JOIN comments ON posts.id = comments.post_id;
			ex: rank comments
				code
					SELECT posts.id AS post_id, comments.id AS comment_id, comments.body AS body,
						dense_rank() OVER (
							PARTITION BY post_id
							ORDER BY comments.created_at DESC
						) AS comment_rank
					FROM posts LEFT OUTER JOIN comments ON posts.id = comments.post_id;
				result
					| POST_ID | COMMENT_ID |       BODY | COMMENT_RANK |
					|---------|------------|------------|--------------|
					|       1 |          4 | foo newest |            1 |
					|       1 |          3 |  foo newer |            2 |
					|       1 |          2 |    foo new |            3 |
					|       1 |          1 |    foo old |            4 |
					|       2 |          8 | bar newest |            1 |
					|       2 |          7 |  bar newer |            2 |
				filter first three
					WHERE comment_rank < 4;
				refactor using CTE
					code
						WITH ranked_comments AS (
							SELECT posts.id AS post_id, comments.id AS comment_id, comments.body AS body,
								dense_rank() OVER (
									PARTITION BY post_id
									ORDER BY comments.created_at DESC
								) AS comment_rank
							FROM posts LEFT OUTER JOIN comments ON posts.id = comments.post_id
						)
						SELECT
							post_id,
							comment_id,
							body
						FROM ranked_comments
						WHERE comment_rank < 4;
	Database constraints in Postgres: The last line of defense id=g10673
		Database constraints in Postgres: The last line of defense <url:file:///~/projects/study/otl/cdb.otl#r=g10673>
		https://www.citusdata.com/blog/2018/03/19/postgres-database-constraints/
		data has inertia once it is stored
		Constraints
			Partial unique indexes
				ex: email is unique but sometimes an account can be deleted
					CREATE UNIQUE INDEX
						ON users (email)
						WHERE deleted_at is null;
			Constraints with custom logic
				ex: all prices to be greater than 0
					CREATE TABLE products (
							product_no integer,
							name text,
							price numeric CHECK (price > 0)
					);
				ex: sale prices are cheaper than the original price
					price numeric CHECK (price > 0),
					sale_price numeric CHECK (sale_price > 0),
					CHECK (price > sale_price)
				ex: custom function to check
					code: is fibonacci
						CREATE OR REPLACE FUNCTION is_fib(i int) RETURNS boolean AS $$
						DECLARE
						 a integer := 5*i*i+4;
						 b integer := 5*i*i-4;
						 asq integer := sqrt(a)::int;
						 bsq integer := sqrt(b)::int;
						BEGIN
						 RETURN asq*asq=a OR bsq*bsq=b;
						end
						$$ LANGUAGE plpgsql IMMUTABLE STRICT; 
					use:
						CREATE TABLE onlyfib( i int CHECK (is_fib(i)) );
						INSERT INTO onlyfib values (5), (8);
						INSERT INTO onlyfib values (6);
							ERROR: new row for relation "onlyfib" violates
			Exclusion constraints
				useful with ranges
				ex: a customer can only be billed one price at a time for each resource (no double billing)
					table
						CREATE TABLE billings (
						 id uuid NOT NULL,
						 period tstzrange NOT NULL,
						 price_per_month integer NOT NULL
						);
					don't allow overlaps 
						ALTER TABLE billings
						ADD CONSTRAINT billings_excl
						EXCLUDE USING gist (
						 id WITH =,
						 period WITH &&
						);
			Datatypes as database constraints
				ex: tweet varchar(140)
	SELECT DISTINCT ON in PostgreSQL id=g10674
		SELECT DISTINCT ON in PostgreSQL <url:file:///~/projects/study/otl/cdb.otl#r=g10674>
		https://www.geekytidbits.com/postgres-distinct-on/
		SELECT DISTINCT ON
			this is not a typical DISTINCT
		ex: what is the most recent duration for each url:
			SELECT l.url, l.request_duration
			FROM log l
			INNER JOIN (
				SELECT url, MAX(timestamp) as max_timestamp
				FROM log
				GROUP BY url
			) last_by_url ON l.url = last_by_url.url AND l.timestamp = last_by_url.max_timestamp;
		alternative ways:	
			WHERE IN
			LATERAL join
			WINDOW function
		think about regular SELECT DISTINCT
			discarding duplicate rows and only retaining a single one
			what if: to only discard based on some filelds
				this is DISTINCT ON
		With DISTINCT ON, You tell PostgreSQL to return a single row for each distinct group defined by the ON clause. Which row in that group is returned is specified with the ORDER BY clause
		ex:
			code
				SELECT DISTINCT ON (url) url, request_duration
				FROM logs
				ORDER BY timestamp DESC
			explain:
				put the logs into groups unique by url (ON (url)), sort each of these groups by most recent (ORDER BY timestamp DESC) and then return fields for the first record in each of these groups (url, request_duration).
	The LATERAL join id=g10675
		The LATERAL join <url:file:///~/projects/study/otl/cdb.otl#r=g10675>
		https://www.geekytidbits.com/lateral-join/
		Examples
			ex: Top-N per group
				for each customer, return 2 most recent orders
				code
					SELECT c.id, o.order_id, o.date, o.amount
					FROM customers c
							CROSS JOIN LATERAL (
									SELECT id as order_id, date, amount
									FROM orders
									WHERE customer_id = c.id
									ORDER BY date DESC
									LIMIT 2
							) o
					ORDER BY c.id;
			ex: Call User-Defined Function for each row
				SELECT t.id, s. p.pl_amount, p.pl_percentage
				FROM trades t
						get_trade_pl(t.id) p;
			ex: Reuse calculated values
				SELECT l.id, c.start_date,  c.end_date, (c.end_date - c.start_date) as days_diff
				FROM log l
						CROSS JOIN LATERAL (
								SELECT to_date(l.start_timestamp, 'MM/dd/YYYY'),
											 to_date(l.end_timestamp, 'MM/dd/YYYY')
						) as c(start_date, end_date);
	Performance Tuning Queries in PostgreSQL
		https://www.geekytidbits.com/performance-tuning-postgres/
		@later
	Customizing My Postgres Shell id=g10676
		Customizing My Postgres Shell <url:file:///~/projects/study/otl/cdb.otl#r=g10676>
		https://www.citusdata.com/blog/2017/07/16/customizing-my-postgres-shell-using-psqlrc/
		~/.psqlrc
		ex:
			\set QUIET 1
			\pset null '¤'
			\set PROMPT1 '%[%033[1m%][%/] # '
			-- SELECT * FROM<enter>. %R shows what type of input it expects.
			\set PROMPT2 '... > '
			\timing
			\x auto
			\set VERBOSITY verbose
			\set HISTFILE ~/.psql_history- :DBNAME
			\set HISTCONTROL ignoredups
			\set COMP_KEYWORD_CASE upper
			\unset QUIET
		exp: \set QUIET 1
			be quiet during startup
		exp: make nulls explicit
			\pset null '¤'
			\pset can customize: borders, footer, chars
		exp: know where you are connected to (prod, staging, dev)
			\set PROMPT1 '%[%033[1m%][%/] # '
		exp: time all things
			\timing
		exp: log all things
		exp: let pgr format everything for you
			\x auto
	An Explained psqlrc id=g10678
		An Explained psqlrc <url:file:///~/projects/study/otl/cdb.otl#r=g10678>
		https://robots.thoughtbot.com/an-explained-psqlrc
		\pset
			for changing output format
		\set
			for everything else
		\set ON_ERROR_ROLLBACK interactive
			default is off
			when `on` errors are ignored
			when `off` you get nothing when there is only one error in transaction
		HISTFILE
			~/.psql_history
			ex
				bands=# \echo hello
				hello
				bands=# \echo :DBNAME
				bands
			ex: history per each db
				\set HISTFILE ~/.psql_history- :DBNAME
		pager
			program that paginates text: more, less
		NULL
			\pset null '(null)'
	The Database As Queue Anti-Pattern
		http://mikehadlow.blogspot.com/2012/04/database-as-queue-anti-pattern.html
		when all you have is a hammer, every problem looks like a nail
		polling db at some interval like:
			SELECT * FROM MyWorkflow WHERE Status = 'New'
		why anti-pattern
			1: there is polling
			2: db is efficient at insert, update, updates. pick two or one.
			3: clearing records is ineffcient
			4: sharing a db between apps is bad
index_rmd for db files id=g10629
	~/projects/itr/vrp_doc/study/db_vrp.Rmd
	~/projects/study/db/study_bq.Rmd id=g10786
		~/projects/study/db/study_bq.Rmd <url:file:///~/projects/study/otl/cdb.otl#r=g10786>
		Basics of BQ
			--use_legacy_sql
				bq query --nouse_legacy_sql 'select count(*) from `bigquery-public-data.samples.shakespeare`'
				bq query --use_legacy_sql=false 'select count(*) from `bigquery-public-data.samples.shakespeare`'
			help
				bq version
				bq help
				bq --help # help on flags
				bq help <command>
				bq <command> --help # help on flags
			$HOME/.bigqueryrc
				project_id = bizqualify
				credential_file = .../singlestore_bq.json
				[ls]
				--max_results=3000
				[query]
				--use_legacy_sql=false
			query
				bq query 'select count(*) from `bigquery-public-data.samples.shakespeare`'
				bq query 'select count(*) from bq_data.data_20190103_ts'
		Introduction to Interacting with BigQuery
			Creating and using datasets
				create a new dataset
					bq --location=US mk -d --default_table_expiration 3600 --description "This is my dataset." mydataset
				Listing datasets
					bq ls --format=prettyjson --project_id [PROJECT_ID]
					bq ls --format=prettyjson # default project
					bq ls --format=sparse 
				Getting dataset metadata
					SELECT * FROM `bigquery-public-data.samples.__TABLES_SUMMARY__`
					SELECT table_id FROM `bigquery-public-data.samples.__TABLES_SUMMARY__`
			Loading data into BigQuery
				Loading data from Cloud Storage
					CSV Files
						inline schema:
							bq --location=US load --source_format=CSV mydataset.mytable gs://mybucket/mydata.csv qtr:STRING,sales:FLOAT,year:STRING
						schema file:
							bq --location=asia-northeast1 load --skip_leading_rows=2 --source_format=CSV mydataset.mytable gs://mybucket/mydata.csv ./myschema.json
				Loading data from local file
					bq --location=US load --source_format=NEWLINE_DELIMITED_JSON mydataset.mytable ./mydata.json ./myschema.json
	~/projects/study/db/study_db.Rmd id=g10785
		~/projects/study/db/study_db.Rmd <url:file:///~/projects/study/otl/cdb.otl#r=g10785>
	~/projects/study/db/study_meta_data.Rmd
	~/projects/study/db/study_postgrest.Rmd id=g10633
		~/projects/study/db/study_postgrest.Rmd <url:file:///~/projects/study/otl/cdb.otl#r=g10633>
		ref
			~/projects/study/js/study_expressjs_server.Rmd <url:file:///~/gdrive/mynotes/content/code/cjs/cjs.md#r=g10634>
			~/projects/study/r/study_plumber_restful_apis.Rmd <url:file:///~/gdrive/mynotes/content/code/cr/cr.md#r=g10653>
		Start postgrest server
			initialize psk postgrest server
				subzero base-project
					##> user inputs: psk01
					##> choose database migration n
				cd psk01
			bzq postgrest server
				ref: `00: Start docker  <url:file:///~/projects/bizqualify/BQ-data-run/datarun/make_api_ready_for_creditsafe_20181219.md#r=g10761>`
				Edit `docker-compose.yml`
					volumes:
						- ~/postgresql:/var/lib/postgresql/data
				Edit `~/.bashrc`
					export DB_HOST=localhost
					...
				Edit `.env`
					DB_HOST=db
					...
			vrp postgrest server
				First prepare the data: `walkthrough yuml2data01 <url:file:///~/projects/itr/vrp_doc/doc_itr.md#r=g10550>`
				ref: `walkthrough vrp_psk01 <url:file:///~/projects/itr/vrp_doc/doc_itr.md#r=g10551>`
				Starting database and postgrest with `~/projects/itr/vrp_psk01/docker-compose.yml`
		Create new data from scratch
			Toy Data 
				sql
					CREATE SCHEMA schema01;
					CREATE TABLE schema01.table01 ...
					COPY schema01.table01 (id,title) ...
					GRANT USAGE ON SCHEMA schema01 TO anonymous, webuser;
					CREATE VIEW api.table01 AS SELECT id, title FROM schema01.table01;
					GRANT SELECT ON api.table01 TO anonymous;
					GRANT SELECT, INSERT, UPDATE, DELETE ON api.table01 to webuser;
		Make a query 
			Use curl to make a query
				opt01: Default data
					ref: `walkthrough study_psk <url:file:///~/projects/study/otl/cdb.otl#r=g10522>`
					curl http://localhost:8080/rest/todos?select=id,todo
				opt02: vrp data
					ref: `walkthrough vrp_psk01 <url:file:///~/projects/itr/vrp_doc/doc_itr.md#r=g10551>`
					curl -H "Authorization: Bearer $JWT_TOKEN" http://localhost:8080/rest/address
				opt03: sample toy data
					ref: `02.04.02. Create a new schema and define an api for it <url:file:///~/projects/study/db/tutorial_postgrest.Rmd#r=g10769>`
					curl http://localhost:8080/rest/t01?select=id
				opt04: bzq data
					ref: `04.02. Convert v1/search API with Postgrest Query Parameters <url:file:///~/projects/bizqualify/BQ-data-run/datarun/make_api_ready_for_creditsafe_20181219.md#r=g10770>`
					curl -H "Authorization: Bearer $JWT_TOKEN" http://34.221.238.13:8080/rest/all_data?limit=3 > ../log/log_curl_all_data_limit_3.json
					curl -H "Authorization: Bearer $JWT_TOKEN" http://34.221.238.13:8080/rest/all_data?bq_website=eq.zerotaxplan.com > ../log/log_curl_bq_website.json
			Use superagent or cyclejs to make a query
				opt01: default data
				opt02: vrp data
					Superagent use. ref: `Use superagent or cyclejs <url:file:///~/projects/study/db/study_postgrest.Rmd#r=g10763>`
						superagent
							.get( 'http://localhost:8080/rest/company' )
					cyclejs use. From `p03: Multiple HTTP Requests <url:file:///~/projects/study/js/study_notes_cyclejs.Rmd#r=g10764>`
						const requests$ = xs.from( [ {
							url: 'http://localhost:8080/rest/plan?select=plan_id,usr,depot_id',
							method: 'GET',
							headers: {
								"Authorization": "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoxLCJyb2xlIjoid2VidXNlciJ9.uSsS2cukBlM6QXe4Y0H90fsdkJSGcle9b7p_kMV1Ymk"
				opt03: toy data
				opt04: bzq data
			Use R to make a query
				opt01: default data
				opt02: vrp data
					response <- httr::GET("http://localhost:8080/rest/address",
						add_headers(
							"Authorization" = "Bearer ..."
					httr::content(response, as = "text")
					httr::content(response, as = "parsed") 
				opt03: toy data
					response <- httr::GET("http://localhost:8080/rest/table01")
				opt04: bzq data
		Bulk Insert Data with CSV or JSON
			Use curl to insert data
				opt02: vrp data insert
					JSON Bulk Data Insert
						Opt01: POST data as HTML form
							-d --data explicit data
								curl ... -H "Content-Type: application/json" -d '[{"company_id": "103", "company_name":"company_103"},...]' http:.../company
						Opt02: Send data in a json file
							--data-binary @filename json/csv file
								curl ... -H "Content-Type: text/csv" --data-binary @company01.csv http:.../company
						Opt03: Send data using form-data
							curl -H "Authorization: Bearer $JWT_TOKEN" -H "Content-Type: application/json" --form @company02.json http://localhost:8080/rest/company
							error: curl: option -F: is badly used here
					CSV Bulk Data Insert
						Opt01: Embed text inside form data `-d`
							curl ... -H "Content-Type: text/csv" -d $'company_id,company_extid,company_name\n201,,company_201'  http://localhost:8080/rest/company
						Opt02: Send csv file
							curl ... -H "Content-Type: text/csv" --data-binary @company01.csv http://localhost:8080/rest/company
				opt03: toy data insert 
					opt01: Embed data
						curl ... -d $'id,title\n401,t401\n402,t402' http://localhost:8080/rest/table01
					opt02: Send file
						curl ... --data-binary @.../table01.csv http://localhost:8080/rest/table01
			Use superagent or cyclejs 
				opt02: vrp data insert
					Opt01: Use superagent to send file data
						superagent
							superagent.post( 'http://localhost:8080/rest/company' )
								.set('Content-Type', 'text/csv')
								.set('Authorization', '...')
								.send('company_id,company_extid,company_name\n201,,company_201')
					Opt02: Use Cyclejs to send file using attach.name
						this doesn't work with postgrest
					Opt03: Use Cyclejs to send file data
						cyclejs
							url: 'http://localhost:8080/rest/company',
							method: 'POST',
							headers: { 'Content-Type': 'text/csv', "Authorization": "..." },
							send: 'company_id,company_extid,company_name\n203,,company_203',
			Use R to upload data 
				opt02: vrp data
					opt01: csv in body
						response <- httr::POST("http://localhost:8080/rest/company", 
							body = "company_id,company_extid,company_name\n201,,company_201",
							add_headers(
								"Authorization" = "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoxLCJyb2xlIjoid2VidXNlciJ9.uSsS2cukBlM6QXe4Y0H90fsdkJSGcle9b7p_kMV1Ymk",
								"Content-Type" = "text/csv"
						status_code(response)
					opt02: list (json) in body
						response <- httr::POST("http://localhost:8080/rest/company", 
							body = list(company_id = 201, company_name = "c201"),
							encode = "json",
					opt03: csv from file
						response <- httr::POST("http://localhost:8080/rest/company", 
							body = upload_file("~/projects/study/db/ex/study_postgrest/e03/company01.csv"),
				opt03: toy data
					opt01: csv in body
						response <- httr::POST("http://localhost:8080/rest/table01", 
							body = "id,title\n501,t501\n502,t502",
					opt02: list (json) in body
						response <- httr::POST("http://localhost:8080/rest/table01", 
							body = list(id = 601, title = "t601"),
							encode = "json",
					opt03: csv from file
						response <- httr::POST("http://localhost:8080/rest/table01", 
							body = upload_file("~/projects/study/db/ex/study_postgrest/e03/table01.csv"),
			Evaluation of Different Methods
				ref: veri gönderme - http <url:file:///~/gdrive/mynotes/content/code/ccode.md#r=g10651>
				postgrest supports only `x-www-form-urlencoded` 
				send data directly as post body
					curl -H "Authorization: Bearer $JWT_TOKEN" -H "Content-Type: application/json" -d '[{"company_id": "103", "company_name":"company_103"},{"company_id": "102", "company_name":"company_102"}]'  http://localhost:8080/rest/company
					superagent
						.send('company_id,company_extid,company_name\n201,,company_201')
				Upload data to postgrest with cyclejs 
					cyclejs
						send: 'company_id,company_extid,company_name\n203,,company_203',
			as application/json or text/csv
				curl
			as form-data
				note: postgrest doesn't support form-data
				curl
					curl -i -X POST -H "Content-Type: multipart/form-data" -F "avatar=@/Users/mertnuhoglu/projects/study/js/ex/study_expressjs_server/img/galileo.jpg" http://localhost:3000/profile
				superagent
				cyclejs
					attach file
						{
							url: 'http://localhost:4755/echo2',
							method: 'POST',
							attach: [
								{
									name: 'upload',
									path: '/Users/mertnuhoglu/projects/study/r/ex/study_plumber_restful_apis/e01.R',
									filename: 'e01.R'
								}
							],
	~/projects/study/db/tutorial_postgrest.Rmd id=g10765
		~/projects/study/db/tutorial_postgrest.Rmd <url:file:///~/projects/study/otl/cdb.otl#r=g10765>
		01. Postgrest Official Tutorial 
			normal flow
				create api schema
					CREATE SCHEMA api;
					CREATE TABLE api.todos ( id SERIAL PRIMARY KEY, done BOOLEAN NOT NULL DEFAULT FALSE, .. )
					INSERT INTO api.todos (task) VALUES (..)
				grant access rights
					CREATE USER postgres SUPERUSER;
					CREATE role web_anon nologin;
					GRANT web_anon to postgres;
					GRANT USAGE ON schema api to web_anon;
					GRANT SELECT ON api.todos to web_anon;
				Edit `tutorial.conf`
					db-uri = "postgres://superuser:superuserpass@localhost/bizqualify"
					db-schema = "api"
					db-anon-role = "web_anon"
				run server:
					postgrest tutorial.conf
				test
					curl http://localhost:3000/todos
			Error: Remote call 
				curl http://34.215.116.149:3000/todos
					curl: (7) Failed to connect to 34.215.116.149 port 3000: Connection refused
				cause: 
					ubuntu@ip-172-31-5-223:~$ netstat -lntp | grep 3000
						##> tcp        0      0 127.0.0.1:3000          0.0.0.0:*               LISTEN      128145/postgrest
					postgrest only listens requests from localhost.
		02. PSK (Postgrest Starter Kit) Official Tutorial
			normal flow
				Create a new project
					subzero base-project
					cd psk01
				test
					curl http://localhost:8080/rest/todos?select=id
			02.01. Change Credentials of Postgres 
				Edit `~/psk02/.env`
					DB_HOST=localhost
					DB_PORT=5432
					...
			02.02. Run the database separately
				Edit `~/psk04/docker-compose.yml`
					volumes:
						- ~/bizqualify_data:$HOME/bizqualify_data
						- ~/postgresql:/var/lib/postgresql/data
			02.03. Restore Database
				Restore data as in `~/projects/bizqualify/BQ-data-run/datarun/process_restoring_datasets.md`
					pg_restore -d bizqualify ~/backup/data_20181202.dump
			02.04. Setup API for our data
				02.04.01. Grant access rights
					Error: schema data_20181220 does not exist
						db_1              | ERROR:  schema "data_20181220" does not exist
						cause:
							subzero dashboard
							this command resets all the database after every change in db/src files
				02.04.02. Create a new schema and define an api for it
					02.04.02. Create a new schema and define an api for it <url:file:///~/projects/study/db/tutorial_postgrest.Rmd#r=g10769>
					use toy data
						Now, create some toy data.
							CREATE SCHEMA test2;
							CREATE TABLE test2.t03 (
								id bigint,
								title text
							);
							COPY test2.t03 (id,title) FROM STDIN (ENCODING 'utf-8');
							101	t101
							101	t201
							\.
						create api view
							CREATE VIEW api.t03 AS SELECT id, title FROM test2.t03;
						grant rights
							GRANT USAGE ON SCHEMA test2 TO anonymous, webuser;
							GRANT SELECT ON api.t03 TO anonymous, webuser;
						test
							curl http://localhost:8080/rest/t03?select=id
BigQuery  id=g10664
	ref
		~/projects/study/db/study_bq.Rmd <url:file:///~/projects/study/otl/cdb.otl#r=g10786>
	points
		limit results of bq ls
			code: ~/.bigqueryrc
				[ls]
				--max_results=3000
	bq cli
		--flagfile=file.sql  # query using file 
			bq query --flagfile=/home/user/abc.sql
	articles
		https://cloud.google.com/bigquery/docs/quickstarts/quickstart-command-line
			Examine a table
				bq show <projectId>:<datasetId>.<tableId>
				bq show publicdata:samples.shakespeare
			Help 
				bq help
				bq help query
			Run a query
				bq query "<sql>"
				bq query "SELECT word, SUM(word_count) as count FROM publicdata:samples.shakespeare WHERE word CONTAINS 'raisin' GROUP BY word"
			Create a new table
				[dataset] 1-n [table]
				[project] 1-n [dataset]
				tutorial: baby names
					02: create a new dataset
						bq ls
						bq ls <project>:
						bq ls bizqualify:
						bq mk tmp_babynames
					03: upload table
						bq load tmp_babynames.names2010 yob2010.txt name:string,gender:string,count:integer
							names2010: tableID
						bq ls tmp_babynames
						bq show tmp_babynames.names2010
					04: run queries
						bq query "SELECT name,count FROM tmp_babynames.names2010 WHERE gender = 'F' ORDER BY count DESC LIMIT 5"
					05: clean dataset
						bq rm -r tmp_babynames
		http://postgres.tobigquery.com
			Loading data into BQ
				bq cli tool
				`bq load`
Other id=g10457
	Good Resources
		https://stackoverflow.com/users/939860/erwin-brandstetter?tab=answers
	Codes/Issues
		Upsert
			INSERT ..
			ON CONLICT(pk_field)
			UPDATE ..
	Refcards/Examples
		Postgresql DB Introspection Notebook
			https://github.com/catherinedevlin/db-introspection-notebook/blob/master/postgresql/postgresql.ipynb
			ex: Locks
				select t.relname,l.locktype,page,virtualtransaction,pid,mode,granted from pg_locks l, pg_stat_all_tables t where l.relation=t.relid order by relation asc;
			ex: Total size of biggest tables (including their indexes)
				SELECT nspname || '.' || relname AS "relation",
					pg_size_pretty(pg_total_relation_size(C.oid)) AS "total_size"
				FROM pg_class C
				LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
				WHERE nspname NOT IN ('pg_catalog', 'information_schema')
					AND C.relkind <> 'i'
					AND nspname !~ '^pg_toast'
				ORDER BY pg_total_relation_size(C.oid) DESC
				LIMIT 20;
			ex: connection
				import os
				import pwd
				sqla_conn = os.getenv('SQLA_CONN') or \
						'postgresql://%s:%s@%s/%s' % (os.getenv('PG_USERNAME') or '', os.getenv('PG_PASSWORD') or '',
																					os.getenv('PG_HOST') or '', 
																					os.getenv('PG_DATABASE') or pwd.getpwuid(os.getuid()).pw_name)
	Index-only scans
		https://wiki.postgresql.org/wiki/Index-only_scans
		allows certain queries to be run just by getting data from indexes, not from tables
		Example queries where index-only scans could be used in principle
			select count(*) from categories;
				assuming: an index on a column
			select 1st_indexed_col, 2nd_indexed_col from categories;
				assuming: composite index 1. 2. col
	https://blog.jooq.org/2014/12/30/the-awesome-postgresql-9-4-sql2003-filter-clause-for-aggregate-functions/
		ex: get number of countries with GDP higher than 40000 for each year id=g10483
			ex: get number of countries with GDP higher than 40000 for each year <url:file:///~/projects/study/otl/cdb.otl#r=g10483>
			CREATE TABLE countries (
				code CHAR(2) NOT NULL,
				year INT NOT NULL,
				gdp_per_capita DECIMAL(10, 2) NOT NULL
			);
			SELECT
				year,
				count(*) FILTER (WHERE gdp_per_capita >= 40000)
			FROM countries
			GROUP BY year
			-->
			year   count
			------------
			2012   4
			2011   5
		ex: use as window function
			SELECT
				year,
				code,
				gdp_per_capita,
				count(*) 
					FILTER (WHERE gdp_per_capita >= 40000) 
					OVER   (PARTITION BY year)
			FROM
				countries
			-->
			year   code   gdp_per_capita   count
			------------------------------------
			2009   CA           40764.00       4
			2009   DE           40270.00       4
	filter by count of records id=g10484
		filter by count of records <url:file:///~/projects/study/otl/cdb.otl#r=g10484>
		http://sqlfiddle.com/#!12/bbbdd/3
		schema
			CREATE TABLE states
				("name" int, "admin" int)
			;
			INSERT INTO states
				("name", "admin")
			VALUES
				(1, 1),
				(2, 1),
				(3, 1),
				(4, 2),
				(5, 3)
			;
		code:
			SELECT s.name,s.admin 
			FROM states s
			INNER JOIN (
			SELECT ss.admin
			FROM states ss
			GROUP BY ss.admin
			HAVING COUNT(*) < 100
			) a ON a.admin = s.admin
			ORDER BY s.admin ASC;
		out
			name  admin
			4 2
			5 3
	Processing graphs
		http://aprogrammerwrites.eu/?p=1391#.Wfgzdj_EeRs
		shortest route from person A to B in LinkedIn/Facebook
		for certain classes of graphs: relational db can offer better performance than graph databases
